{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vPSvsQ_V9v4",
        "outputId": "75413e3e-678b-482b-bee2-296924de948c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DXxAI873XvxU"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "import seaborn as sb\n",
        "import warnings \n",
        "\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4PSQcawHYhHo"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"drive/MyDrive/ST4248/data/resale_train_feature_selection.csv\")\n",
        "test_df = pd.read_csv(\"drive/MyDrive/ST4248/data/resale_test_feature_selection.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9cxcJZia3ZW"
      },
      "source": [
        "# Price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oytmvw_Gxwla"
      },
      "source": [
        "## Data Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z_sYbY5Qa2le"
      },
      "outputs": [],
      "source": [
        "X_train = train_df.drop(\"resale_price\", axis = 1)\n",
        "X_test = test_df.drop(\"resale_price\", axis = 1)\n",
        "\n",
        "y_train = train_df[\"resale_price\"]\n",
        "y_test = test_df[\"resale_price\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "wHUFQOmVRN2R",
        "outputId": "61ebcbdc-2018-4d16-d78a-bf2397b39845"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8529e219-c062-4500-af6d-90d10ed33d25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor_area_sqm</th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.132448</td>\n",
              "      <td>-0.676490</td>\n",
              "      <td>-0.443615</td>\n",
              "      <td>-1.143025</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-2.010290</td>\n",
              "      <td>0.201327</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>1.352967</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.166660</td>\n",
              "      <td>-0.384369</td>\n",
              "      <td>-0.198538</td>\n",
              "      <td>-0.290599</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.372423</td>\n",
              "      <td>1.731095</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>-0.613741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.087399</td>\n",
              "      <td>1.054745</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>0.857112</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 74 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8529e219-c062-4500-af6d-90d10ed33d25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8529e219-c062-4500-af6d-90d10ed33d25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8529e219-c062-4500-af6d-90d10ed33d25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   floor_area_sqm  nearest_mrt_dist  total_resales_in_town  remaining_lease  \\\n",
              "0       -1.132448         -0.676490              -0.443615        -1.143025   \n",
              "1       -2.010290          0.201327               1.252320         1.352967   \n",
              "2        1.166660         -0.384369              -0.198538        -0.290599   \n",
              "3        0.372423          1.731095               1.252320        -0.613741   \n",
              "4       -0.087399          1.054745               1.291532         0.857112   \n",
              "\n",
              "   town_BUKIT MERAH  street_name_ANG MO KIO ST 51  street_name_DAWSON RD  \\\n",
              "0         -0.193241                     -0.050572              -0.044588   \n",
              "1         -0.193241                     -0.050572              -0.044588   \n",
              "2         -0.193241                     -0.050572              -0.044588   \n",
              "3         -0.193241                     -0.050572              -0.044588   \n",
              "4         -0.193241                     -0.050572              -0.044588   \n",
              "\n",
              "   flat_type_3 ROOM  storey_range_01 TO 03  storey_range_04 TO 06  ...  \\\n",
              "0          1.693641              -0.459028              -0.542843  ...   \n",
              "1         -0.590444               2.178515              -0.542843  ...   \n",
              "2         -0.590444               2.178515              -0.542843  ...   \n",
              "3         -0.590444              -0.459028              -0.542843  ...   \n",
              "4         -0.590444              -0.459028               1.842152  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 74 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "zi5U5-GZRO_W",
        "outputId": "af6b8f48-f457-4a09-ece1-d4c84a7eda87"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d71a39d5-c8e5-41cf-8521-bc87385d7573\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>floor_area_sqm</th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.212805</td>\n",
              "      <td>0.004724</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.224825</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.087399</td>\n",
              "      <td>1.306314</td>\n",
              "      <td>0.242602</td>\n",
              "      <td>1.403110</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.038007</td>\n",
              "      <td>-0.341318</td>\n",
              "      <td>0.644528</td>\n",
              "      <td>-0.808741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.748640</td>\n",
              "      <td>1.265904</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.386396</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.547221</td>\n",
              "      <td>-0.978913</td>\n",
              "      <td>0.036737</td>\n",
              "      <td>-1.209882</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 74 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d71a39d5-c8e5-41cf-8521-bc87385d7573')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d71a39d5-c8e5-41cf-8521-bc87385d7573 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d71a39d5-c8e5-41cf-8521-bc87385d7573');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   floor_area_sqm  nearest_mrt_dist  total_resales_in_town  remaining_lease  \\\n",
              "0       -0.212805          0.004724               1.291532         1.224825   \n",
              "1       -0.087399          1.306314               0.242602         1.403110   \n",
              "2        0.038007         -0.341318               0.644528        -0.808741   \n",
              "3        0.748640          1.265904               1.291532         1.386396   \n",
              "4       -0.547221         -0.978913               0.036737        -1.209882   \n",
              "\n",
              "   town_BUKIT MERAH  street_name_ANG MO KIO ST 51  street_name_DAWSON RD  \\\n",
              "0         -0.193241                     -0.050572              -0.044588   \n",
              "1         -0.193241                     -0.050572              -0.044588   \n",
              "2         -0.193241                     -0.050572              -0.044588   \n",
              "3         -0.193241                     -0.050572              -0.044588   \n",
              "4         -0.193241                     -0.050572              -0.044588   \n",
              "\n",
              "   flat_type_3 ROOM  storey_range_01 TO 03  storey_range_04 TO 06  ...  \\\n",
              "0         -0.590444              -0.459028              -0.542843  ...   \n",
              "1         -0.590444              -0.459028              -0.542843  ...   \n",
              "2         -0.590444              -0.459028               1.842152  ...   \n",
              "3         -0.590444              -0.459028              -0.542843  ...   \n",
              "4          1.693641              -0.459028              -0.542843  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 74 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPKYTh4wtinr"
      },
      "source": [
        "## Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CsJsrFlIvmzf"
      },
      "outputs": [],
      "source": [
        "NN_model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrZVKcMbwCcI"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ILFBftZnvqFj"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6RW_lRRwG2i"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Yz61h9vLv7xz"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSMp0zJwKRc"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "S9v_kTrsv-38"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHpg10glxNam"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROXr07cAv-pW",
        "outputId": "8bbac2f4-cee4-4f1d-998b-1d3f306300ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               9600      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,465\n",
            "Trainable params: 174,465\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMirCFkrxUA8"
      },
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P4kZNz7HxafP"
      },
      "outputs": [],
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgT8VgWxicp"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGEt2nCHzMZ0",
        "outputId": "1e8ace5e-94b8-4a30-e62e-c555f6dcb888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 542214.7500 - mean_absolute_error: 542214.7500\n",
            "Epoch 1: val_loss improved from inf to 427010.40625, saving model to Weights-001--427010.40625.hdf5\n",
            "89/89 [==============================] - 3s 14ms/step - loss: 540692.5625 - mean_absolute_error: 540692.5625 - val_loss: 427010.4062 - val_mean_absolute_error: 427010.4062\n",
            "Epoch 2/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 145862.0312 - mean_absolute_error: 145862.0312\n",
            "Epoch 2: val_loss improved from 427010.40625 to 67819.64062, saving model to Weights-002--67819.64062.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 145620.1562 - mean_absolute_error: 145620.1562 - val_loss: 67819.6406 - val_mean_absolute_error: 67819.6406\n",
            "Epoch 3/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 56517.3594 - mean_absolute_error: 56517.3594\n",
            "Epoch 3: val_loss improved from 67819.64062 to 50547.55859, saving model to Weights-003--50547.55859.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 56589.1523 - mean_absolute_error: 56589.1523 - val_loss: 50547.5586 - val_mean_absolute_error: 50547.5586\n",
            "Epoch 4/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 46282.9766 - mean_absolute_error: 46282.9766\n",
            "Epoch 4: val_loss improved from 50547.55859 to 44113.47656, saving model to Weights-004--44113.47656.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 45807.1445 - mean_absolute_error: 45807.1445 - val_loss: 44113.4766 - val_mean_absolute_error: 44113.4766\n",
            "Epoch 5/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 40947.9570 - mean_absolute_error: 40947.9570\n",
            "Epoch 5: val_loss improved from 44113.47656 to 43319.37500, saving model to Weights-005--43319.37500.hdf5\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 40943.6641 - mean_absolute_error: 40943.6641 - val_loss: 43319.3750 - val_mean_absolute_error: 43319.3750\n",
            "Epoch 6/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 38344.6719 - mean_absolute_error: 38344.6719\n",
            "Epoch 6: val_loss improved from 43319.37500 to 41245.97656, saving model to Weights-006--41245.97656.hdf5\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 38529.9023 - mean_absolute_error: 38529.9023 - val_loss: 41245.9766 - val_mean_absolute_error: 41245.9766\n",
            "Epoch 7/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 36466.4531 - mean_absolute_error: 36466.4531\n",
            "Epoch 7: val_loss improved from 41245.97656 to 38920.65234, saving model to Weights-007--38920.65234.hdf5\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 36747.9336 - mean_absolute_error: 36747.9336 - val_loss: 38920.6523 - val_mean_absolute_error: 38920.6523\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 35068.3516 - mean_absolute_error: 35068.3516\n",
            "Epoch 8: val_loss improved from 38920.65234 to 37208.14062, saving model to Weights-008--37208.14062.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 35068.3516 - mean_absolute_error: 35068.3516 - val_loss: 37208.1406 - val_mean_absolute_error: 37208.1406\n",
            "Epoch 9/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 33713.5508 - mean_absolute_error: 33713.5508\n",
            "Epoch 9: val_loss did not improve from 37208.14062\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 33701.0898 - mean_absolute_error: 33701.0898 - val_loss: 37924.2734 - val_mean_absolute_error: 37924.2734\n",
            "Epoch 10/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 32411.8750 - mean_absolute_error: 32411.8750\n",
            "Epoch 10: val_loss improved from 37208.14062 to 36756.33203, saving model to Weights-010--36756.33203.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 32387.8359 - mean_absolute_error: 32387.8359 - val_loss: 36756.3320 - val_mean_absolute_error: 36756.3320\n",
            "Epoch 11/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 32168.5918 - mean_absolute_error: 32168.5918\n",
            "Epoch 11: val_loss improved from 36756.33203 to 36606.87891, saving model to Weights-011--36606.87891.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 32255.0195 - mean_absolute_error: 32255.0195 - val_loss: 36606.8789 - val_mean_absolute_error: 36606.8789\n",
            "Epoch 12/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 31579.3711 - mean_absolute_error: 31579.3711\n",
            "Epoch 12: val_loss improved from 36606.87891 to 35575.51562, saving model to Weights-012--35575.51562.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 31411.8691 - mean_absolute_error: 31411.8691 - val_loss: 35575.5156 - val_mean_absolute_error: 35575.5156\n",
            "Epoch 13/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 30059.9043 - mean_absolute_error: 30059.9043\n",
            "Epoch 13: val_loss improved from 35575.51562 to 34486.19922, saving model to Weights-013--34486.19922.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 30314.7383 - mean_absolute_error: 30314.7383 - val_loss: 34486.1992 - val_mean_absolute_error: 34486.1992\n",
            "Epoch 14/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 30115.4590 - mean_absolute_error: 30115.4590\n",
            "Epoch 14: val_loss did not improve from 34486.19922\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 30392.8086 - mean_absolute_error: 30392.8086 - val_loss: 35858.0547 - val_mean_absolute_error: 35858.0547\n",
            "Epoch 15/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 29551.8555 - mean_absolute_error: 29551.8555\n",
            "Epoch 15: val_loss did not improve from 34486.19922\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 29521.8340 - mean_absolute_error: 29521.8340 - val_loss: 34772.6328 - val_mean_absolute_error: 34772.6328\n",
            "Epoch 16/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 29062.2324 - mean_absolute_error: 29062.2324\n",
            "Epoch 16: val_loss did not improve from 34486.19922\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 28756.6602 - mean_absolute_error: 28756.6602 - val_loss: 34648.6406 - val_mean_absolute_error: 34648.6406\n",
            "Epoch 17/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 29333.9336 - mean_absolute_error: 29333.9336\n",
            "Epoch 17: val_loss did not improve from 34486.19922\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 29214.9590 - mean_absolute_error: 29214.9590 - val_loss: 35258.9414 - val_mean_absolute_error: 35258.9414\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 28870.2109 - mean_absolute_error: 28870.2109\n",
            "Epoch 18: val_loss improved from 34486.19922 to 34285.32031, saving model to Weights-018--34285.32031.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 28870.2109 - mean_absolute_error: 28870.2109 - val_loss: 34285.3203 - val_mean_absolute_error: 34285.3203\n",
            "Epoch 19/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 28021.2910 - mean_absolute_error: 28021.2910\n",
            "Epoch 19: val_loss improved from 34285.32031 to 33620.28125, saving model to Weights-019--33620.28125.hdf5\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 28104.0879 - mean_absolute_error: 28104.0879 - val_loss: 33620.2812 - val_mean_absolute_error: 33620.2812\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 27784.3398 - mean_absolute_error: 27784.3398\n",
            "Epoch 20: val_loss improved from 33620.28125 to 33308.95703, saving model to Weights-020--33308.95703.hdf5\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 27784.3398 - mean_absolute_error: 27784.3398 - val_loss: 33308.9570 - val_mean_absolute_error: 33308.9570\n",
            "Epoch 21/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 27259.6426 - mean_absolute_error: 27259.6426\n",
            "Epoch 21: val_loss did not improve from 33308.95703\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 27242.7188 - mean_absolute_error: 27242.7188 - val_loss: 33966.1914 - val_mean_absolute_error: 33966.1914\n",
            "Epoch 22/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 26890.9297 - mean_absolute_error: 26890.9297\n",
            "Epoch 22: val_loss did not improve from 33308.95703\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 26865.9512 - mean_absolute_error: 26865.9512 - val_loss: 34169.9844 - val_mean_absolute_error: 34169.9844\n",
            "Epoch 23/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 27063.1230 - mean_absolute_error: 27063.1230\n",
            "Epoch 23: val_loss improved from 33308.95703 to 33223.80078, saving model to Weights-023--33223.80078.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 26926.5938 - mean_absolute_error: 26926.5938 - val_loss: 33223.8008 - val_mean_absolute_error: 33223.8008\n",
            "Epoch 24/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 26758.6426 - mean_absolute_error: 26758.6426\n",
            "Epoch 24: val_loss did not improve from 33223.80078\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 26725.8906 - mean_absolute_error: 26725.8906 - val_loss: 33319.1758 - val_mean_absolute_error: 33319.1758\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 26899.3047 - mean_absolute_error: 26899.3047\n",
            "Epoch 25: val_loss improved from 33223.80078 to 33184.25781, saving model to Weights-025--33184.25781.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 26899.3047 - mean_absolute_error: 26899.3047 - val_loss: 33184.2578 - val_mean_absolute_error: 33184.2578\n",
            "Epoch 26/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 26011.6855 - mean_absolute_error: 26011.6855\n",
            "Epoch 26: val_loss improved from 33184.25781 to 32857.51953, saving model to Weights-026--32857.51953.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25905.8223 - mean_absolute_error: 25905.8223 - val_loss: 32857.5195 - val_mean_absolute_error: 32857.5195\n",
            "Epoch 27/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 25886.3613 - mean_absolute_error: 25886.3613\n",
            "Epoch 27: val_loss did not improve from 32857.51953\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 25871.1816 - mean_absolute_error: 25871.1816 - val_loss: 34363.5859 - val_mean_absolute_error: 34363.5859\n",
            "Epoch 28/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 26138.0137 - mean_absolute_error: 26138.0137\n",
            "Epoch 28: val_loss did not improve from 32857.51953\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 26121.3730 - mean_absolute_error: 26121.3730 - val_loss: 32931.5977 - val_mean_absolute_error: 32931.5977\n",
            "Epoch 29/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 25911.2773 - mean_absolute_error: 25911.2773\n",
            "Epoch 29: val_loss improved from 32857.51953 to 32648.81055, saving model to Weights-029--32648.81055.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25860.8184 - mean_absolute_error: 25860.8184 - val_loss: 32648.8105 - val_mean_absolute_error: 32648.8105\n",
            "Epoch 30/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 25122.7754 - mean_absolute_error: 25122.7754\n",
            "Epoch 30: val_loss improved from 32648.81055 to 32083.79688, saving model to Weights-030--32083.79688.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 25180.6895 - mean_absolute_error: 25180.6895 - val_loss: 32083.7969 - val_mean_absolute_error: 32083.7969\n",
            "Epoch 31/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 25368.6133 - mean_absolute_error: 25368.6133\n",
            "Epoch 31: val_loss did not improve from 32083.79688\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 24986.4805 - mean_absolute_error: 24986.4805 - val_loss: 32895.5820 - val_mean_absolute_error: 32895.5820\n",
            "Epoch 32/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 24856.8281 - mean_absolute_error: 24856.8281\n",
            "Epoch 32: val_loss did not improve from 32083.79688\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 24952.0293 - mean_absolute_error: 24952.0293 - val_loss: 33342.4297 - val_mean_absolute_error: 33342.4297\n",
            "Epoch 33/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 24616.6211 - mean_absolute_error: 24616.6211\n",
            "Epoch 33: val_loss did not improve from 32083.79688\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 25247.9746 - mean_absolute_error: 25247.9746 - val_loss: 33345.2969 - val_mean_absolute_error: 33345.2969\n",
            "Epoch 34/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 24747.9922 - mean_absolute_error: 24747.9922\n",
            "Epoch 34: val_loss improved from 32083.79688 to 31959.87500, saving model to Weights-034--31959.87500.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 24714.1914 - mean_absolute_error: 24714.1914 - val_loss: 31959.8750 - val_mean_absolute_error: 31959.8750\n",
            "Epoch 35/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 24492.0703 - mean_absolute_error: 24492.0703\n",
            "Epoch 35: val_loss improved from 31959.87500 to 31321.26953, saving model to Weights-035--31321.26953.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 24290.4473 - mean_absolute_error: 24290.4473 - val_loss: 31321.2695 - val_mean_absolute_error: 31321.2695\n",
            "Epoch 36/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 24198.6191 - mean_absolute_error: 24198.6191\n",
            "Epoch 36: val_loss did not improve from 31321.26953\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 24267.0762 - mean_absolute_error: 24267.0762 - val_loss: 31799.8750 - val_mean_absolute_error: 31799.8750\n",
            "Epoch 37/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 23812.5488 - mean_absolute_error: 23812.5488\n",
            "Epoch 37: val_loss did not improve from 31321.26953\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 23832.5469 - mean_absolute_error: 23832.5469 - val_loss: 31360.9824 - val_mean_absolute_error: 31360.9824\n",
            "Epoch 38/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 24119.7168 - mean_absolute_error: 24119.7168\n",
            "Epoch 38: val_loss improved from 31321.26953 to 31196.86328, saving model to Weights-038--31196.86328.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 24131.9629 - mean_absolute_error: 24131.9629 - val_loss: 31196.8633 - val_mean_absolute_error: 31196.8633\n",
            "Epoch 39/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 23800.8223 - mean_absolute_error: 23800.8223\n",
            "Epoch 39: val_loss did not improve from 31196.86328\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 23800.5449 - mean_absolute_error: 23800.5449 - val_loss: 31496.2695 - val_mean_absolute_error: 31496.2695\n",
            "Epoch 40/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 23908.6660 - mean_absolute_error: 23908.6660\n",
            "Epoch 40: val_loss did not improve from 31196.86328\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 23976.4160 - mean_absolute_error: 23976.4160 - val_loss: 31443.8633 - val_mean_absolute_error: 31443.8633\n",
            "Epoch 41/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 23689.2598 - mean_absolute_error: 23689.2598\n",
            "Epoch 41: val_loss did not improve from 31196.86328\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 23584.9355 - mean_absolute_error: 23584.9355 - val_loss: 31616.4570 - val_mean_absolute_error: 31616.4570\n",
            "Epoch 42/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 23070.3906 - mean_absolute_error: 23070.3906\n",
            "Epoch 42: val_loss did not improve from 31196.86328\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 23100.6055 - mean_absolute_error: 23100.6055 - val_loss: 31809.3066 - val_mean_absolute_error: 31809.3066\n",
            "Epoch 43/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 23504.2031 - mean_absolute_error: 23504.2031\n",
            "Epoch 43: val_loss did not improve from 31196.86328\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 23383.2637 - mean_absolute_error: 23383.2637 - val_loss: 31492.1680 - val_mean_absolute_error: 31492.1680\n",
            "Epoch 44/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 23030.8242 - mean_absolute_error: 23030.8242\n",
            "Epoch 44: val_loss improved from 31196.86328 to 30945.33203, saving model to Weights-044--30945.33203.hdf5\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 22954.5625 - mean_absolute_error: 22954.5625 - val_loss: 30945.3320 - val_mean_absolute_error: 30945.3320\n",
            "Epoch 45/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 22623.1953 - mean_absolute_error: 22623.1953\n",
            "Epoch 45: val_loss did not improve from 30945.33203\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 22674.7617 - mean_absolute_error: 22674.7617 - val_loss: 31186.2500 - val_mean_absolute_error: 31186.2500\n",
            "Epoch 46/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 22869.1523 - mean_absolute_error: 22869.1523\n",
            "Epoch 46: val_loss improved from 30945.33203 to 30912.05078, saving model to Weights-046--30912.05078.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 22888.4492 - mean_absolute_error: 22888.4492 - val_loss: 30912.0508 - val_mean_absolute_error: 30912.0508\n",
            "Epoch 47/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 22820.6035 - mean_absolute_error: 22820.6035\n",
            "Epoch 47: val_loss improved from 30912.05078 to 30909.55859, saving model to Weights-047--30909.55859.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22780.3359 - mean_absolute_error: 22780.3359 - val_loss: 30909.5586 - val_mean_absolute_error: 30909.5586\n",
            "Epoch 48/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 22473.0332 - mean_absolute_error: 22473.0332\n",
            "Epoch 48: val_loss improved from 30909.55859 to 30900.91016, saving model to Weights-048--30900.91016.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22186.8496 - mean_absolute_error: 22186.8496 - val_loss: 30900.9102 - val_mean_absolute_error: 30900.9102\n",
            "Epoch 49/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 22790.7305 - mean_absolute_error: 22790.7305\n",
            "Epoch 49: val_loss improved from 30900.91016 to 30874.23438, saving model to Weights-049--30874.23438.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22652.0098 - mean_absolute_error: 22652.0098 - val_loss: 30874.2344 - val_mean_absolute_error: 30874.2344\n",
            "Epoch 50/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 21864.1582 - mean_absolute_error: 21864.1582\n",
            "Epoch 50: val_loss improved from 30874.23438 to 30871.58594, saving model to Weights-050--30871.58594.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 21869.8867 - mean_absolute_error: 21869.8867 - val_loss: 30871.5859 - val_mean_absolute_error: 30871.5859\n",
            "Epoch 51/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 22359.1289 - mean_absolute_error: 22359.1289\n",
            "Epoch 51: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22412.5410 - mean_absolute_error: 22412.5410 - val_loss: 31790.7188 - val_mean_absolute_error: 31790.7188\n",
            "Epoch 52/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 22191.2520 - mean_absolute_error: 22191.2520\n",
            "Epoch 52: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22224.9961 - mean_absolute_error: 22224.9961 - val_loss: 31497.3066 - val_mean_absolute_error: 31497.3066\n",
            "Epoch 53/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 22214.6172 - mean_absolute_error: 22214.6172\n",
            "Epoch 53: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 22208.2012 - mean_absolute_error: 22208.2012 - val_loss: 31600.3066 - val_mean_absolute_error: 31600.3066\n",
            "Epoch 54/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 21624.9492 - mean_absolute_error: 21624.9492\n",
            "Epoch 54: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21606.0859 - mean_absolute_error: 21606.0859 - val_loss: 31746.8359 - val_mean_absolute_error: 31746.8359\n",
            "Epoch 55/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 22394.4453 - mean_absolute_error: 22394.4453\n",
            "Epoch 55: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 22293.9570 - mean_absolute_error: 22293.9570 - val_loss: 31328.3594 - val_mean_absolute_error: 31328.3594\n",
            "Epoch 56/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 21569.1738 - mean_absolute_error: 21569.1738\n",
            "Epoch 56: val_loss did not improve from 30871.58594\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 21507.5566 - mean_absolute_error: 21507.5566 - val_loss: 31228.8496 - val_mean_absolute_error: 31228.8496\n",
            "Epoch 57/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 21711.4355 - mean_absolute_error: 21711.4355\n",
            "Epoch 57: val_loss improved from 30871.58594 to 30473.93164, saving model to Weights-057--30473.93164.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 21679.9258 - mean_absolute_error: 21679.9258 - val_loss: 30473.9316 - val_mean_absolute_error: 30473.9316\n",
            "Epoch 58/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 21053.3730 - mean_absolute_error: 21053.3730\n",
            "Epoch 58: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 21056.0059 - mean_absolute_error: 21056.0059 - val_loss: 31928.4297 - val_mean_absolute_error: 31928.4297\n",
            "Epoch 59/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 22164.1719 - mean_absolute_error: 22164.1719\n",
            "Epoch 59: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 22067.1660 - mean_absolute_error: 22067.1660 - val_loss: 31099.2207 - val_mean_absolute_error: 31099.2207\n",
            "Epoch 60/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 21518.9531 - mean_absolute_error: 21518.9531\n",
            "Epoch 60: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21561.7852 - mean_absolute_error: 21561.7852 - val_loss: 30958.8730 - val_mean_absolute_error: 30958.8730\n",
            "Epoch 61/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 21666.5723 - mean_absolute_error: 21666.5723\n",
            "Epoch 61: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21619.4062 - mean_absolute_error: 21619.4062 - val_loss: 31176.7305 - val_mean_absolute_error: 31176.7305\n",
            "Epoch 62/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 21578.7812 - mean_absolute_error: 21578.7812\n",
            "Epoch 62: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 21592.7578 - mean_absolute_error: 21592.7578 - val_loss: 31991.2559 - val_mean_absolute_error: 31991.2559\n",
            "Epoch 63/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 21332.2852 - mean_absolute_error: 21332.2852\n",
            "Epoch 63: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 21409.5176 - mean_absolute_error: 21409.5176 - val_loss: 31576.2969 - val_mean_absolute_error: 31576.2969\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 21011.4785 - mean_absolute_error: 21011.4785\n",
            "Epoch 64: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 21011.4785 - mean_absolute_error: 21011.4785 - val_loss: 30515.9766 - val_mean_absolute_error: 30515.9766\n",
            "Epoch 65/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 20409.9629 - mean_absolute_error: 20409.9629\n",
            "Epoch 65: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 20513.8672 - mean_absolute_error: 20513.8672 - val_loss: 30998.0430 - val_mean_absolute_error: 30998.0430\n",
            "Epoch 66/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 21005.5039 - mean_absolute_error: 21005.5039\n",
            "Epoch 66: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 21031.8340 - mean_absolute_error: 21031.8340 - val_loss: 31420.2754 - val_mean_absolute_error: 31420.2754\n",
            "Epoch 67/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 21361.4629 - mean_absolute_error: 21361.4629\n",
            "Epoch 67: val_loss did not improve from 30473.93164\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21338.6406 - mean_absolute_error: 21338.6406 - val_loss: 30513.8887 - val_mean_absolute_error: 30513.8887\n",
            "Epoch 68/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 21484.7520 - mean_absolute_error: 21484.7520\n",
            "Epoch 68: val_loss improved from 30473.93164 to 30061.02344, saving model to Weights-068--30061.02344.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 21464.4688 - mean_absolute_error: 21464.4688 - val_loss: 30061.0234 - val_mean_absolute_error: 30061.0234\n",
            "Epoch 69/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 20355.3125 - mean_absolute_error: 20355.3125\n",
            "Epoch 69: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20443.7363 - mean_absolute_error: 20443.7363 - val_loss: 30710.0684 - val_mean_absolute_error: 30710.0684\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 20936.3711 - mean_absolute_error: 20936.3711\n",
            "Epoch 70: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20936.3711 - mean_absolute_error: 20936.3711 - val_loss: 31267.8691 - val_mean_absolute_error: 31267.8691\n",
            "Epoch 71/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 20057.3730 - mean_absolute_error: 20057.3730\n",
            "Epoch 71: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20141.6133 - mean_absolute_error: 20141.6133 - val_loss: 30429.6719 - val_mean_absolute_error: 30429.6719\n",
            "Epoch 72/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 20252.9922 - mean_absolute_error: 20252.9922\n",
            "Epoch 72: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20333.9043 - mean_absolute_error: 20333.9043 - val_loss: 30090.7930 - val_mean_absolute_error: 30090.7930\n",
            "Epoch 73/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 20677.2383 - mean_absolute_error: 20677.2383\n",
            "Epoch 73: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 20627.0664 - mean_absolute_error: 20627.0664 - val_loss: 30616.0000 - val_mean_absolute_error: 30616.0000\n",
            "Epoch 74/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 20403.3145 - mean_absolute_error: 20403.3145\n",
            "Epoch 74: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 20256.6777 - mean_absolute_error: 20256.6777 - val_loss: 30971.1895 - val_mean_absolute_error: 30971.1895\n",
            "Epoch 75/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 20238.2266 - mean_absolute_error: 20238.2266\n",
            "Epoch 75: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20197.2109 - mean_absolute_error: 20197.2109 - val_loss: 31130.4141 - val_mean_absolute_error: 31130.4141\n",
            "Epoch 76/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19807.5996 - mean_absolute_error: 19807.5996\n",
            "Epoch 76: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19909.7637 - mean_absolute_error: 19909.7637 - val_loss: 30728.5039 - val_mean_absolute_error: 30728.5039\n",
            "Epoch 77/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 20113.9004 - mean_absolute_error: 20113.9004\n",
            "Epoch 77: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20064.1484 - mean_absolute_error: 20064.1484 - val_loss: 32638.1328 - val_mean_absolute_error: 32638.1328\n",
            "Epoch 78/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 19965.1504 - mean_absolute_error: 19965.1504\n",
            "Epoch 78: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20008.1270 - mean_absolute_error: 20008.1270 - val_loss: 30710.3164 - val_mean_absolute_error: 30710.3164\n",
            "Epoch 79/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19762.0898 - mean_absolute_error: 19762.0898\n",
            "Epoch 79: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19771.3457 - mean_absolute_error: 19771.3457 - val_loss: 30135.7402 - val_mean_absolute_error: 30135.7402\n",
            "Epoch 80/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 20078.1133 - mean_absolute_error: 20078.1133\n",
            "Epoch 80: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 20056.9590 - mean_absolute_error: 20056.9590 - val_loss: 30737.6562 - val_mean_absolute_error: 30737.6562\n",
            "Epoch 81/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 20010.7070 - mean_absolute_error: 20010.7070\n",
            "Epoch 81: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19874.4922 - mean_absolute_error: 19874.4922 - val_loss: 30153.0293 - val_mean_absolute_error: 30153.0293\n",
            "Epoch 82/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 19644.3535 - mean_absolute_error: 19644.3535\n",
            "Epoch 82: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19718.6367 - mean_absolute_error: 19718.6367 - val_loss: 30359.8496 - val_mean_absolute_error: 30359.8496\n",
            "Epoch 83/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 20060.7461 - mean_absolute_error: 20060.7461\n",
            "Epoch 83: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 20086.8906 - mean_absolute_error: 20086.8906 - val_loss: 30540.1133 - val_mean_absolute_error: 30540.1133\n",
            "Epoch 84/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 20226.3438 - mean_absolute_error: 20226.3438\n",
            "Epoch 84: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 20206.4102 - mean_absolute_error: 20206.4102 - val_loss: 31030.0508 - val_mean_absolute_error: 31030.0508\n",
            "Epoch 85/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19365.2090 - mean_absolute_error: 19365.2090\n",
            "Epoch 85: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 19386.3906 - mean_absolute_error: 19386.3906 - val_loss: 30292.2246 - val_mean_absolute_error: 30292.2246\n",
            "Epoch 86/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 19406.6348 - mean_absolute_error: 19406.6348\n",
            "Epoch 86: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 19309.4434 - mean_absolute_error: 19309.4434 - val_loss: 30906.0957 - val_mean_absolute_error: 30906.0957\n",
            "Epoch 87/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 19663.3770 - mean_absolute_error: 19663.3770\n",
            "Epoch 87: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 19639.7500 - mean_absolute_error: 19639.7500 - val_loss: 30812.5703 - val_mean_absolute_error: 30812.5703\n",
            "Epoch 88/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19270.9492 - mean_absolute_error: 19270.9492\n",
            "Epoch 88: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 19242.7480 - mean_absolute_error: 19242.7480 - val_loss: 31615.7285 - val_mean_absolute_error: 31615.7285\n",
            "Epoch 89/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 19210.5352 - mean_absolute_error: 19210.5352\n",
            "Epoch 89: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 19320.1484 - mean_absolute_error: 19320.1484 - val_loss: 30419.8574 - val_mean_absolute_error: 30419.8574\n",
            "Epoch 90/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 19386.2578 - mean_absolute_error: 19386.2578\n",
            "Epoch 90: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19431.6641 - mean_absolute_error: 19431.6641 - val_loss: 30397.6426 - val_mean_absolute_error: 30397.6426\n",
            "Epoch 91/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18991.8047 - mean_absolute_error: 18991.8047\n",
            "Epoch 91: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18973.5547 - mean_absolute_error: 18973.5547 - val_loss: 31292.4219 - val_mean_absolute_error: 31292.4219\n",
            "Epoch 92/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 19209.7148 - mean_absolute_error: 19209.7148\n",
            "Epoch 92: val_loss did not improve from 30061.02344\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 19284.7266 - mean_absolute_error: 19284.7266 - val_loss: 30209.5840 - val_mean_absolute_error: 30209.5840\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 19570.1621 - mean_absolute_error: 19570.1621\n",
            "Epoch 93: val_loss improved from 30061.02344 to 29897.30859, saving model to Weights-093--29897.30859.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 19570.1621 - mean_absolute_error: 19570.1621 - val_loss: 29897.3086 - val_mean_absolute_error: 29897.3086\n",
            "Epoch 94/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 18975.0000 - mean_absolute_error: 18975.0000\n",
            "Epoch 94: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18972.0840 - mean_absolute_error: 18972.0840 - val_loss: 30368.0430 - val_mean_absolute_error: 30368.0430\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 19320.8145 - mean_absolute_error: 19320.8145\n",
            "Epoch 95: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19320.8145 - mean_absolute_error: 19320.8145 - val_loss: 30127.7871 - val_mean_absolute_error: 30127.7871\n",
            "Epoch 96/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 19095.3965 - mean_absolute_error: 19095.3965\n",
            "Epoch 96: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 19283.7930 - mean_absolute_error: 19283.7930 - val_loss: 30652.3906 - val_mean_absolute_error: 30652.3906\n",
            "Epoch 97/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19460.0898 - mean_absolute_error: 19460.0898\n",
            "Epoch 97: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 19492.7207 - mean_absolute_error: 19492.7207 - val_loss: 30460.1582 - val_mean_absolute_error: 30460.1582\n",
            "Epoch 98/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18555.0234 - mean_absolute_error: 18555.0234\n",
            "Epoch 98: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18555.3008 - mean_absolute_error: 18555.3008 - val_loss: 31109.3711 - val_mean_absolute_error: 31109.3711\n",
            "Epoch 99/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 19477.6504 - mean_absolute_error: 19477.6504\n",
            "Epoch 99: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 19507.1680 - mean_absolute_error: 19507.1680 - val_loss: 29995.6289 - val_mean_absolute_error: 29995.6289\n",
            "Epoch 100/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 19197.9922 - mean_absolute_error: 19197.9922\n",
            "Epoch 100: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 19086.0488 - mean_absolute_error: 19086.0488 - val_loss: 30709.3887 - val_mean_absolute_error: 30709.3887\n",
            "Epoch 101/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 19072.2871 - mean_absolute_error: 19072.2871\n",
            "Epoch 101: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 19029.6328 - mean_absolute_error: 19029.6328 - val_loss: 30553.2969 - val_mean_absolute_error: 30553.2969\n",
            "Epoch 102/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 19010.0488 - mean_absolute_error: 19010.0488\n",
            "Epoch 102: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 18987.1953 - mean_absolute_error: 18987.1953 - val_loss: 30209.1816 - val_mean_absolute_error: 30209.1816\n",
            "Epoch 103/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 19051.2500 - mean_absolute_error: 19051.2500\n",
            "Epoch 103: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 19135.9844 - mean_absolute_error: 19135.9844 - val_loss: 30305.2266 - val_mean_absolute_error: 30305.2266\n",
            "Epoch 104/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 18597.7852 - mean_absolute_error: 18597.7852\n",
            "Epoch 104: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 18511.3984 - mean_absolute_error: 18511.3984 - val_loss: 29954.6797 - val_mean_absolute_error: 29954.6797\n",
            "Epoch 105/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 18450.4141 - mean_absolute_error: 18450.4141\n",
            "Epoch 105: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 18513.7188 - mean_absolute_error: 18513.7188 - val_loss: 30492.9141 - val_mean_absolute_error: 30492.9141\n",
            "Epoch 106/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 18920.7266 - mean_absolute_error: 18920.7266\n",
            "Epoch 106: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 19006.2578 - mean_absolute_error: 19006.2578 - val_loss: 30759.1953 - val_mean_absolute_error: 30759.1953\n",
            "Epoch 107/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 18606.2246 - mean_absolute_error: 18606.2246\n",
            "Epoch 107: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 18660.0430 - mean_absolute_error: 18660.0430 - val_loss: 31615.9883 - val_mean_absolute_error: 31615.9883\n",
            "Epoch 108/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 18605.9160 - mean_absolute_error: 18605.9160\n",
            "Epoch 108: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 18566.7734 - mean_absolute_error: 18566.7734 - val_loss: 30458.3457 - val_mean_absolute_error: 30458.3457\n",
            "Epoch 109/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 18518.5449 - mean_absolute_error: 18518.5449\n",
            "Epoch 109: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18582.7090 - mean_absolute_error: 18582.7090 - val_loss: 31375.2969 - val_mean_absolute_error: 31375.2969\n",
            "Epoch 110/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 18389.3965 - mean_absolute_error: 18389.3965\n",
            "Epoch 110: val_loss did not improve from 29897.30859\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18439.0098 - mean_absolute_error: 18439.0098 - val_loss: 30056.6934 - val_mean_absolute_error: 30056.6934\n",
            "Epoch 111/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 18912.1387 - mean_absolute_error: 18912.1387\n",
            "Epoch 111: val_loss improved from 29897.30859 to 29742.12109, saving model to Weights-111--29742.12109.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18783.9883 - mean_absolute_error: 18783.9883 - val_loss: 29742.1211 - val_mean_absolute_error: 29742.1211\n",
            "Epoch 112/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18543.9785 - mean_absolute_error: 18543.9785\n",
            "Epoch 112: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18535.5176 - mean_absolute_error: 18535.5176 - val_loss: 30156.8418 - val_mean_absolute_error: 30156.8418\n",
            "Epoch 113/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 18314.6816 - mean_absolute_error: 18314.6816\n",
            "Epoch 113: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18489.1367 - mean_absolute_error: 18489.1367 - val_loss: 31170.8164 - val_mean_absolute_error: 31170.8164\n",
            "Epoch 114/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18606.0605 - mean_absolute_error: 18606.0605\n",
            "Epoch 114: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18596.7559 - mean_absolute_error: 18596.7559 - val_loss: 30224.2910 - val_mean_absolute_error: 30224.2910\n",
            "Epoch 115/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 19188.2598 - mean_absolute_error: 19188.2598\n",
            "Epoch 115: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 19205.7539 - mean_absolute_error: 19205.7539 - val_loss: 30316.2285 - val_mean_absolute_error: 30316.2285\n",
            "Epoch 116/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 18533.7773 - mean_absolute_error: 18533.7773\n",
            "Epoch 116: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 18565.1797 - mean_absolute_error: 18565.1797 - val_loss: 30542.1836 - val_mean_absolute_error: 30542.1836\n",
            "Epoch 117/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18648.5410 - mean_absolute_error: 18648.5410\n",
            "Epoch 117: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 18638.5176 - mean_absolute_error: 18638.5176 - val_loss: 30650.8320 - val_mean_absolute_error: 30650.8320\n",
            "Epoch 118/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 18657.0039 - mean_absolute_error: 18657.0039\n",
            "Epoch 118: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18699.0469 - mean_absolute_error: 18699.0469 - val_loss: 30830.3633 - val_mean_absolute_error: 30830.3633\n",
            "Epoch 119/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18543.8945 - mean_absolute_error: 18543.8945\n",
            "Epoch 119: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 18537.4219 - mean_absolute_error: 18537.4219 - val_loss: 30385.7402 - val_mean_absolute_error: 30385.7402\n",
            "Epoch 120/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 18009.9199 - mean_absolute_error: 18009.9199\n",
            "Epoch 120: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17945.8809 - mean_absolute_error: 17945.8809 - val_loss: 30349.0957 - val_mean_absolute_error: 30349.0957\n",
            "Epoch 121/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 18256.4355 - mean_absolute_error: 18256.4355\n",
            "Epoch 121: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18299.6250 - mean_absolute_error: 18299.6250 - val_loss: 31376.9004 - val_mean_absolute_error: 31376.9004\n",
            "Epoch 122/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 18711.1465 - mean_absolute_error: 18711.1465\n",
            "Epoch 122: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18852.7754 - mean_absolute_error: 18852.7754 - val_loss: 30986.2402 - val_mean_absolute_error: 30986.2402\n",
            "Epoch 123/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 18161.6016 - mean_absolute_error: 18161.6016\n",
            "Epoch 123: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18142.7305 - mean_absolute_error: 18142.7305 - val_loss: 30585.5020 - val_mean_absolute_error: 30585.5020\n",
            "Epoch 124/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 18478.7363 - mean_absolute_error: 18478.7363\n",
            "Epoch 124: val_loss did not improve from 29742.12109\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18461.0215 - mean_absolute_error: 18461.0215 - val_loss: 29772.1523 - val_mean_absolute_error: 29772.1523\n",
            "Epoch 125/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 18246.9648 - mean_absolute_error: 18246.9648\n",
            "Epoch 125: val_loss improved from 29742.12109 to 29737.28516, saving model to Weights-125--29737.28516.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18330.1309 - mean_absolute_error: 18330.1309 - val_loss: 29737.2852 - val_mean_absolute_error: 29737.2852\n",
            "Epoch 126/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17952.1543 - mean_absolute_error: 17952.1543\n",
            "Epoch 126: val_loss did not improve from 29737.28516\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 17972.7324 - mean_absolute_error: 17972.7324 - val_loss: 30244.5723 - val_mean_absolute_error: 30244.5723\n",
            "Epoch 127/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17911.4609 - mean_absolute_error: 17911.4609\n",
            "Epoch 127: val_loss did not improve from 29737.28516\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 17905.0488 - mean_absolute_error: 17905.0488 - val_loss: 30306.2012 - val_mean_absolute_error: 30306.2012\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 18057.9668 - mean_absolute_error: 18057.9668\n",
            "Epoch 128: val_loss did not improve from 29737.28516\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 18057.9668 - mean_absolute_error: 18057.9668 - val_loss: 29859.8164 - val_mean_absolute_error: 29859.8164\n",
            "Epoch 129/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 18313.5449 - mean_absolute_error: 18313.5449\n",
            "Epoch 129: val_loss did not improve from 29737.28516\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 18358.2012 - mean_absolute_error: 18358.2012 - val_loss: 31897.5430 - val_mean_absolute_error: 31897.5430\n",
            "Epoch 130/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17867.0078 - mean_absolute_error: 17867.0078\n",
            "Epoch 130: val_loss did not improve from 29737.28516\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 17858.1895 - mean_absolute_error: 17858.1895 - val_loss: 30878.1113 - val_mean_absolute_error: 30878.1113\n",
            "Epoch 131/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 18093.2266 - mean_absolute_error: 18093.2266\n",
            "Epoch 131: val_loss improved from 29737.28516 to 29548.34766, saving model to Weights-131--29548.34766.hdf5\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 17995.2676 - mean_absolute_error: 17995.2676 - val_loss: 29548.3477 - val_mean_absolute_error: 29548.3477\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 17620.4492 - mean_absolute_error: 17620.4492\n",
            "Epoch 132: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17620.4492 - mean_absolute_error: 17620.4492 - val_loss: 30532.0430 - val_mean_absolute_error: 30532.0430\n",
            "Epoch 133/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17750.7148 - mean_absolute_error: 17750.7148\n",
            "Epoch 133: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17733.3613 - mean_absolute_error: 17733.3613 - val_loss: 30003.5430 - val_mean_absolute_error: 30003.5430\n",
            "Epoch 134/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 17655.1641 - mean_absolute_error: 17655.1641\n",
            "Epoch 134: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17764.6426 - mean_absolute_error: 17764.6426 - val_loss: 31112.8105 - val_mean_absolute_error: 31112.8105\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 18020.2344 - mean_absolute_error: 18020.2344\n",
            "Epoch 135: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 18020.2344 - mean_absolute_error: 18020.2344 - val_loss: 29905.2324 - val_mean_absolute_error: 29905.2324\n",
            "Epoch 136/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 18438.8359 - mean_absolute_error: 18438.8359\n",
            "Epoch 136: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18376.9668 - mean_absolute_error: 18376.9668 - val_loss: 30107.3887 - val_mean_absolute_error: 30107.3887\n",
            "Epoch 137/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17686.3398 - mean_absolute_error: 17686.3398\n",
            "Epoch 137: val_loss did not improve from 29548.34766\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17690.8242 - mean_absolute_error: 17690.8242 - val_loss: 30443.7812 - val_mean_absolute_error: 30443.7812\n",
            "Epoch 138/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 17966.6133 - mean_absolute_error: 17966.6133\n",
            "Epoch 138: val_loss improved from 29548.34766 to 29367.81641, saving model to Weights-138--29367.81641.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17816.6738 - mean_absolute_error: 17816.6738 - val_loss: 29367.8164 - val_mean_absolute_error: 29367.8164\n",
            "Epoch 139/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 17533.0547 - mean_absolute_error: 17533.0547\n",
            "Epoch 139: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17450.7559 - mean_absolute_error: 17450.7559 - val_loss: 30028.5039 - val_mean_absolute_error: 30028.5039\n",
            "Epoch 140/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 17673.4805 - mean_absolute_error: 17673.4805\n",
            "Epoch 140: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17567.5215 - mean_absolute_error: 17567.5215 - val_loss: 31148.7988 - val_mean_absolute_error: 31148.7988\n",
            "Epoch 141/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 18303.7207 - mean_absolute_error: 18303.7207\n",
            "Epoch 141: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 18268.0859 - mean_absolute_error: 18268.0859 - val_loss: 30499.3125 - val_mean_absolute_error: 30499.3125\n",
            "Epoch 142/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 17753.9531 - mean_absolute_error: 17753.9531\n",
            "Epoch 142: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17550.4629 - mean_absolute_error: 17550.4629 - val_loss: 29677.7344 - val_mean_absolute_error: 29677.7344\n",
            "Epoch 143/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17283.7930 - mean_absolute_error: 17283.7930\n",
            "Epoch 143: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17344.0430 - mean_absolute_error: 17344.0430 - val_loss: 30224.0762 - val_mean_absolute_error: 30224.0762\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 17690.5137 - mean_absolute_error: 17690.5137\n",
            "Epoch 144: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17690.5137 - mean_absolute_error: 17690.5137 - val_loss: 30614.3203 - val_mean_absolute_error: 30614.3203\n",
            "Epoch 145/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17210.1426 - mean_absolute_error: 17210.1426\n",
            "Epoch 145: val_loss did not improve from 29367.81641\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17198.6641 - mean_absolute_error: 17198.6641 - val_loss: 29955.2520 - val_mean_absolute_error: 29955.2520\n",
            "Epoch 146/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17663.1719 - mean_absolute_error: 17663.1719\n",
            "Epoch 146: val_loss improved from 29367.81641 to 29185.43359, saving model to Weights-146--29185.43359.hdf5\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 17686.6816 - mean_absolute_error: 17686.6816 - val_loss: 29185.4336 - val_mean_absolute_error: 29185.4336\n",
            "Epoch 147/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17211.3281 - mean_absolute_error: 17211.3281\n",
            "Epoch 147: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 17409.2207 - mean_absolute_error: 17409.2207 - val_loss: 29976.8926 - val_mean_absolute_error: 29976.8926\n",
            "Epoch 148/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17648.7539 - mean_absolute_error: 17648.7539\n",
            "Epoch 148: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 17594.9160 - mean_absolute_error: 17594.9160 - val_loss: 30372.7852 - val_mean_absolute_error: 30372.7852\n",
            "Epoch 149/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17403.3047 - mean_absolute_error: 17403.3047\n",
            "Epoch 149: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 17299.8320 - mean_absolute_error: 17299.8320 - val_loss: 29845.0254 - val_mean_absolute_error: 29845.0254\n",
            "Epoch 150/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 17348.8613 - mean_absolute_error: 17348.8613\n",
            "Epoch 150: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 17301.8984 - mean_absolute_error: 17301.8984 - val_loss: 29703.8184 - val_mean_absolute_error: 29703.8184\n",
            "Epoch 151/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17326.9102 - mean_absolute_error: 17326.9102\n",
            "Epoch 151: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 17301.5020 - mean_absolute_error: 17301.5020 - val_loss: 29434.9434 - val_mean_absolute_error: 29434.9434\n",
            "Epoch 152/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17377.0059 - mean_absolute_error: 17377.0059\n",
            "Epoch 152: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17436.7266 - mean_absolute_error: 17436.7266 - val_loss: 29655.9941 - val_mean_absolute_error: 29655.9941\n",
            "Epoch 153/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 17560.5547 - mean_absolute_error: 17560.5547\n",
            "Epoch 153: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 17418.1992 - mean_absolute_error: 17418.1992 - val_loss: 30194.3770 - val_mean_absolute_error: 30194.3770\n",
            "Epoch 154/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17492.5469 - mean_absolute_error: 17492.5469\n",
            "Epoch 154: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17560.8438 - mean_absolute_error: 17560.8438 - val_loss: 30005.2402 - val_mean_absolute_error: 30005.2402\n",
            "Epoch 155/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17265.6582 - mean_absolute_error: 17265.6582\n",
            "Epoch 155: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17337.0820 - mean_absolute_error: 17337.0820 - val_loss: 30065.5527 - val_mean_absolute_error: 30065.5527\n",
            "Epoch 156/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16984.1641 - mean_absolute_error: 16984.1641\n",
            "Epoch 156: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16951.6094 - mean_absolute_error: 16951.6094 - val_loss: 30086.4453 - val_mean_absolute_error: 30086.4453\n",
            "Epoch 157/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17362.3203 - mean_absolute_error: 17362.3203\n",
            "Epoch 157: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17454.5254 - mean_absolute_error: 17454.5254 - val_loss: 30203.8750 - val_mean_absolute_error: 30203.8750\n",
            "Epoch 158/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17747.5371 - mean_absolute_error: 17747.5371\n",
            "Epoch 158: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17643.2988 - mean_absolute_error: 17643.2988 - val_loss: 29493.8496 - val_mean_absolute_error: 29493.8496\n",
            "Epoch 159/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 17661.9355 - mean_absolute_error: 17661.9355\n",
            "Epoch 159: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17779.6973 - mean_absolute_error: 17779.6973 - val_loss: 29893.3457 - val_mean_absolute_error: 29893.3457\n",
            "Epoch 160/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 17669.0586 - mean_absolute_error: 17669.0586\n",
            "Epoch 160: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17614.2871 - mean_absolute_error: 17614.2871 - val_loss: 29976.3086 - val_mean_absolute_error: 29976.3086\n",
            "Epoch 161/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 17397.6289 - mean_absolute_error: 17397.6289\n",
            "Epoch 161: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17447.9824 - mean_absolute_error: 17447.9824 - val_loss: 29773.0879 - val_mean_absolute_error: 29773.0879\n",
            "Epoch 162/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17151.1348 - mean_absolute_error: 17151.1348\n",
            "Epoch 162: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17056.2617 - mean_absolute_error: 17056.2617 - val_loss: 30087.2129 - val_mean_absolute_error: 30087.2129\n",
            "Epoch 163/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16785.4922 - mean_absolute_error: 16785.4922\n",
            "Epoch 163: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16784.2930 - mean_absolute_error: 16784.2930 - val_loss: 29577.7812 - val_mean_absolute_error: 29577.7812\n",
            "Epoch 164/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17201.0938 - mean_absolute_error: 17201.0938\n",
            "Epoch 164: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17198.3574 - mean_absolute_error: 17198.3574 - val_loss: 30162.0234 - val_mean_absolute_error: 30162.0234\n",
            "Epoch 165/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 16857.6816 - mean_absolute_error: 16857.6816\n",
            "Epoch 165: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17006.9688 - mean_absolute_error: 17006.9688 - val_loss: 29846.8496 - val_mean_absolute_error: 29846.8496\n",
            "Epoch 166/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17059.3750 - mean_absolute_error: 17059.3750\n",
            "Epoch 166: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17109.0254 - mean_absolute_error: 17109.0254 - val_loss: 29429.2012 - val_mean_absolute_error: 29429.2012\n",
            "Epoch 167/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 16532.7676 - mean_absolute_error: 16532.7676\n",
            "Epoch 167: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16682.5820 - mean_absolute_error: 16682.5820 - val_loss: 30174.4199 - val_mean_absolute_error: 30174.4199\n",
            "Epoch 168/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17671.0098 - mean_absolute_error: 17671.0098\n",
            "Epoch 168: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17656.3359 - mean_absolute_error: 17656.3359 - val_loss: 29873.2520 - val_mean_absolute_error: 29873.2520\n",
            "Epoch 169/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16745.4414 - mean_absolute_error: 16745.4414\n",
            "Epoch 169: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16774.9902 - mean_absolute_error: 16774.9902 - val_loss: 30364.5352 - val_mean_absolute_error: 30364.5352\n",
            "Epoch 170/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17342.7793 - mean_absolute_error: 17342.7793\n",
            "Epoch 170: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 17244.6074 - mean_absolute_error: 17244.6074 - val_loss: 29458.0820 - val_mean_absolute_error: 29458.0820\n",
            "Epoch 171/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17031.8574 - mean_absolute_error: 17031.8574\n",
            "Epoch 171: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 17073.8066 - mean_absolute_error: 17073.8066 - val_loss: 29709.4707 - val_mean_absolute_error: 29709.4707\n",
            "Epoch 172/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16836.8281 - mean_absolute_error: 16836.8281\n",
            "Epoch 172: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 16841.6543 - mean_absolute_error: 16841.6543 - val_loss: 29794.8320 - val_mean_absolute_error: 29794.8320\n",
            "Epoch 173/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16789.6875 - mean_absolute_error: 16789.6875\n",
            "Epoch 173: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16834.0703 - mean_absolute_error: 16834.0703 - val_loss: 29429.6875 - val_mean_absolute_error: 29429.6875\n",
            "Epoch 174/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17035.4473 - mean_absolute_error: 17035.4473\n",
            "Epoch 174: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16954.4648 - mean_absolute_error: 16954.4648 - val_loss: 29822.5430 - val_mean_absolute_error: 29822.5430\n",
            "Epoch 175/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17300.8730 - mean_absolute_error: 17300.8730\n",
            "Epoch 175: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17280.3906 - mean_absolute_error: 17280.3906 - val_loss: 30260.7812 - val_mean_absolute_error: 30260.7812\n",
            "Epoch 176/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17304.3242 - mean_absolute_error: 17304.3242\n",
            "Epoch 176: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17336.4805 - mean_absolute_error: 17336.4805 - val_loss: 30029.8809 - val_mean_absolute_error: 30029.8809\n",
            "Epoch 177/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17490.7578 - mean_absolute_error: 17490.7578\n",
            "Epoch 177: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17500.5156 - mean_absolute_error: 17500.5156 - val_loss: 30157.2012 - val_mean_absolute_error: 30157.2012\n",
            "Epoch 178/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 17405.9375 - mean_absolute_error: 17405.9375\n",
            "Epoch 178: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17408.2617 - mean_absolute_error: 17408.2617 - val_loss: 29487.2500 - val_mean_absolute_error: 29487.2500\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 17000.9102 - mean_absolute_error: 17000.9102\n",
            "Epoch 179: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17000.9102 - mean_absolute_error: 17000.9102 - val_loss: 30776.5527 - val_mean_absolute_error: 30776.5527\n",
            "Epoch 180/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 17384.9336 - mean_absolute_error: 17384.9336\n",
            "Epoch 180: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17364.9219 - mean_absolute_error: 17364.9219 - val_loss: 29799.4277 - val_mean_absolute_error: 29799.4277\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16995.3613 - mean_absolute_error: 16995.3613\n",
            "Epoch 181: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16995.3613 - mean_absolute_error: 16995.3613 - val_loss: 30459.9414 - val_mean_absolute_error: 30459.9414\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16446.4160 - mean_absolute_error: 16446.4160\n",
            "Epoch 182: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16446.4160 - mean_absolute_error: 16446.4160 - val_loss: 29794.7402 - val_mean_absolute_error: 29794.7402\n",
            "Epoch 183/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16925.1973 - mean_absolute_error: 16925.1973\n",
            "Epoch 183: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17091.5684 - mean_absolute_error: 17091.5684 - val_loss: 29228.3027 - val_mean_absolute_error: 29228.3027\n",
            "Epoch 184/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 16569.0684 - mean_absolute_error: 16569.0684\n",
            "Epoch 184: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16646.3730 - mean_absolute_error: 16646.3730 - val_loss: 29300.7559 - val_mean_absolute_error: 29300.7559\n",
            "Epoch 185/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 17144.8867 - mean_absolute_error: 17144.8867\n",
            "Epoch 185: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17202.1016 - mean_absolute_error: 17202.1016 - val_loss: 30359.8379 - val_mean_absolute_error: 30359.8379\n",
            "Epoch 186/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16723.7129 - mean_absolute_error: 16723.7129\n",
            "Epoch 186: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16670.7539 - mean_absolute_error: 16670.7539 - val_loss: 29684.0449 - val_mean_absolute_error: 29684.0449\n",
            "Epoch 187/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17140.4961 - mean_absolute_error: 17140.4961\n",
            "Epoch 187: val_loss did not improve from 29185.43359\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 17068.1211 - mean_absolute_error: 17068.1211 - val_loss: 29203.8848 - val_mean_absolute_error: 29203.8848\n",
            "Epoch 188/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 16653.2441 - mean_absolute_error: 16653.2441\n",
            "Epoch 188: val_loss improved from 29185.43359 to 29131.96680, saving model to Weights-188--29131.96680.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16640.9492 - mean_absolute_error: 16640.9492 - val_loss: 29131.9668 - val_mean_absolute_error: 29131.9668\n",
            "Epoch 189/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 16544.9473 - mean_absolute_error: 16544.9473\n",
            "Epoch 189: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16426.5391 - mean_absolute_error: 16426.5391 - val_loss: 29308.0566 - val_mean_absolute_error: 29308.0566\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16753.5859 - mean_absolute_error: 16753.5859\n",
            "Epoch 190: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16753.5859 - mean_absolute_error: 16753.5859 - val_loss: 29738.1641 - val_mean_absolute_error: 29738.1641\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16643.3145 - mean_absolute_error: 16643.3145\n",
            "Epoch 191: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16643.3145 - mean_absolute_error: 16643.3145 - val_loss: 29588.7051 - val_mean_absolute_error: 29588.7051\n",
            "Epoch 192/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16259.4072 - mean_absolute_error: 16259.4072\n",
            "Epoch 192: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16311.1465 - mean_absolute_error: 16311.1465 - val_loss: 29361.4531 - val_mean_absolute_error: 29361.4531\n",
            "Epoch 193/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16596.2637 - mean_absolute_error: 16596.2637\n",
            "Epoch 193: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16544.4902 - mean_absolute_error: 16544.4902 - val_loss: 29549.2266 - val_mean_absolute_error: 29549.2266\n",
            "Epoch 194/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16526.0781 - mean_absolute_error: 16526.0781\n",
            "Epoch 194: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 16466.3672 - mean_absolute_error: 16466.3672 - val_loss: 29898.3828 - val_mean_absolute_error: 29898.3828\n",
            "Epoch 195/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17114.3105 - mean_absolute_error: 17114.3105\n",
            "Epoch 195: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 17113.5820 - mean_absolute_error: 17113.5820 - val_loss: 31194.6562 - val_mean_absolute_error: 31194.6562\n",
            "Epoch 196/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 16941.8516 - mean_absolute_error: 16941.8516\n",
            "Epoch 196: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16775.4551 - mean_absolute_error: 16775.4551 - val_loss: 29695.6875 - val_mean_absolute_error: 29695.6875\n",
            "Epoch 197/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16512.6797 - mean_absolute_error: 16512.6797\n",
            "Epoch 197: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16506.9219 - mean_absolute_error: 16506.9219 - val_loss: 29599.0898 - val_mean_absolute_error: 29599.0898\n",
            "Epoch 198/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16462.9375 - mean_absolute_error: 16462.9375\n",
            "Epoch 198: val_loss did not improve from 29131.96680\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16436.7148 - mean_absolute_error: 16436.7148 - val_loss: 30476.8184 - val_mean_absolute_error: 30476.8184\n",
            "Epoch 199/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 17085.8125 - mean_absolute_error: 17085.8125\n",
            "Epoch 199: val_loss improved from 29131.96680 to 29043.86719, saving model to Weights-199--29043.86719.hdf5\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 17142.8809 - mean_absolute_error: 17142.8809 - val_loss: 29043.8672 - val_mean_absolute_error: 29043.8672\n",
            "Epoch 200/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16670.4883 - mean_absolute_error: 16670.4883\n",
            "Epoch 200: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16640.8789 - mean_absolute_error: 16640.8789 - val_loss: 29396.9375 - val_mean_absolute_error: 29396.9375\n",
            "Epoch 201/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16671.8965 - mean_absolute_error: 16671.8965\n",
            "Epoch 201: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16678.4551 - mean_absolute_error: 16678.4551 - val_loss: 29282.1133 - val_mean_absolute_error: 29282.1133\n",
            "Epoch 202/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16189.5479 - mean_absolute_error: 16189.5479\n",
            "Epoch 202: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16219.0732 - mean_absolute_error: 16219.0732 - val_loss: 29929.7969 - val_mean_absolute_error: 29929.7969\n",
            "Epoch 203/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 17015.4141 - mean_absolute_error: 17015.4141\n",
            "Epoch 203: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 17070.4902 - mean_absolute_error: 17070.4902 - val_loss: 29547.1582 - val_mean_absolute_error: 29547.1582\n",
            "Epoch 204/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 17096.3301 - mean_absolute_error: 17096.3301\n",
            "Epoch 204: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 17049.1191 - mean_absolute_error: 17049.1191 - val_loss: 29954.9863 - val_mean_absolute_error: 29954.9863\n",
            "Epoch 205/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16132.4551 - mean_absolute_error: 16132.4551\n",
            "Epoch 205: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16103.4873 - mean_absolute_error: 16103.4873 - val_loss: 29956.9434 - val_mean_absolute_error: 29956.9434\n",
            "Epoch 206/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16281.7412 - mean_absolute_error: 16281.7412\n",
            "Epoch 206: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16226.6621 - mean_absolute_error: 16226.6621 - val_loss: 29301.2188 - val_mean_absolute_error: 29301.2188\n",
            "Epoch 207/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16440.0449 - mean_absolute_error: 16440.0449\n",
            "Epoch 207: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16555.8809 - mean_absolute_error: 16555.8809 - val_loss: 30089.8672 - val_mean_absolute_error: 30089.8672\n",
            "Epoch 208/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16310.6250 - mean_absolute_error: 16310.6250\n",
            "Epoch 208: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16347.0811 - mean_absolute_error: 16347.0811 - val_loss: 29463.7109 - val_mean_absolute_error: 29463.7109\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16347.5098 - mean_absolute_error: 16347.5098\n",
            "Epoch 209: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16347.5098 - mean_absolute_error: 16347.5098 - val_loss: 29640.8945 - val_mean_absolute_error: 29640.8945\n",
            "Epoch 210/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16362.6924 - mean_absolute_error: 16362.6924\n",
            "Epoch 210: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16513.5000 - mean_absolute_error: 16513.5000 - val_loss: 29429.2031 - val_mean_absolute_error: 29429.2031\n",
            "Epoch 211/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16254.9707 - mean_absolute_error: 16254.9707\n",
            "Epoch 211: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16273.4443 - mean_absolute_error: 16273.4443 - val_loss: 29187.0625 - val_mean_absolute_error: 29187.0625\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16199.4092 - mean_absolute_error: 16199.4092\n",
            "Epoch 212: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 16199.4092 - mean_absolute_error: 16199.4092 - val_loss: 30014.9941 - val_mean_absolute_error: 30014.9941\n",
            "Epoch 213/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16075.1045 - mean_absolute_error: 16075.1045\n",
            "Epoch 213: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16173.2744 - mean_absolute_error: 16173.2744 - val_loss: 30060.8809 - val_mean_absolute_error: 30060.8809\n",
            "Epoch 214/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16844.4727 - mean_absolute_error: 16844.4727\n",
            "Epoch 214: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 16811.2578 - mean_absolute_error: 16811.2578 - val_loss: 29851.3320 - val_mean_absolute_error: 29851.3320\n",
            "Epoch 215/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16194.3760 - mean_absolute_error: 16194.3760\n",
            "Epoch 215: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16223.6865 - mean_absolute_error: 16223.6865 - val_loss: 29717.5781 - val_mean_absolute_error: 29717.5781\n",
            "Epoch 216/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16436.2480 - mean_absolute_error: 16436.2480\n",
            "Epoch 216: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 16345.3691 - mean_absolute_error: 16345.3691 - val_loss: 29444.8359 - val_mean_absolute_error: 29444.8359\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16324.5029 - mean_absolute_error: 16324.5029\n",
            "Epoch 217: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16324.5029 - mean_absolute_error: 16324.5029 - val_loss: 29849.5898 - val_mean_absolute_error: 29849.5898\n",
            "Epoch 218/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 16330.8789 - mean_absolute_error: 16330.8789\n",
            "Epoch 218: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16427.5020 - mean_absolute_error: 16427.5020 - val_loss: 29753.3398 - val_mean_absolute_error: 29753.3398\n",
            "Epoch 219/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 16125.2158 - mean_absolute_error: 16125.2158\n",
            "Epoch 219: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16077.4688 - mean_absolute_error: 16077.4688 - val_loss: 29388.5781 - val_mean_absolute_error: 29388.5781\n",
            "Epoch 220/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16585.3379 - mean_absolute_error: 16585.3379\n",
            "Epoch 220: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16538.1738 - mean_absolute_error: 16538.1738 - val_loss: 29881.4023 - val_mean_absolute_error: 29881.4023\n",
            "Epoch 221/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16483.4082 - mean_absolute_error: 16483.4082\n",
            "Epoch 221: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16571.8262 - mean_absolute_error: 16571.8262 - val_loss: 30487.2500 - val_mean_absolute_error: 30487.2500\n",
            "Epoch 222/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16272.9248 - mean_absolute_error: 16272.9248\n",
            "Epoch 222: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16260.7822 - mean_absolute_error: 16260.7822 - val_loss: 29420.1953 - val_mean_absolute_error: 29420.1953\n",
            "Epoch 223/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 16114.9062 - mean_absolute_error: 16114.9062\n",
            "Epoch 223: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16174.9141 - mean_absolute_error: 16174.9141 - val_loss: 29804.9883 - val_mean_absolute_error: 29804.9883\n",
            "Epoch 224/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16524.3711 - mean_absolute_error: 16524.3711\n",
            "Epoch 224: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16429.3574 - mean_absolute_error: 16429.3574 - val_loss: 30913.7227 - val_mean_absolute_error: 30913.7227\n",
            "Epoch 225/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16245.3740 - mean_absolute_error: 16245.3740\n",
            "Epoch 225: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16245.4404 - mean_absolute_error: 16245.4404 - val_loss: 30067.0059 - val_mean_absolute_error: 30067.0059\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15978.4492 - mean_absolute_error: 15978.4492\n",
            "Epoch 226: val_loss did not improve from 29043.86719\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15978.4492 - mean_absolute_error: 15978.4492 - val_loss: 29426.9375 - val_mean_absolute_error: 29426.9375\n",
            "Epoch 227/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15751.0986 - mean_absolute_error: 15751.0986\n",
            "Epoch 227: val_loss improved from 29043.86719 to 28844.82812, saving model to Weights-227--28844.82812.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15806.2334 - mean_absolute_error: 15806.2334 - val_loss: 28844.8281 - val_mean_absolute_error: 28844.8281\n",
            "Epoch 228/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15863.8672 - mean_absolute_error: 15863.8672\n",
            "Epoch 228: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15848.4072 - mean_absolute_error: 15848.4072 - val_loss: 29274.7168 - val_mean_absolute_error: 29274.7168\n",
            "Epoch 229/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 15922.3906 - mean_absolute_error: 15922.3906\n",
            "Epoch 229: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15867.8779 - mean_absolute_error: 15867.8779 - val_loss: 29528.4863 - val_mean_absolute_error: 29528.4863\n",
            "Epoch 230/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15534.8965 - mean_absolute_error: 15534.8965\n",
            "Epoch 230: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15738.8633 - mean_absolute_error: 15738.8633 - val_loss: 30388.3848 - val_mean_absolute_error: 30388.3848\n",
            "Epoch 231/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15797.1084 - mean_absolute_error: 15797.1084\n",
            "Epoch 231: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15813.7363 - mean_absolute_error: 15813.7363 - val_loss: 28992.5391 - val_mean_absolute_error: 28992.5391\n",
            "Epoch 232/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 16135.8018 - mean_absolute_error: 16135.8018\n",
            "Epoch 232: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16325.4316 - mean_absolute_error: 16325.4316 - val_loss: 29946.9121 - val_mean_absolute_error: 29946.9121\n",
            "Epoch 233/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16222.8701 - mean_absolute_error: 16222.8701\n",
            "Epoch 233: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 16420.0586 - mean_absolute_error: 16420.0586 - val_loss: 29926.4531 - val_mean_absolute_error: 29926.4531\n",
            "Epoch 234/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15992.8984 - mean_absolute_error: 15992.8984\n",
            "Epoch 234: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 15883.1826 - mean_absolute_error: 15883.1826 - val_loss: 29505.6465 - val_mean_absolute_error: 29505.6465\n",
            "Epoch 235/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15683.9004 - mean_absolute_error: 15683.9004\n",
            "Epoch 235: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15639.8564 - mean_absolute_error: 15639.8564 - val_loss: 29628.1113 - val_mean_absolute_error: 29628.1113\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15570.6133 - mean_absolute_error: 15570.6133\n",
            "Epoch 236: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 15570.6133 - mean_absolute_error: 15570.6133 - val_loss: 30865.3535 - val_mean_absolute_error: 30865.3535\n",
            "Epoch 237/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16188.6240 - mean_absolute_error: 16188.6240\n",
            "Epoch 237: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 16169.9551 - mean_absolute_error: 16169.9551 - val_loss: 29547.6367 - val_mean_absolute_error: 29547.6367\n",
            "Epoch 238/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 15499.4443 - mean_absolute_error: 15499.4443\n",
            "Epoch 238: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15262.8066 - mean_absolute_error: 15262.8066 - val_loss: 29432.7246 - val_mean_absolute_error: 29432.7246\n",
            "Epoch 239/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 16014.5352 - mean_absolute_error: 16014.5352\n",
            "Epoch 239: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16152.3916 - mean_absolute_error: 16152.3916 - val_loss: 30284.5586 - val_mean_absolute_error: 30284.5586\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 16282.5244 - mean_absolute_error: 16282.5244\n",
            "Epoch 240: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16282.5244 - mean_absolute_error: 16282.5244 - val_loss: 29170.2715 - val_mean_absolute_error: 29170.2715\n",
            "Epoch 241/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16212.8574 - mean_absolute_error: 16212.8574\n",
            "Epoch 241: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16138.0762 - mean_absolute_error: 16138.0762 - val_loss: 30595.1465 - val_mean_absolute_error: 30595.1465\n",
            "Epoch 242/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15883.7461 - mean_absolute_error: 15883.7461\n",
            "Epoch 242: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15893.5068 - mean_absolute_error: 15893.5068 - val_loss: 29620.7305 - val_mean_absolute_error: 29620.7305\n",
            "Epoch 243/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16187.2344 - mean_absolute_error: 16187.2344\n",
            "Epoch 243: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 16180.3867 - mean_absolute_error: 16180.3867 - val_loss: 29638.7188 - val_mean_absolute_error: 29638.7188\n",
            "Epoch 244/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 16111.2969 - mean_absolute_error: 16111.2969\n",
            "Epoch 244: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15985.9492 - mean_absolute_error: 15985.9492 - val_loss: 29755.3027 - val_mean_absolute_error: 29755.3027\n",
            "Epoch 245/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15691.6006 - mean_absolute_error: 15691.6006\n",
            "Epoch 245: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15654.1689 - mean_absolute_error: 15654.1689 - val_loss: 29488.9492 - val_mean_absolute_error: 29488.9492\n",
            "Epoch 246/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 15586.9580 - mean_absolute_error: 15586.9580\n",
            "Epoch 246: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15569.4502 - mean_absolute_error: 15569.4502 - val_loss: 30317.3887 - val_mean_absolute_error: 30317.3887\n",
            "Epoch 247/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 16167.5205 - mean_absolute_error: 16167.5205\n",
            "Epoch 247: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16084.1719 - mean_absolute_error: 16084.1719 - val_loss: 29528.9414 - val_mean_absolute_error: 29528.9414\n",
            "Epoch 248/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15484.2930 - mean_absolute_error: 15484.2930\n",
            "Epoch 248: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15569.1084 - mean_absolute_error: 15569.1084 - val_loss: 30036.6934 - val_mean_absolute_error: 30036.6934\n",
            "Epoch 249/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 16022.8877 - mean_absolute_error: 16022.8877\n",
            "Epoch 249: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16026.3242 - mean_absolute_error: 16026.3242 - val_loss: 29000.0430 - val_mean_absolute_error: 29000.0430\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15749.7588 - mean_absolute_error: 15749.7588\n",
            "Epoch 250: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15749.7588 - mean_absolute_error: 15749.7588 - val_loss: 29507.2383 - val_mean_absolute_error: 29507.2383\n",
            "Epoch 251/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 15580.1299 - mean_absolute_error: 15580.1299\n",
            "Epoch 251: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15617.2217 - mean_absolute_error: 15617.2217 - val_loss: 29313.6309 - val_mean_absolute_error: 29313.6309\n",
            "Epoch 252/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 16100.1436 - mean_absolute_error: 16100.1436\n",
            "Epoch 252: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 16215.8691 - mean_absolute_error: 16215.8691 - val_loss: 29300.3086 - val_mean_absolute_error: 29300.3086\n",
            "Epoch 253/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15594.6318 - mean_absolute_error: 15594.6318\n",
            "Epoch 253: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 15587.0967 - mean_absolute_error: 15587.0967 - val_loss: 29639.6621 - val_mean_absolute_error: 29639.6621\n",
            "Epoch 254/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15783.6074 - mean_absolute_error: 15783.6074\n",
            "Epoch 254: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15768.3262 - mean_absolute_error: 15768.3262 - val_loss: 30112.8887 - val_mean_absolute_error: 30112.8887\n",
            "Epoch 255/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 16257.8467 - mean_absolute_error: 16257.8467\n",
            "Epoch 255: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 16256.7188 - mean_absolute_error: 16256.7188 - val_loss: 29620.8008 - val_mean_absolute_error: 29620.8008\n",
            "Epoch 256/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15841.6738 - mean_absolute_error: 15841.6738\n",
            "Epoch 256: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15890.8262 - mean_absolute_error: 15890.8262 - val_loss: 29608.1113 - val_mean_absolute_error: 29608.1113\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15536.8506 - mean_absolute_error: 15536.8506\n",
            "Epoch 257: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15536.8506 - mean_absolute_error: 15536.8506 - val_loss: 29963.3164 - val_mean_absolute_error: 29963.3164\n",
            "Epoch 258/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15818.3887 - mean_absolute_error: 15818.3887\n",
            "Epoch 258: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15770.0449 - mean_absolute_error: 15770.0449 - val_loss: 30707.7812 - val_mean_absolute_error: 30707.7812\n",
            "Epoch 259/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15970.2559 - mean_absolute_error: 15970.2559\n",
            "Epoch 259: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15974.2568 - mean_absolute_error: 15974.2568 - val_loss: 28852.2090 - val_mean_absolute_error: 28852.2090\n",
            "Epoch 260/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15848.5098 - mean_absolute_error: 15848.5098\n",
            "Epoch 260: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15819.1895 - mean_absolute_error: 15819.1895 - val_loss: 30404.6523 - val_mean_absolute_error: 30404.6523\n",
            "Epoch 261/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15615.8340 - mean_absolute_error: 15615.8340\n",
            "Epoch 261: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15569.4785 - mean_absolute_error: 15569.4785 - val_loss: 29605.6543 - val_mean_absolute_error: 29605.6543\n",
            "Epoch 262/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15304.0137 - mean_absolute_error: 15304.0137\n",
            "Epoch 262: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15344.3652 - mean_absolute_error: 15344.3652 - val_loss: 29571.9512 - val_mean_absolute_error: 29571.9512\n",
            "Epoch 263/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15812.0557 - mean_absolute_error: 15812.0557\n",
            "Epoch 263: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15781.2217 - mean_absolute_error: 15781.2217 - val_loss: 29423.9668 - val_mean_absolute_error: 29423.9668\n",
            "Epoch 264/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 15810.1299 - mean_absolute_error: 15810.1299\n",
            "Epoch 264: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15826.9004 - mean_absolute_error: 15826.9004 - val_loss: 29327.3965 - val_mean_absolute_error: 29327.3965\n",
            "Epoch 265/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15514.8457 - mean_absolute_error: 15514.8457\n",
            "Epoch 265: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15780.3486 - mean_absolute_error: 15780.3486 - val_loss: 30085.7793 - val_mean_absolute_error: 30085.7793\n",
            "Epoch 266/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15831.1758 - mean_absolute_error: 15831.1758\n",
            "Epoch 266: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15732.3330 - mean_absolute_error: 15732.3330 - val_loss: 29519.4453 - val_mean_absolute_error: 29519.4453\n",
            "Epoch 267/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 15210.3877 - mean_absolute_error: 15210.3877\n",
            "Epoch 267: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15456.4873 - mean_absolute_error: 15456.4873 - val_loss: 29998.7305 - val_mean_absolute_error: 29998.7305\n",
            "Epoch 268/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15745.9932 - mean_absolute_error: 15745.9932\n",
            "Epoch 268: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15706.6660 - mean_absolute_error: 15706.6660 - val_loss: 30073.0137 - val_mean_absolute_error: 30073.0137\n",
            "Epoch 269/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15498.9951 - mean_absolute_error: 15498.9951\n",
            "Epoch 269: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15527.5205 - mean_absolute_error: 15527.5205 - val_loss: 29645.8320 - val_mean_absolute_error: 29645.8320\n",
            "Epoch 270/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15544.8389 - mean_absolute_error: 15544.8389\n",
            "Epoch 270: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15572.8203 - mean_absolute_error: 15572.8203 - val_loss: 30771.0762 - val_mean_absolute_error: 30771.0762\n",
            "Epoch 271/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15909.1963 - mean_absolute_error: 15909.1963\n",
            "Epoch 271: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15877.3936 - mean_absolute_error: 15877.3936 - val_loss: 30369.4102 - val_mean_absolute_error: 30369.4102\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15436.1592 - mean_absolute_error: 15436.1592\n",
            "Epoch 272: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15436.1592 - mean_absolute_error: 15436.1592 - val_loss: 29170.9941 - val_mean_absolute_error: 29170.9941\n",
            "Epoch 273/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15745.2480 - mean_absolute_error: 15745.2480\n",
            "Epoch 273: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 16042.7285 - mean_absolute_error: 16042.7285 - val_loss: 29867.2520 - val_mean_absolute_error: 29867.2520\n",
            "Epoch 274/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15352.4150 - mean_absolute_error: 15352.4150\n",
            "Epoch 274: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15352.2383 - mean_absolute_error: 15352.2383 - val_loss: 30104.2012 - val_mean_absolute_error: 30104.2012\n",
            "Epoch 275/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15696.0586 - mean_absolute_error: 15696.0586\n",
            "Epoch 275: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15590.6270 - mean_absolute_error: 15590.6270 - val_loss: 29560.5293 - val_mean_absolute_error: 29560.5293\n",
            "Epoch 276/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15529.8096 - mean_absolute_error: 15529.8096\n",
            "Epoch 276: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15513.5137 - mean_absolute_error: 15513.5137 - val_loss: 29764.4023 - val_mean_absolute_error: 29764.4023\n",
            "Epoch 277/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15546.2852 - mean_absolute_error: 15546.2852\n",
            "Epoch 277: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15564.2109 - mean_absolute_error: 15564.2109 - val_loss: 29671.2910 - val_mean_absolute_error: 29671.2910\n",
            "Epoch 278/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15476.5498 - mean_absolute_error: 15476.5498\n",
            "Epoch 278: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 15493.5850 - mean_absolute_error: 15493.5850 - val_loss: 29847.5605 - val_mean_absolute_error: 29847.5605\n",
            "Epoch 279/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15420.2402 - mean_absolute_error: 15420.2402\n",
            "Epoch 279: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15451.8457 - mean_absolute_error: 15451.8457 - val_loss: 29545.6738 - val_mean_absolute_error: 29545.6738\n",
            "Epoch 280/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15299.0508 - mean_absolute_error: 15299.0508\n",
            "Epoch 280: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15327.4697 - mean_absolute_error: 15327.4697 - val_loss: 30095.0879 - val_mean_absolute_error: 30095.0879\n",
            "Epoch 281/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15422.0205 - mean_absolute_error: 15422.0205\n",
            "Epoch 281: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15369.4639 - mean_absolute_error: 15369.4639 - val_loss: 29481.2910 - val_mean_absolute_error: 29481.2910\n",
            "Epoch 282/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 15441.8906 - mean_absolute_error: 15441.8906\n",
            "Epoch 282: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15561.6035 - mean_absolute_error: 15561.6035 - val_loss: 29895.0488 - val_mean_absolute_error: 29895.0488\n",
            "Epoch 283/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15395.4912 - mean_absolute_error: 15395.4912\n",
            "Epoch 283: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15398.4678 - mean_absolute_error: 15398.4678 - val_loss: 29595.1445 - val_mean_absolute_error: 29595.1445\n",
            "Epoch 284/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15209.8711 - mean_absolute_error: 15209.8711\n",
            "Epoch 284: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15196.9834 - mean_absolute_error: 15196.9834 - val_loss: 29214.0801 - val_mean_absolute_error: 29214.0801\n",
            "Epoch 285/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 15603.0059 - mean_absolute_error: 15603.0059\n",
            "Epoch 285: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15473.9814 - mean_absolute_error: 15473.9814 - val_loss: 29474.6211 - val_mean_absolute_error: 29474.6211\n",
            "Epoch 286/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15183.9561 - mean_absolute_error: 15183.9561\n",
            "Epoch 286: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15226.3164 - mean_absolute_error: 15226.3164 - val_loss: 29628.4512 - val_mean_absolute_error: 29628.4512\n",
            "Epoch 287/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15294.6289 - mean_absolute_error: 15294.6289\n",
            "Epoch 287: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15284.7842 - mean_absolute_error: 15284.7842 - val_loss: 29275.1113 - val_mean_absolute_error: 29275.1113\n",
            "Epoch 288/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 14891.5156 - mean_absolute_error: 14891.5156\n",
            "Epoch 288: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15032.7012 - mean_absolute_error: 15032.7012 - val_loss: 30291.4590 - val_mean_absolute_error: 30291.4590\n",
            "Epoch 289/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 16163.1934 - mean_absolute_error: 16163.1934\n",
            "Epoch 289: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 16195.4385 - mean_absolute_error: 16195.4385 - val_loss: 29568.5156 - val_mean_absolute_error: 29568.5156\n",
            "Epoch 290/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14972.1504 - mean_absolute_error: 14972.1504\n",
            "Epoch 290: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15107.3906 - mean_absolute_error: 15107.3906 - val_loss: 29742.2383 - val_mean_absolute_error: 29742.2383\n",
            "Epoch 291/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 15570.4473 - mean_absolute_error: 15570.4473\n",
            "Epoch 291: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15457.6455 - mean_absolute_error: 15457.6455 - val_loss: 29510.4414 - val_mean_absolute_error: 29510.4414\n",
            "Epoch 292/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15794.3428 - mean_absolute_error: 15794.3428\n",
            "Epoch 292: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15816.4092 - mean_absolute_error: 15816.4092 - val_loss: 29241.8281 - val_mean_absolute_error: 29241.8281\n",
            "Epoch 293/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15337.1719 - mean_absolute_error: 15337.1719\n",
            "Epoch 293: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15455.8799 - mean_absolute_error: 15455.8799 - val_loss: 30255.7305 - val_mean_absolute_error: 30255.7305\n",
            "Epoch 294/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 15249.0400 - mean_absolute_error: 15249.0400\n",
            "Epoch 294: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15306.4990 - mean_absolute_error: 15306.4990 - val_loss: 29172.9180 - val_mean_absolute_error: 29172.9180\n",
            "Epoch 295/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15286.8135 - mean_absolute_error: 15286.8135\n",
            "Epoch 295: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15293.0635 - mean_absolute_error: 15293.0635 - val_loss: 29912.6719 - val_mean_absolute_error: 29912.6719\n",
            "Epoch 296/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15221.8467 - mean_absolute_error: 15221.8467\n",
            "Epoch 296: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15238.8066 - mean_absolute_error: 15238.8066 - val_loss: 29557.5723 - val_mean_absolute_error: 29557.5723\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 15316.5312 - mean_absolute_error: 15316.5312\n",
            "Epoch 297: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 15316.5312 - mean_absolute_error: 15316.5312 - val_loss: 30082.0488 - val_mean_absolute_error: 30082.0488\n",
            "Epoch 298/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15350.0869 - mean_absolute_error: 15350.0869\n",
            "Epoch 298: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15351.7490 - mean_absolute_error: 15351.7490 - val_loss: 29562.0059 - val_mean_absolute_error: 29562.0059\n",
            "Epoch 299/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15288.9824 - mean_absolute_error: 15288.9824\n",
            "Epoch 299: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 15242.2285 - mean_absolute_error: 15242.2285 - val_loss: 29727.7656 - val_mean_absolute_error: 29727.7656\n",
            "Epoch 300/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15119.8135 - mean_absolute_error: 15119.8135\n",
            "Epoch 300: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 15120.6914 - mean_absolute_error: 15120.6914 - val_loss: 29501.2832 - val_mean_absolute_error: 29501.2832\n",
            "Epoch 301/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15246.8369 - mean_absolute_error: 15246.8369\n",
            "Epoch 301: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15338.4453 - mean_absolute_error: 15338.4453 - val_loss: 30036.0371 - val_mean_absolute_error: 30036.0371\n",
            "Epoch 302/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15525.4365 - mean_absolute_error: 15525.4365\n",
            "Epoch 302: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15507.8906 - mean_absolute_error: 15507.8906 - val_loss: 29635.9238 - val_mean_absolute_error: 29635.9238\n",
            "Epoch 303/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15064.2949 - mean_absolute_error: 15064.2949\n",
            "Epoch 303: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15028.7480 - mean_absolute_error: 15028.7480 - val_loss: 29952.6562 - val_mean_absolute_error: 29952.6562\n",
            "Epoch 304/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 15844.6270 - mean_absolute_error: 15844.6270\n",
            "Epoch 304: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15915.9902 - mean_absolute_error: 15915.9902 - val_loss: 30358.9824 - val_mean_absolute_error: 30358.9824\n",
            "Epoch 305/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15280.4551 - mean_absolute_error: 15280.4551\n",
            "Epoch 305: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 15348.4404 - mean_absolute_error: 15348.4404 - val_loss: 29894.1074 - val_mean_absolute_error: 29894.1074\n",
            "Epoch 306/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 14936.9980 - mean_absolute_error: 14936.9980\n",
            "Epoch 306: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15108.7148 - mean_absolute_error: 15108.7148 - val_loss: 29539.2520 - val_mean_absolute_error: 29539.2520\n",
            "Epoch 307/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15033.3027 - mean_absolute_error: 15033.3027\n",
            "Epoch 307: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15167.7070 - mean_absolute_error: 15167.7070 - val_loss: 29505.4785 - val_mean_absolute_error: 29505.4785\n",
            "Epoch 308/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15051.5000 - mean_absolute_error: 15051.5000\n",
            "Epoch 308: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15197.8174 - mean_absolute_error: 15197.8174 - val_loss: 30934.7363 - val_mean_absolute_error: 30934.7363\n",
            "Epoch 309/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 15214.2334 - mean_absolute_error: 15214.2334\n",
            "Epoch 309: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15200.8887 - mean_absolute_error: 15200.8887 - val_loss: 29962.8164 - val_mean_absolute_error: 29962.8164\n",
            "Epoch 310/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15546.0693 - mean_absolute_error: 15546.0693\n",
            "Epoch 310: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15552.6729 - mean_absolute_error: 15552.6729 - val_loss: 29923.4609 - val_mean_absolute_error: 29923.4609\n",
            "Epoch 311/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14929.7002 - mean_absolute_error: 14929.7002\n",
            "Epoch 311: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14939.9189 - mean_absolute_error: 14939.9189 - val_loss: 29609.7285 - val_mean_absolute_error: 29609.7285\n",
            "Epoch 312/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15169.6826 - mean_absolute_error: 15169.6826\n",
            "Epoch 312: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15142.3945 - mean_absolute_error: 15142.3945 - val_loss: 30012.3848 - val_mean_absolute_error: 30012.3848\n",
            "Epoch 313/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14780.3232 - mean_absolute_error: 14780.3232\n",
            "Epoch 313: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14756.7324 - mean_absolute_error: 14756.7324 - val_loss: 30262.9180 - val_mean_absolute_error: 30262.9180\n",
            "Epoch 314/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14888.9941 - mean_absolute_error: 14888.9941\n",
            "Epoch 314: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14890.8574 - mean_absolute_error: 14890.8574 - val_loss: 30064.5137 - val_mean_absolute_error: 30064.5137\n",
            "Epoch 315/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14693.2285 - mean_absolute_error: 14693.2285\n",
            "Epoch 315: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14782.8906 - mean_absolute_error: 14782.8906 - val_loss: 29824.3965 - val_mean_absolute_error: 29824.3965\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14923.1182 - mean_absolute_error: 14923.1182\n",
            "Epoch 316: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14923.1182 - mean_absolute_error: 14923.1182 - val_loss: 29687.6035 - val_mean_absolute_error: 29687.6035\n",
            "Epoch 317/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14778.4756 - mean_absolute_error: 14778.4756\n",
            "Epoch 317: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14914.2949 - mean_absolute_error: 14914.2949 - val_loss: 29391.3906 - val_mean_absolute_error: 29391.3906\n",
            "Epoch 318/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14644.3506 - mean_absolute_error: 14644.3506\n",
            "Epoch 318: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14628.2861 - mean_absolute_error: 14628.2861 - val_loss: 29682.5977 - val_mean_absolute_error: 29682.5977\n",
            "Epoch 319/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14856.2783 - mean_absolute_error: 14856.2783\n",
            "Epoch 319: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14917.7168 - mean_absolute_error: 14917.7168 - val_loss: 29266.1523 - val_mean_absolute_error: 29266.1523\n",
            "Epoch 320/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14752.4902 - mean_absolute_error: 14752.4902\n",
            "Epoch 320: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14805.3789 - mean_absolute_error: 14805.3789 - val_loss: 30143.3887 - val_mean_absolute_error: 30143.3887\n",
            "Epoch 321/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14914.0840 - mean_absolute_error: 14914.0840\n",
            "Epoch 321: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14798.4922 - mean_absolute_error: 14798.4922 - val_loss: 29512.5723 - val_mean_absolute_error: 29512.5723\n",
            "Epoch 322/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14986.1016 - mean_absolute_error: 14986.1016\n",
            "Epoch 322: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14984.8438 - mean_absolute_error: 14984.8438 - val_loss: 30147.6406 - val_mean_absolute_error: 30147.6406\n",
            "Epoch 323/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14989.2646 - mean_absolute_error: 14989.2646\n",
            "Epoch 323: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 15018.0352 - mean_absolute_error: 15018.0352 - val_loss: 28940.4824 - val_mean_absolute_error: 28940.4824\n",
            "Epoch 324/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 15107.8203 - mean_absolute_error: 15107.8203\n",
            "Epoch 324: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15070.4150 - mean_absolute_error: 15070.4150 - val_loss: 30078.8887 - val_mean_absolute_error: 30078.8887\n",
            "Epoch 325/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15646.3594 - mean_absolute_error: 15646.3594\n",
            "Epoch 325: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15626.5879 - mean_absolute_error: 15626.5879 - val_loss: 29772.5469 - val_mean_absolute_error: 29772.5469\n",
            "Epoch 326/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14791.8213 - mean_absolute_error: 14791.8213\n",
            "Epoch 326: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15001.8525 - mean_absolute_error: 15001.8525 - val_loss: 29451.7480 - val_mean_absolute_error: 29451.7480\n",
            "Epoch 327/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14692.8115 - mean_absolute_error: 14692.8115\n",
            "Epoch 327: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14698.4805 - mean_absolute_error: 14698.4805 - val_loss: 29662.7852 - val_mean_absolute_error: 29662.7852\n",
            "Epoch 328/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15420.8691 - mean_absolute_error: 15420.8691\n",
            "Epoch 328: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15481.0078 - mean_absolute_error: 15481.0078 - val_loss: 29951.5801 - val_mean_absolute_error: 29951.5801\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14764.6709 - mean_absolute_error: 14764.6709\n",
            "Epoch 329: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14764.6709 - mean_absolute_error: 14764.6709 - val_loss: 29527.5430 - val_mean_absolute_error: 29527.5430\n",
            "Epoch 330/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14869.2520 - mean_absolute_error: 14869.2520\n",
            "Epoch 330: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14808.2666 - mean_absolute_error: 14808.2666 - val_loss: 29751.8438 - val_mean_absolute_error: 29751.8438\n",
            "Epoch 331/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14948.2148 - mean_absolute_error: 14948.2148\n",
            "Epoch 331: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14933.7773 - mean_absolute_error: 14933.7773 - val_loss: 29366.4863 - val_mean_absolute_error: 29366.4863\n",
            "Epoch 332/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14690.0840 - mean_absolute_error: 14690.0840\n",
            "Epoch 332: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14646.8223 - mean_absolute_error: 14646.8223 - val_loss: 29510.4727 - val_mean_absolute_error: 29510.4727\n",
            "Epoch 333/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14757.1953 - mean_absolute_error: 14757.1953\n",
            "Epoch 333: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14904.2354 - mean_absolute_error: 14904.2354 - val_loss: 30394.8184 - val_mean_absolute_error: 30394.8184\n",
            "Epoch 334/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15367.0986 - mean_absolute_error: 15367.0986\n",
            "Epoch 334: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15304.9609 - mean_absolute_error: 15304.9609 - val_loss: 29374.7227 - val_mean_absolute_error: 29374.7227\n",
            "Epoch 335/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14892.5186 - mean_absolute_error: 14892.5186\n",
            "Epoch 335: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14878.9834 - mean_absolute_error: 14878.9834 - val_loss: 29857.9883 - val_mean_absolute_error: 29857.9883\n",
            "Epoch 336/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15572.8916 - mean_absolute_error: 15572.8916\n",
            "Epoch 336: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15423.8311 - mean_absolute_error: 15423.8311 - val_loss: 29726.7090 - val_mean_absolute_error: 29726.7090\n",
            "Epoch 337/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14618.4150 - mean_absolute_error: 14618.4150\n",
            "Epoch 337: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14705.2559 - mean_absolute_error: 14705.2559 - val_loss: 29642.4590 - val_mean_absolute_error: 29642.4590\n",
            "Epoch 338/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14616.9668 - mean_absolute_error: 14616.9668\n",
            "Epoch 338: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14631.7969 - mean_absolute_error: 14631.7969 - val_loss: 29613.6719 - val_mean_absolute_error: 29613.6719\n",
            "Epoch 339/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14589.5947 - mean_absolute_error: 14589.5947\n",
            "Epoch 339: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14608.6621 - mean_absolute_error: 14608.6621 - val_loss: 29940.4277 - val_mean_absolute_error: 29940.4277\n",
            "Epoch 340/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14736.5400 - mean_absolute_error: 14736.5400\n",
            "Epoch 340: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14764.9395 - mean_absolute_error: 14764.9395 - val_loss: 29604.0254 - val_mean_absolute_error: 29604.0254\n",
            "Epoch 341/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14657.5508 - mean_absolute_error: 14657.5508\n",
            "Epoch 341: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 14650.8955 - mean_absolute_error: 14650.8955 - val_loss: 30124.8887 - val_mean_absolute_error: 30124.8887\n",
            "Epoch 342/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 15119.0264 - mean_absolute_error: 15119.0264\n",
            "Epoch 342: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 15062.0186 - mean_absolute_error: 15062.0186 - val_loss: 29728.7344 - val_mean_absolute_error: 29728.7344\n",
            "Epoch 343/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 15184.8574 - mean_absolute_error: 15184.8574\n",
            "Epoch 343: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 15157.4072 - mean_absolute_error: 15157.4072 - val_loss: 29078.0234 - val_mean_absolute_error: 29078.0234\n",
            "Epoch 344/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14491.7930 - mean_absolute_error: 14491.7930\n",
            "Epoch 344: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14622.9004 - mean_absolute_error: 14622.9004 - val_loss: 30086.3027 - val_mean_absolute_error: 30086.3027\n",
            "Epoch 345/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 15114.9551 - mean_absolute_error: 15114.9551\n",
            "Epoch 345: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15005.6191 - mean_absolute_error: 15005.6191 - val_loss: 29778.3594 - val_mean_absolute_error: 29778.3594\n",
            "Epoch 346/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14251.3438 - mean_absolute_error: 14251.3438\n",
            "Epoch 346: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14319.1045 - mean_absolute_error: 14319.1045 - val_loss: 29668.7051 - val_mean_absolute_error: 29668.7051\n",
            "Epoch 347/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14454.3174 - mean_absolute_error: 14454.3174\n",
            "Epoch 347: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14587.8770 - mean_absolute_error: 14587.8770 - val_loss: 29748.9434 - val_mean_absolute_error: 29748.9434\n",
            "Epoch 348/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 15195.9570 - mean_absolute_error: 15195.9570\n",
            "Epoch 348: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15020.6338 - mean_absolute_error: 15020.6338 - val_loss: 29585.4297 - val_mean_absolute_error: 29585.4297\n",
            "Epoch 349/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14833.0557 - mean_absolute_error: 14833.0557\n",
            "Epoch 349: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14727.0576 - mean_absolute_error: 14727.0576 - val_loss: 29916.1367 - val_mean_absolute_error: 29916.1367\n",
            "Epoch 350/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14993.3037 - mean_absolute_error: 14993.3037\n",
            "Epoch 350: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14945.1611 - mean_absolute_error: 14945.1611 - val_loss: 29293.3379 - val_mean_absolute_error: 29293.3379\n",
            "Epoch 351/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14908.3936 - mean_absolute_error: 14908.3936\n",
            "Epoch 351: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14876.4092 - mean_absolute_error: 14876.4092 - val_loss: 29392.8887 - val_mean_absolute_error: 29392.8887\n",
            "Epoch 352/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14569.2168 - mean_absolute_error: 14569.2168\n",
            "Epoch 352: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 14508.1895 - mean_absolute_error: 14508.1895 - val_loss: 29634.2637 - val_mean_absolute_error: 29634.2637\n",
            "Epoch 353/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14879.4111 - mean_absolute_error: 14879.4111\n",
            "Epoch 353: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 15047.7480 - mean_absolute_error: 15047.7480 - val_loss: 30325.2402 - val_mean_absolute_error: 30325.2402\n",
            "Epoch 354/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14518.9229 - mean_absolute_error: 14518.9229\n",
            "Epoch 354: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14456.4561 - mean_absolute_error: 14456.4561 - val_loss: 29401.2031 - val_mean_absolute_error: 29401.2031\n",
            "Epoch 355/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14553.0117 - mean_absolute_error: 14553.0117\n",
            "Epoch 355: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14531.6865 - mean_absolute_error: 14531.6865 - val_loss: 29328.7676 - val_mean_absolute_error: 29328.7676\n",
            "Epoch 356/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14566.7324 - mean_absolute_error: 14566.7324\n",
            "Epoch 356: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14531.0391 - mean_absolute_error: 14531.0391 - val_loss: 29042.4199 - val_mean_absolute_error: 29042.4199\n",
            "Epoch 357/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14681.5166 - mean_absolute_error: 14681.5166\n",
            "Epoch 357: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14759.8545 - mean_absolute_error: 14759.8545 - val_loss: 29915.5742 - val_mean_absolute_error: 29915.5742\n",
            "Epoch 358/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14836.9375 - mean_absolute_error: 14836.9375\n",
            "Epoch 358: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14812.5469 - mean_absolute_error: 14812.5469 - val_loss: 30581.1582 - val_mean_absolute_error: 30581.1582\n",
            "Epoch 359/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14330.2422 - mean_absolute_error: 14330.2422\n",
            "Epoch 359: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14474.4795 - mean_absolute_error: 14474.4795 - val_loss: 28912.0957 - val_mean_absolute_error: 28912.0957\n",
            "Epoch 360/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 15211.3984 - mean_absolute_error: 15211.3984\n",
            "Epoch 360: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 15183.7559 - mean_absolute_error: 15183.7559 - val_loss: 29487.3164 - val_mean_absolute_error: 29487.3164\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14308.2959 - mean_absolute_error: 14308.2959\n",
            "Epoch 361: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14308.2959 - mean_absolute_error: 14308.2959 - val_loss: 29577.3906 - val_mean_absolute_error: 29577.3906\n",
            "Epoch 362/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14541.9180 - mean_absolute_error: 14541.9180\n",
            "Epoch 362: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14561.4248 - mean_absolute_error: 14561.4248 - val_loss: 29552.2910 - val_mean_absolute_error: 29552.2910\n",
            "Epoch 363/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14305.4717 - mean_absolute_error: 14305.4717\n",
            "Epoch 363: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14248.4707 - mean_absolute_error: 14248.4707 - val_loss: 29414.2598 - val_mean_absolute_error: 29414.2598\n",
            "Epoch 364/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14194.5117 - mean_absolute_error: 14194.5117\n",
            "Epoch 364: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14226.5107 - mean_absolute_error: 14226.5107 - val_loss: 29448.2852 - val_mean_absolute_error: 29448.2852\n",
            "Epoch 365/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14353.4941 - mean_absolute_error: 14353.4941\n",
            "Epoch 365: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14466.9570 - mean_absolute_error: 14466.9570 - val_loss: 29853.6211 - val_mean_absolute_error: 29853.6211\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14199.1973 - mean_absolute_error: 14199.1973\n",
            "Epoch 366: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14199.1973 - mean_absolute_error: 14199.1973 - val_loss: 29706.5605 - val_mean_absolute_error: 29706.5605\n",
            "Epoch 367/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14464.7021 - mean_absolute_error: 14464.7021\n",
            "Epoch 367: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14441.1992 - mean_absolute_error: 14441.1992 - val_loss: 29316.3828 - val_mean_absolute_error: 29316.3828\n",
            "Epoch 368/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14435.8896 - mean_absolute_error: 14435.8896\n",
            "Epoch 368: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14543.4248 - mean_absolute_error: 14543.4248 - val_loss: 29647.5801 - val_mean_absolute_error: 29647.5801\n",
            "Epoch 369/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14794.0166 - mean_absolute_error: 14794.0166\n",
            "Epoch 369: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14646.7793 - mean_absolute_error: 14646.7793 - val_loss: 29471.0332 - val_mean_absolute_error: 29471.0332\n",
            "Epoch 370/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14734.7119 - mean_absolute_error: 14734.7119\n",
            "Epoch 370: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14705.0244 - mean_absolute_error: 14705.0244 - val_loss: 30031.7793 - val_mean_absolute_error: 30031.7793\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14621.7871 - mean_absolute_error: 14621.7871\n",
            "Epoch 371: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14621.7871 - mean_absolute_error: 14621.7871 - val_loss: 29400.3340 - val_mean_absolute_error: 29400.3340\n",
            "Epoch 372/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14603.1963 - mean_absolute_error: 14603.1963\n",
            "Epoch 372: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14663.7842 - mean_absolute_error: 14663.7842 - val_loss: 30610.8164 - val_mean_absolute_error: 30610.8164\n",
            "Epoch 373/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14846.4043 - mean_absolute_error: 14846.4043\n",
            "Epoch 373: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14758.0625 - mean_absolute_error: 14758.0625 - val_loss: 29448.1133 - val_mean_absolute_error: 29448.1133\n",
            "Epoch 374/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14401.3174 - mean_absolute_error: 14401.3174\n",
            "Epoch 374: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14280.0439 - mean_absolute_error: 14280.0439 - val_loss: 29716.6680 - val_mean_absolute_error: 29716.6680\n",
            "Epoch 375/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 14621.0137 - mean_absolute_error: 14621.0137\n",
            "Epoch 375: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14635.7705 - mean_absolute_error: 14635.7705 - val_loss: 30139.1699 - val_mean_absolute_error: 30139.1699\n",
            "Epoch 376/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14591.8691 - mean_absolute_error: 14591.8691\n",
            "Epoch 376: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14615.4189 - mean_absolute_error: 14615.4189 - val_loss: 29953.7090 - val_mean_absolute_error: 29953.7090\n",
            "Epoch 377/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14515.2695 - mean_absolute_error: 14515.2695\n",
            "Epoch 377: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14719.0420 - mean_absolute_error: 14719.0420 - val_loss: 30194.0430 - val_mean_absolute_error: 30194.0430\n",
            "Epoch 378/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 15039.4980 - mean_absolute_error: 15039.4980\n",
            "Epoch 378: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14926.7871 - mean_absolute_error: 14926.7871 - val_loss: 29993.6875 - val_mean_absolute_error: 29993.6875\n",
            "Epoch 379/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14981.1504 - mean_absolute_error: 14981.1504\n",
            "Epoch 379: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14890.1104 - mean_absolute_error: 14890.1104 - val_loss: 29889.1562 - val_mean_absolute_error: 29889.1562\n",
            "Epoch 380/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 14223.0742 - mean_absolute_error: 14223.0742\n",
            "Epoch 380: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14412.2314 - mean_absolute_error: 14412.2314 - val_loss: 30245.0059 - val_mean_absolute_error: 30245.0059\n",
            "Epoch 381/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14239.5098 - mean_absolute_error: 14239.5098\n",
            "Epoch 381: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14226.5391 - mean_absolute_error: 14226.5391 - val_loss: 31658.5723 - val_mean_absolute_error: 31658.5723\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14436.1221 - mean_absolute_error: 14436.1221\n",
            "Epoch 382: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14436.1221 - mean_absolute_error: 14436.1221 - val_loss: 29602.4414 - val_mean_absolute_error: 29602.4414\n",
            "Epoch 383/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14521.3496 - mean_absolute_error: 14521.3496\n",
            "Epoch 383: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 14527.5068 - mean_absolute_error: 14527.5068 - val_loss: 29337.7539 - val_mean_absolute_error: 29337.7539\n",
            "Epoch 384/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14358.2178 - mean_absolute_error: 14358.2178\n",
            "Epoch 384: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14341.3193 - mean_absolute_error: 14341.3193 - val_loss: 29842.7480 - val_mean_absolute_error: 29842.7480\n",
            "Epoch 385/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14731.5498 - mean_absolute_error: 14731.5498\n",
            "Epoch 385: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14651.6650 - mean_absolute_error: 14651.6650 - val_loss: 29232.2812 - val_mean_absolute_error: 29232.2812\n",
            "Epoch 386/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14472.3291 - mean_absolute_error: 14472.3291\n",
            "Epoch 386: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14447.1650 - mean_absolute_error: 14447.1650 - val_loss: 30226.5703 - val_mean_absolute_error: 30226.5703\n",
            "Epoch 387/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14128.5000 - mean_absolute_error: 14128.5000\n",
            "Epoch 387: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14188.2598 - mean_absolute_error: 14188.2598 - val_loss: 29713.5352 - val_mean_absolute_error: 29713.5352\n",
            "Epoch 388/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14398.4941 - mean_absolute_error: 14398.4941\n",
            "Epoch 388: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14501.6387 - mean_absolute_error: 14501.6387 - val_loss: 28886.5547 - val_mean_absolute_error: 28886.5547\n",
            "Epoch 389/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14296.4629 - mean_absolute_error: 14296.4629\n",
            "Epoch 389: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14328.6660 - mean_absolute_error: 14328.6660 - val_loss: 29412.4863 - val_mean_absolute_error: 29412.4863\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14537.9492 - mean_absolute_error: 14537.9492\n",
            "Epoch 390: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14537.9492 - mean_absolute_error: 14537.9492 - val_loss: 29206.7305 - val_mean_absolute_error: 29206.7305\n",
            "Epoch 391/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14922.2900 - mean_absolute_error: 14922.2900\n",
            "Epoch 391: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14847.2266 - mean_absolute_error: 14847.2266 - val_loss: 30863.7676 - val_mean_absolute_error: 30863.7676\n",
            "Epoch 392/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14258.2402 - mean_absolute_error: 14258.2402\n",
            "Epoch 392: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14268.3867 - mean_absolute_error: 14268.3867 - val_loss: 29952.7168 - val_mean_absolute_error: 29952.7168\n",
            "Epoch 393/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14283.6436 - mean_absolute_error: 14283.6436\n",
            "Epoch 393: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14275.0742 - mean_absolute_error: 14275.0742 - val_loss: 29145.5137 - val_mean_absolute_error: 29145.5137\n",
            "Epoch 394/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14686.7148 - mean_absolute_error: 14686.7148\n",
            "Epoch 394: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14696.0322 - mean_absolute_error: 14696.0322 - val_loss: 29609.7188 - val_mean_absolute_error: 29609.7188\n",
            "Epoch 395/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 13990.1895 - mean_absolute_error: 13990.1895\n",
            "Epoch 395: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13929.3525 - mean_absolute_error: 13929.3525 - val_loss: 29256.2031 - val_mean_absolute_error: 29256.2031\n",
            "Epoch 396/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14240.1182 - mean_absolute_error: 14240.1182\n",
            "Epoch 396: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14191.5986 - mean_absolute_error: 14191.5986 - val_loss: 29751.6035 - val_mean_absolute_error: 29751.6035\n",
            "Epoch 397/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14443.7646 - mean_absolute_error: 14443.7646\n",
            "Epoch 397: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14506.6367 - mean_absolute_error: 14506.6367 - val_loss: 29490.9883 - val_mean_absolute_error: 29490.9883\n",
            "Epoch 398/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14258.5967 - mean_absolute_error: 14258.5967\n",
            "Epoch 398: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14198.0312 - mean_absolute_error: 14198.0312 - val_loss: 29457.3008 - val_mean_absolute_error: 29457.3008\n",
            "Epoch 399/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14212.8916 - mean_absolute_error: 14212.8916\n",
            "Epoch 399: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14200.6572 - mean_absolute_error: 14200.6572 - val_loss: 29176.0000 - val_mean_absolute_error: 29176.0000\n",
            "Epoch 400/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14165.8594 - mean_absolute_error: 14165.8594\n",
            "Epoch 400: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14158.6182 - mean_absolute_error: 14158.6182 - val_loss: 29815.6934 - val_mean_absolute_error: 29815.6934\n",
            "Epoch 401/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14324.3633 - mean_absolute_error: 14324.3633\n",
            "Epoch 401: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14405.6045 - mean_absolute_error: 14405.6045 - val_loss: 29149.8281 - val_mean_absolute_error: 29149.8281\n",
            "Epoch 402/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14060.2295 - mean_absolute_error: 14060.2295\n",
            "Epoch 402: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14185.5908 - mean_absolute_error: 14185.5908 - val_loss: 29324.9453 - val_mean_absolute_error: 29324.9453\n",
            "Epoch 403/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13961.5498 - mean_absolute_error: 13961.5498\n",
            "Epoch 403: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13987.8877 - mean_absolute_error: 13987.8877 - val_loss: 29511.6035 - val_mean_absolute_error: 29511.6035\n",
            "Epoch 404/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14116.5781 - mean_absolute_error: 14116.5781\n",
            "Epoch 404: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14037.0020 - mean_absolute_error: 14037.0020 - val_loss: 29945.8164 - val_mean_absolute_error: 29945.8164\n",
            "Epoch 405/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14650.9678 - mean_absolute_error: 14650.9678\n",
            "Epoch 405: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14646.2441 - mean_absolute_error: 14646.2441 - val_loss: 29587.6973 - val_mean_absolute_error: 29587.6973\n",
            "Epoch 406/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13770.4736 - mean_absolute_error: 13770.4736\n",
            "Epoch 406: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13882.6807 - mean_absolute_error: 13882.6807 - val_loss: 30036.3711 - val_mean_absolute_error: 30036.3711\n",
            "Epoch 407/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14149.8252 - mean_absolute_error: 14149.8252\n",
            "Epoch 407: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14171.6006 - mean_absolute_error: 14171.6006 - val_loss: 29247.1055 - val_mean_absolute_error: 29247.1055\n",
            "Epoch 408/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14258.3379 - mean_absolute_error: 14258.3379\n",
            "Epoch 408: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14372.8193 - mean_absolute_error: 14372.8193 - val_loss: 29436.2441 - val_mean_absolute_error: 29436.2441\n",
            "Epoch 409/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14256.8115 - mean_absolute_error: 14256.8115\n",
            "Epoch 409: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14245.7969 - mean_absolute_error: 14245.7969 - val_loss: 28851.6855 - val_mean_absolute_error: 28851.6855\n",
            "Epoch 410/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14297.1104 - mean_absolute_error: 14297.1104\n",
            "Epoch 410: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14389.1904 - mean_absolute_error: 14389.1904 - val_loss: 30520.8809 - val_mean_absolute_error: 30520.8809\n",
            "Epoch 411/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14180.7852 - mean_absolute_error: 14180.7852\n",
            "Epoch 411: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14167.0488 - mean_absolute_error: 14167.0488 - val_loss: 30245.0625 - val_mean_absolute_error: 30245.0625\n",
            "Epoch 412/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14137.3252 - mean_absolute_error: 14137.3252\n",
            "Epoch 412: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14099.1641 - mean_absolute_error: 14099.1641 - val_loss: 29053.0996 - val_mean_absolute_error: 29053.0996\n",
            "Epoch 413/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13879.6758 - mean_absolute_error: 13879.6758\n",
            "Epoch 413: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13930.8701 - mean_absolute_error: 13930.8701 - val_loss: 29547.8008 - val_mean_absolute_error: 29547.8008\n",
            "Epoch 414/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13941.9512 - mean_absolute_error: 13941.9512\n",
            "Epoch 414: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13898.1348 - mean_absolute_error: 13898.1348 - val_loss: 29377.3848 - val_mean_absolute_error: 29377.3848\n",
            "Epoch 415/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 14475.8486 - mean_absolute_error: 14475.8486\n",
            "Epoch 415: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14280.7881 - mean_absolute_error: 14280.7881 - val_loss: 29168.1992 - val_mean_absolute_error: 29168.1992\n",
            "Epoch 416/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14115.7783 - mean_absolute_error: 14115.7783\n",
            "Epoch 416: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14075.3662 - mean_absolute_error: 14075.3662 - val_loss: 29479.5840 - val_mean_absolute_error: 29479.5840\n",
            "Epoch 417/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14223.3584 - mean_absolute_error: 14223.3584\n",
            "Epoch 417: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14329.8174 - mean_absolute_error: 14329.8174 - val_loss: 29282.4297 - val_mean_absolute_error: 29282.4297\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13923.4629 - mean_absolute_error: 13923.4629\n",
            "Epoch 418: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13923.4629 - mean_absolute_error: 13923.4629 - val_loss: 28970.3887 - val_mean_absolute_error: 28970.3887\n",
            "Epoch 419/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14173.3506 - mean_absolute_error: 14173.3506\n",
            "Epoch 419: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14181.6787 - mean_absolute_error: 14181.6787 - val_loss: 29747.0332 - val_mean_absolute_error: 29747.0332\n",
            "Epoch 420/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14182.9229 - mean_absolute_error: 14182.9229\n",
            "Epoch 420: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 14118.9453 - mean_absolute_error: 14118.9453 - val_loss: 29400.0859 - val_mean_absolute_error: 29400.0859\n",
            "Epoch 421/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14081.0078 - mean_absolute_error: 14081.0078\n",
            "Epoch 421: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14043.1455 - mean_absolute_error: 14043.1455 - val_loss: 29135.6543 - val_mean_absolute_error: 29135.6543\n",
            "Epoch 422/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13851.7588 - mean_absolute_error: 13851.7588\n",
            "Epoch 422: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13848.7598 - mean_absolute_error: 13848.7598 - val_loss: 30294.4473 - val_mean_absolute_error: 30294.4473\n",
            "Epoch 423/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14357.3516 - mean_absolute_error: 14357.3516\n",
            "Epoch 423: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 14385.3027 - mean_absolute_error: 14385.3027 - val_loss: 30572.8672 - val_mean_absolute_error: 30572.8672\n",
            "Epoch 424/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 14001.5811 - mean_absolute_error: 14001.5811\n",
            "Epoch 424: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14024.9482 - mean_absolute_error: 14024.9482 - val_loss: 29090.7988 - val_mean_absolute_error: 29090.7988\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14116.4922 - mean_absolute_error: 14116.4922\n",
            "Epoch 425: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14116.4922 - mean_absolute_error: 14116.4922 - val_loss: 29424.5137 - val_mean_absolute_error: 29424.5137\n",
            "Epoch 426/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14008.1064 - mean_absolute_error: 14008.1064\n",
            "Epoch 426: val_loss did not improve from 28844.82812\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 13932.4238 - mean_absolute_error: 13932.4238 - val_loss: 29028.0000 - val_mean_absolute_error: 29028.0000\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13807.1533 - mean_absolute_error: 13807.1533\n",
            "Epoch 427: val_loss improved from 28844.82812 to 28760.77148, saving model to Weights-427--28760.77148.hdf5\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13807.1533 - mean_absolute_error: 13807.1533 - val_loss: 28760.7715 - val_mean_absolute_error: 28760.7715\n",
            "Epoch 428/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13892.3486 - mean_absolute_error: 13892.3486\n",
            "Epoch 428: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13954.8760 - mean_absolute_error: 13954.8760 - val_loss: 30199.5469 - val_mean_absolute_error: 30199.5469\n",
            "Epoch 429/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14143.4736 - mean_absolute_error: 14143.4736\n",
            "Epoch 429: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14183.9131 - mean_absolute_error: 14183.9131 - val_loss: 30158.9434 - val_mean_absolute_error: 30158.9434\n",
            "Epoch 430/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 14388.7500 - mean_absolute_error: 14388.7500\n",
            "Epoch 430: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14406.3633 - mean_absolute_error: 14406.3633 - val_loss: 29224.6914 - val_mean_absolute_error: 29224.6914\n",
            "Epoch 431/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 13646.5430 - mean_absolute_error: 13646.5430\n",
            "Epoch 431: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13706.8906 - mean_absolute_error: 13706.8906 - val_loss: 30798.6738 - val_mean_absolute_error: 30798.6738\n",
            "Epoch 432/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 14099.8994 - mean_absolute_error: 14099.8994\n",
            "Epoch 432: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14147.4951 - mean_absolute_error: 14147.4951 - val_loss: 30240.4004 - val_mean_absolute_error: 30240.4004\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14357.2236 - mean_absolute_error: 14357.2236\n",
            "Epoch 433: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14357.2236 - mean_absolute_error: 14357.2236 - val_loss: 29545.4004 - val_mean_absolute_error: 29545.4004\n",
            "Epoch 434/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13840.0195 - mean_absolute_error: 13840.0195\n",
            "Epoch 434: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13828.9355 - mean_absolute_error: 13828.9355 - val_loss: 29201.0176 - val_mean_absolute_error: 29201.0176\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14173.3037 - mean_absolute_error: 14173.3037\n",
            "Epoch 435: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14173.3037 - mean_absolute_error: 14173.3037 - val_loss: 29299.3770 - val_mean_absolute_error: 29299.3770\n",
            "Epoch 436/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 13829.8584 - mean_absolute_error: 13829.8584\n",
            "Epoch 436: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13887.0166 - mean_absolute_error: 13887.0166 - val_loss: 29573.8164 - val_mean_absolute_error: 29573.8164\n",
            "Epoch 437/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13891.8135 - mean_absolute_error: 13891.8135\n",
            "Epoch 437: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13952.4492 - mean_absolute_error: 13952.4492 - val_loss: 29632.6875 - val_mean_absolute_error: 29632.6875\n",
            "Epoch 438/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13724.2109 - mean_absolute_error: 13724.2109\n",
            "Epoch 438: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13790.2168 - mean_absolute_error: 13790.2168 - val_loss: 29243.4668 - val_mean_absolute_error: 29243.4668\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13786.7559 - mean_absolute_error: 13786.7559\n",
            "Epoch 439: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13786.7559 - mean_absolute_error: 13786.7559 - val_loss: 29399.3145 - val_mean_absolute_error: 29399.3145\n",
            "Epoch 440/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 13813.5312 - mean_absolute_error: 13813.5312\n",
            "Epoch 440: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13821.8457 - mean_absolute_error: 13821.8457 - val_loss: 29533.4219 - val_mean_absolute_error: 29533.4219\n",
            "Epoch 441/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13849.1055 - mean_absolute_error: 13849.1055\n",
            "Epoch 441: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13976.8281 - mean_absolute_error: 13976.8281 - val_loss: 29496.2520 - val_mean_absolute_error: 29496.2520\n",
            "Epoch 442/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 14126.1494 - mean_absolute_error: 14126.1494\n",
            "Epoch 442: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 14087.3037 - mean_absolute_error: 14087.3037 - val_loss: 29272.2461 - val_mean_absolute_error: 29272.2461\n",
            "Epoch 443/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 13557.1553 - mean_absolute_error: 13557.1553\n",
            "Epoch 443: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13604.1943 - mean_absolute_error: 13604.1943 - val_loss: 30861.5645 - val_mean_absolute_error: 30861.5645\n",
            "Epoch 444/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14138.0137 - mean_absolute_error: 14138.0137\n",
            "Epoch 444: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 14089.3340 - mean_absolute_error: 14089.3340 - val_loss: 29697.8887 - val_mean_absolute_error: 29697.8887\n",
            "Epoch 445/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13827.3916 - mean_absolute_error: 13827.3916\n",
            "Epoch 445: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13709.3369 - mean_absolute_error: 13709.3369 - val_loss: 29359.0508 - val_mean_absolute_error: 29359.0508\n",
            "Epoch 446/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13665.9189 - mean_absolute_error: 13665.9189\n",
            "Epoch 446: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13671.4434 - mean_absolute_error: 13671.4434 - val_loss: 31112.6914 - val_mean_absolute_error: 31112.6914\n",
            "Epoch 447/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13944.1309 - mean_absolute_error: 13944.1309\n",
            "Epoch 447: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13928.5674 - mean_absolute_error: 13928.5674 - val_loss: 29024.7246 - val_mean_absolute_error: 29024.7246\n",
            "Epoch 448/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 13600.8438 - mean_absolute_error: 13600.8438\n",
            "Epoch 448: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13572.3047 - mean_absolute_error: 13572.3047 - val_loss: 29534.1895 - val_mean_absolute_error: 29534.1895\n",
            "Epoch 449/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13651.4160 - mean_absolute_error: 13651.4160\n",
            "Epoch 449: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13740.6377 - mean_absolute_error: 13740.6377 - val_loss: 28958.1133 - val_mean_absolute_error: 28958.1133\n",
            "Epoch 450/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13880.2852 - mean_absolute_error: 13880.2852\n",
            "Epoch 450: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13778.2949 - mean_absolute_error: 13778.2949 - val_loss: 29180.0137 - val_mean_absolute_error: 29180.0137\n",
            "Epoch 451/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13749.6758 - mean_absolute_error: 13749.6758\n",
            "Epoch 451: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13755.2344 - mean_absolute_error: 13755.2344 - val_loss: 29330.2031 - val_mean_absolute_error: 29330.2031\n",
            "Epoch 452/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 14291.8086 - mean_absolute_error: 14291.8086\n",
            "Epoch 452: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 14230.4619 - mean_absolute_error: 14230.4619 - val_loss: 29171.3965 - val_mean_absolute_error: 29171.3965\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 14133.1797 - mean_absolute_error: 14133.1797\n",
            "Epoch 453: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 14133.1797 - mean_absolute_error: 14133.1797 - val_loss: 29309.2285 - val_mean_absolute_error: 29309.2285\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13663.9248 - mean_absolute_error: 13663.9248\n",
            "Epoch 454: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13663.9248 - mean_absolute_error: 13663.9248 - val_loss: 29314.6172 - val_mean_absolute_error: 29314.6172\n",
            "Epoch 455/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 13944.0244 - mean_absolute_error: 13944.0244\n",
            "Epoch 455: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13915.8809 - mean_absolute_error: 13915.8809 - val_loss: 29314.0176 - val_mean_absolute_error: 29314.0176\n",
            "Epoch 456/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 13533.1953 - mean_absolute_error: 13533.1953\n",
            "Epoch 456: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13562.2080 - mean_absolute_error: 13562.2080 - val_loss: 29260.0371 - val_mean_absolute_error: 29260.0371\n",
            "Epoch 457/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13632.7842 - mean_absolute_error: 13632.7842\n",
            "Epoch 457: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13808.3672 - mean_absolute_error: 13808.3672 - val_loss: 29326.5098 - val_mean_absolute_error: 29326.5098\n",
            "Epoch 458/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13907.7939 - mean_absolute_error: 13907.7939\n",
            "Epoch 458: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13888.5645 - mean_absolute_error: 13888.5645 - val_loss: 29140.2949 - val_mean_absolute_error: 29140.2949\n",
            "Epoch 459/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13878.8486 - mean_absolute_error: 13878.8486\n",
            "Epoch 459: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13953.1689 - mean_absolute_error: 13953.1689 - val_loss: 29166.2812 - val_mean_absolute_error: 29166.2812\n",
            "Epoch 460/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13600.8193 - mean_absolute_error: 13600.8193\n",
            "Epoch 460: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13592.4170 - mean_absolute_error: 13592.4170 - val_loss: 29074.1465 - val_mean_absolute_error: 29074.1465\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13720.9316 - mean_absolute_error: 13720.9316\n",
            "Epoch 461: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13720.9316 - mean_absolute_error: 13720.9316 - val_loss: 29752.9355 - val_mean_absolute_error: 29752.9355\n",
            "Epoch 462/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13553.2197 - mean_absolute_error: 13553.2197\n",
            "Epoch 462: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13592.2314 - mean_absolute_error: 13592.2314 - val_loss: 28922.7734 - val_mean_absolute_error: 28922.7734\n",
            "Epoch 463/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13549.3867 - mean_absolute_error: 13549.3867\n",
            "Epoch 463: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13610.9004 - mean_absolute_error: 13610.9004 - val_loss: 29519.2832 - val_mean_absolute_error: 29519.2832\n",
            "Epoch 464/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13538.3896 - mean_absolute_error: 13538.3896\n",
            "Epoch 464: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13493.5127 - mean_absolute_error: 13493.5127 - val_loss: 29763.4473 - val_mean_absolute_error: 29763.4473\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13722.1572 - mean_absolute_error: 13722.1572\n",
            "Epoch 465: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13722.1572 - mean_absolute_error: 13722.1572 - val_loss: 28919.1309 - val_mean_absolute_error: 28919.1309\n",
            "Epoch 466/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13651.8633 - mean_absolute_error: 13651.8633\n",
            "Epoch 466: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13535.9844 - mean_absolute_error: 13535.9844 - val_loss: 29183.1074 - val_mean_absolute_error: 29183.1074\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13849.9775 - mean_absolute_error: 13849.9775\n",
            "Epoch 467: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13849.9775 - mean_absolute_error: 13849.9775 - val_loss: 30080.4824 - val_mean_absolute_error: 30080.4824\n",
            "Epoch 468/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 14200.7861 - mean_absolute_error: 14200.7861\n",
            "Epoch 468: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14140.4990 - mean_absolute_error: 14140.4990 - val_loss: 29636.6973 - val_mean_absolute_error: 29636.6973\n",
            "Epoch 469/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13516.0586 - mean_absolute_error: 13516.0586\n",
            "Epoch 469: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13481.6895 - mean_absolute_error: 13481.6895 - val_loss: 29702.8164 - val_mean_absolute_error: 29702.8164\n",
            "Epoch 470/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 14103.1338 - mean_absolute_error: 14103.1338\n",
            "Epoch 470: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 14053.5928 - mean_absolute_error: 14053.5928 - val_loss: 29331.9062 - val_mean_absolute_error: 29331.9062\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13443.0615 - mean_absolute_error: 13443.0615\n",
            "Epoch 471: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13443.0615 - mean_absolute_error: 13443.0615 - val_loss: 29303.8574 - val_mean_absolute_error: 29303.8574\n",
            "Epoch 472/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 13418.0791 - mean_absolute_error: 13418.0791\n",
            "Epoch 472: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13542.2637 - mean_absolute_error: 13542.2637 - val_loss: 29247.5234 - val_mean_absolute_error: 29247.5234\n",
            "Epoch 473/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13815.8799 - mean_absolute_error: 13815.8799\n",
            "Epoch 473: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13797.9707 - mean_absolute_error: 13797.9707 - val_loss: 28856.4336 - val_mean_absolute_error: 28856.4336\n",
            "Epoch 474/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13564.6279 - mean_absolute_error: 13564.6279\n",
            "Epoch 474: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13578.9287 - mean_absolute_error: 13578.9287 - val_loss: 29049.9375 - val_mean_absolute_error: 29049.9375\n",
            "Epoch 475/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13245.3115 - mean_absolute_error: 13245.3115\n",
            "Epoch 475: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13241.5039 - mean_absolute_error: 13241.5039 - val_loss: 28979.4707 - val_mean_absolute_error: 28979.4707\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13772.2920 - mean_absolute_error: 13772.2920\n",
            "Epoch 476: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13772.2920 - mean_absolute_error: 13772.2920 - val_loss: 29053.7969 - val_mean_absolute_error: 29053.7969\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13604.2471 - mean_absolute_error: 13604.2471\n",
            "Epoch 477: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13604.2471 - mean_absolute_error: 13604.2471 - val_loss: 29396.3027 - val_mean_absolute_error: 29396.3027\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13631.5352 - mean_absolute_error: 13631.5352\n",
            "Epoch 478: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13631.5352 - mean_absolute_error: 13631.5352 - val_loss: 29021.0742 - val_mean_absolute_error: 29021.0742\n",
            "Epoch 479/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13567.7236 - mean_absolute_error: 13567.7236\n",
            "Epoch 479: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13534.9912 - mean_absolute_error: 13534.9912 - val_loss: 29902.2715 - val_mean_absolute_error: 29902.2715\n",
            "Epoch 480/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13550.5283 - mean_absolute_error: 13550.5283\n",
            "Epoch 480: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13562.2988 - mean_absolute_error: 13562.2988 - val_loss: 29800.9062 - val_mean_absolute_error: 29800.9062\n",
            "Epoch 481/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13347.8887 - mean_absolute_error: 13347.8887\n",
            "Epoch 481: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13409.8027 - mean_absolute_error: 13409.8027 - val_loss: 30103.2598 - val_mean_absolute_error: 30103.2598\n",
            "Epoch 482/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13461.5283 - mean_absolute_error: 13461.5283\n",
            "Epoch 482: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13442.6309 - mean_absolute_error: 13442.6309 - val_loss: 29206.2637 - val_mean_absolute_error: 29206.2637\n",
            "Epoch 483/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13414.5938 - mean_absolute_error: 13414.5938\n",
            "Epoch 483: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13416.1641 - mean_absolute_error: 13416.1641 - val_loss: 29683.5527 - val_mean_absolute_error: 29683.5527\n",
            "Epoch 484/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13408.2480 - mean_absolute_error: 13408.2480\n",
            "Epoch 484: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13329.9648 - mean_absolute_error: 13329.9648 - val_loss: 30049.3965 - val_mean_absolute_error: 30049.3965\n",
            "Epoch 485/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13582.3662 - mean_absolute_error: 13582.3662\n",
            "Epoch 485: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13463.8232 - mean_absolute_error: 13463.8232 - val_loss: 29513.3145 - val_mean_absolute_error: 29513.3145\n",
            "Epoch 486/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13600.0283 - mean_absolute_error: 13600.0283\n",
            "Epoch 486: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 13596.5879 - mean_absolute_error: 13596.5879 - val_loss: 29525.5918 - val_mean_absolute_error: 29525.5918\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13428.5547 - mean_absolute_error: 13428.5547\n",
            "Epoch 487: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13428.5547 - mean_absolute_error: 13428.5547 - val_loss: 29397.4102 - val_mean_absolute_error: 29397.4102\n",
            "Epoch 488/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 13503.6855 - mean_absolute_error: 13503.6855\n",
            "Epoch 488: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13518.1572 - mean_absolute_error: 13518.1572 - val_loss: 29399.0508 - val_mean_absolute_error: 29399.0508\n",
            "Epoch 489/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13463.4590 - mean_absolute_error: 13463.4590\n",
            "Epoch 489: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13452.4355 - mean_absolute_error: 13452.4355 - val_loss: 29028.1426 - val_mean_absolute_error: 29028.1426\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13298.6787 - mean_absolute_error: 13298.6787\n",
            "Epoch 490: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13298.6787 - mean_absolute_error: 13298.6787 - val_loss: 29198.3789 - val_mean_absolute_error: 29198.3789\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 13349.7559 - mean_absolute_error: 13349.7559\n",
            "Epoch 491: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13349.7559 - mean_absolute_error: 13349.7559 - val_loss: 29215.5410 - val_mean_absolute_error: 29215.5410\n",
            "Epoch 492/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13123.9180 - mean_absolute_error: 13123.9180\n",
            "Epoch 492: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13134.1455 - mean_absolute_error: 13134.1455 - val_loss: 29436.1777 - val_mean_absolute_error: 29436.1777\n",
            "Epoch 493/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13373.4707 - mean_absolute_error: 13373.4707\n",
            "Epoch 493: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13392.7402 - mean_absolute_error: 13392.7402 - val_loss: 28799.2695 - val_mean_absolute_error: 28799.2695\n",
            "Epoch 494/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 13061.9658 - mean_absolute_error: 13061.9658\n",
            "Epoch 494: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13066.4434 - mean_absolute_error: 13066.4434 - val_loss: 29701.0117 - val_mean_absolute_error: 29701.0117\n",
            "Epoch 495/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 13053.3584 - mean_absolute_error: 13053.3584\n",
            "Epoch 495: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13094.3301 - mean_absolute_error: 13094.3301 - val_loss: 29199.2266 - val_mean_absolute_error: 29199.2266\n",
            "Epoch 496/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 13357.3613 - mean_absolute_error: 13357.3613\n",
            "Epoch 496: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 13232.0293 - mean_absolute_error: 13232.0293 - val_loss: 29037.8730 - val_mean_absolute_error: 29037.8730\n",
            "Epoch 497/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13284.5566 - mean_absolute_error: 13284.5566\n",
            "Epoch 497: val_loss did not improve from 28760.77148\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 13267.8115 - mean_absolute_error: 13267.8115 - val_loss: 28914.3828 - val_mean_absolute_error: 28914.3828\n",
            "Epoch 498/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13524.5986 - mean_absolute_error: 13524.5986\n",
            "Epoch 498: val_loss improved from 28760.77148 to 28696.25977, saving model to Weights-498--28696.25977.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 13542.0840 - mean_absolute_error: 13542.0840 - val_loss: 28696.2598 - val_mean_absolute_error: 28696.2598\n",
            "Epoch 499/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 13814.5293 - mean_absolute_error: 13814.5293\n",
            "Epoch 499: val_loss did not improve from 28696.25977\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 13828.8818 - mean_absolute_error: 13828.8818 - val_loss: 28824.0625 - val_mean_absolute_error: 28824.0625\n",
            "Epoch 500/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 13459.0869 - mean_absolute_error: 13459.0869\n",
            "Epoch 500: val_loss did not improve from 28696.25977\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 13460.0293 - mean_absolute_error: 13460.0293 - val_loss: 29973.0703 - val_mean_absolute_error: 29973.0703\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51b761ee80>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Y42dnt8evM"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_XboCvj9Js1",
        "outputId": "2243f0da-e194-4303-98cf-3aba86b4c58a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 1s 3ms/step\n",
            "28/28 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "y_train_pred = NN_model.predict(X_train)\n",
        "y_test_pred = NN_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F3R1G3Ju0iMI"
      },
      "outputs": [],
      "source": [
        "y_train_pred = y_train_pred.flatten()\n",
        "y_test_pred = y_test_pred.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tJAk-m_cgYyH"
      },
      "outputs": [],
      "source": [
        "train_n = X_train.shape[0]\n",
        "train_p = X_train.shape[1]\n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "train_rmse = mean_squared_error(y_train, y_train_pred, squared = False)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (train_n - 1) / (train_n - train_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WCr5L48yhgPP"
      },
      "outputs": [],
      "source": [
        "test_n = X_test.shape[0]\n",
        "test_p = X_test.shape[1]\n",
        "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared = False)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (test_n - 1) / (test_n - test_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFGOvIbd-tiQ",
        "outputId": "0d26b8d5-7056-496e-f6f9-1dcf07165cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train MAPE: 3.11%\n",
            "Train RMSE: 27133.811856856406\n",
            "Train R2: 0.9743566936776727\n",
            "Train Adj R2: 0.9738071412108751\n",
            "\n",
            "Test MAPE: 5.30%\n",
            "Test RMSE: 43526.44345815741\n",
            "Test R2: 0.930602326786164\n",
            "Test Adj R2: 0.9242387235422682\n"
          ]
        }
      ],
      "source": [
        "print(\"Train MAPE: {:.2f}%\".format(train_mape * 100))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Train R2:\", train_r2)\n",
        "print(\"Train Adj R2:\", train_adj_r2)\n",
        "print()\n",
        "print(\"Test MAPE: {:.2f}%\".format(test_mape * 100))\n",
        "print(\"Test RMSE:\", test_rmse)\n",
        "print(\"Test R2:\", test_r2)\n",
        "print(\"Test Adj R2:\", test_adj_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We0T1y5qg920"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V-5bY8wRlhYW"
      },
      "outputs": [],
      "source": [
        "train_residuals = y_train - y_train_pred\n",
        "test_residuals = y_test - y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "hFIQdHAWMI5L",
        "outputId": "39a3ea00-e3e6-4604-a45f-5cbbd5192160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train_pred, train_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Kn7jQRh7MI5O",
        "outputId": "8d3531d6-facb-49cc-e10c-e939952298fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, y_train_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "d0LiYX33hS3J",
        "outputId": "69049852-dee3-4488-c4fd-403b5321cf94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_test_pred, test_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "ueS_vdc-hS9P",
        "outputId": "47c2fc29-3262-4ea3-a86e-f0b119aebf2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_test, y_test_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_xZobpkkFO9"
      },
      "source": [
        "# Price / Sqm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBAtXJCjqlbd"
      },
      "source": [
        "## Data Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4dN6YVaEAmIU"
      },
      "outputs": [],
      "source": [
        "train_df2 = train_df.copy()\n",
        "test_df2 = test_df.copy()\n",
        "\n",
        "train_df2[\"floor_area_sqm\"] = train_df2[\"floor_area_sqm\"].apply(lambda x : x * 23.922319549360488 + 95.09078798185942)\n",
        "test_df2[\"floor_area_sqm\"] = test_df2[\"floor_area_sqm\"].apply(lambda x : x * 23.922319549360488 + 95.09078798185942)\n",
        "\n",
        "train_df2[\"resale_price_per_sqm\"] = train_df2[\"resale_price\"] / train_df2[\"floor_area_sqm\"]\n",
        "test_df2[\"resale_price_per_sqm\"] = test_df2[\"resale_price\"] / test_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3EmQ8jLMAhXM"
      },
      "outputs": [],
      "source": [
        "X_train = train_df2.drop([\"resale_price_per_sqm\", \"resale_price\", \"floor_area_sqm\"], axis = 1)\n",
        "X_test = test_df2.drop([\"resale_price_per_sqm\", \"resale_price\", \"floor_area_sqm\"], axis = 1)\n",
        "\n",
        "y_train = train_df2[\"resale_price_per_sqm\"]\n",
        "y_test = test_df2[\"resale_price_per_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "uirmAeUOqlbn",
        "outputId": "e9cac2ad-99b5-420d-95c6-31636cca83fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c0dcb15a-66e7-49aa-9af5-3c771b19080b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.676490</td>\n",
              "      <td>-0.443615</td>\n",
              "      <td>-1.143025</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.201327</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>1.352967</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.384369</td>\n",
              "      <td>-0.198538</td>\n",
              "      <td>-0.290599</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>2.178515</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.731095</td>\n",
              "      <td>1.252320</td>\n",
              "      <td>-0.613741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.054745</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>0.857112</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 73 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0dcb15a-66e7-49aa-9af5-3c771b19080b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0dcb15a-66e7-49aa-9af5-3c771b19080b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0dcb15a-66e7-49aa-9af5-3c771b19080b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   nearest_mrt_dist  total_resales_in_town  remaining_lease  town_BUKIT MERAH  \\\n",
              "0         -0.676490              -0.443615        -1.143025         -0.193241   \n",
              "1          0.201327               1.252320         1.352967         -0.193241   \n",
              "2         -0.384369              -0.198538        -0.290599         -0.193241   \n",
              "3          1.731095               1.252320        -0.613741         -0.193241   \n",
              "4          1.054745               1.291532         0.857112         -0.193241   \n",
              "\n",
              "   street_name_ANG MO KIO ST 51  street_name_DAWSON RD  flat_type_3 ROOM  \\\n",
              "0                     -0.050572              -0.044588          1.693641   \n",
              "1                     -0.050572              -0.044588         -0.590444   \n",
              "2                     -0.050572              -0.044588         -0.590444   \n",
              "3                     -0.050572              -0.044588         -0.590444   \n",
              "4                     -0.050572              -0.044588         -0.590444   \n",
              "\n",
              "   storey_range_01 TO 03  storey_range_04 TO 06  storey_range_19 TO 21  ...  \\\n",
              "0              -0.459028              -0.542843              -0.128147  ...   \n",
              "1               2.178515              -0.542843              -0.128147  ...   \n",
              "2               2.178515              -0.542843              -0.128147  ...   \n",
              "3              -0.459028              -0.542843              -0.128147  ...   \n",
              "4              -0.459028               1.842152              -0.128147  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "xT7-BAnxqlbo",
        "outputId": "fd8d8ecc-dc12-4a43-f149-c344c1721ee2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-04066ddd-6d5b-4ef5-b297-0650829e7a39\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nearest_mrt_dist</th>\n",
              "      <th>total_resales_in_town</th>\n",
              "      <th>remaining_lease</th>\n",
              "      <th>town_BUKIT MERAH</th>\n",
              "      <th>street_name_ANG MO KIO ST 51</th>\n",
              "      <th>street_name_DAWSON RD</th>\n",
              "      <th>flat_type_3 ROOM</th>\n",
              "      <th>storey_range_01 TO 03</th>\n",
              "      <th>storey_range_04 TO 06</th>\n",
              "      <th>storey_range_19 TO 21</th>\n",
              "      <th>...</th>\n",
              "      <th>town_SENGKANG</th>\n",
              "      <th>town_QUEENSTOWN</th>\n",
              "      <th>street_name_ANG MO KIO AVE 3</th>\n",
              "      <th>storey_range_22 TO 24</th>\n",
              "      <th>storey_range_25 TO 27</th>\n",
              "      <th>storey_range_28 TO 30</th>\n",
              "      <th>storey_range_31 TO 33</th>\n",
              "      <th>storey_range_37 TO 39</th>\n",
              "      <th>town_JURONG EAST</th>\n",
              "      <th>street_name_TELOK BLANGAH ST 31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004724</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.224825</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.306314</td>\n",
              "      <td>0.242602</td>\n",
              "      <td>1.403110</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.341318</td>\n",
              "      <td>0.644528</td>\n",
              "      <td>-0.808741</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>1.842152</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.265904</td>\n",
              "      <td>1.291532</td>\n",
              "      <td>1.386396</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>-0.590444</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.978913</td>\n",
              "      <td>0.036737</td>\n",
              "      <td>-1.209882</td>\n",
              "      <td>-0.193241</td>\n",
              "      <td>-0.050572</td>\n",
              "      <td>-0.044588</td>\n",
              "      <td>1.693641</td>\n",
              "      <td>-0.459028</td>\n",
              "      <td>-0.542843</td>\n",
              "      <td>-0.128147</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.261617</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.089443</td>\n",
              "      <td>-0.111079</td>\n",
              "      <td>-0.1001</td>\n",
              "      <td>-0.067497</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.047673</td>\n",
              "      <td>-0.148379</td>\n",
              "      <td>-0.075507</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 73 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-04066ddd-6d5b-4ef5-b297-0650829e7a39')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-04066ddd-6d5b-4ef5-b297-0650829e7a39 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-04066ddd-6d5b-4ef5-b297-0650829e7a39');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   nearest_mrt_dist  total_resales_in_town  remaining_lease  town_BUKIT MERAH  \\\n",
              "0          0.004724               1.291532         1.224825         -0.193241   \n",
              "1          1.306314               0.242602         1.403110         -0.193241   \n",
              "2         -0.341318               0.644528        -0.808741         -0.193241   \n",
              "3          1.265904               1.291532         1.386396         -0.193241   \n",
              "4         -0.978913               0.036737        -1.209882         -0.193241   \n",
              "\n",
              "   street_name_ANG MO KIO ST 51  street_name_DAWSON RD  flat_type_3 ROOM  \\\n",
              "0                     -0.050572              -0.044588         -0.590444   \n",
              "1                     -0.050572              -0.044588         -0.590444   \n",
              "2                     -0.050572              -0.044588         -0.590444   \n",
              "3                     -0.050572              -0.044588         -0.590444   \n",
              "4                     -0.050572              -0.044588          1.693641   \n",
              "\n",
              "   storey_range_01 TO 03  storey_range_04 TO 06  storey_range_19 TO 21  ...  \\\n",
              "0              -0.459028              -0.542843              -0.128147  ...   \n",
              "1              -0.459028              -0.542843              -0.128147  ...   \n",
              "2              -0.459028               1.842152              -0.128147  ...   \n",
              "3              -0.459028              -0.542843              -0.128147  ...   \n",
              "4              -0.459028              -0.542843              -0.128147  ...   \n",
              "\n",
              "   town_SENGKANG  town_QUEENSTOWN  street_name_ANG MO KIO AVE 3  \\\n",
              "0      -0.261617        -0.148379                     -0.089443   \n",
              "1      -0.261617        -0.148379                     -0.089443   \n",
              "2      -0.261617        -0.148379                     -0.089443   \n",
              "3      -0.261617        -0.148379                     -0.089443   \n",
              "4      -0.261617        -0.148379                     -0.089443   \n",
              "\n",
              "   storey_range_22 TO 24  storey_range_25 TO 27  storey_range_28 TO 30  \\\n",
              "0              -0.111079                -0.1001              -0.067497   \n",
              "1              -0.111079                -0.1001              -0.067497   \n",
              "2              -0.111079                -0.1001              -0.067497   \n",
              "3              -0.111079                -0.1001              -0.067497   \n",
              "4              -0.111079                -0.1001              -0.067497   \n",
              "\n",
              "   storey_range_31 TO 33  storey_range_37 TO 39  town_JURONG EAST  \\\n",
              "0              -0.047673              -0.047673         -0.148379   \n",
              "1              -0.047673              -0.047673         -0.148379   \n",
              "2              -0.047673              -0.047673         -0.148379   \n",
              "3              -0.047673              -0.047673         -0.148379   \n",
              "4              -0.047673              -0.047673         -0.148379   \n",
              "\n",
              "   street_name_TELOK BLANGAH ST 31  \n",
              "0                        -0.075507  \n",
              "1                        -0.075507  \n",
              "2                        -0.075507  \n",
              "3                        -0.075507  \n",
              "4                        -0.075507  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FoJ5lZqlbo"
      },
      "source": [
        "## Make the Deep Neural Network\n",
        " * Define a sequential model\n",
        " * Add some dense layers\n",
        " * Use '**relu**' as the activation function in the hidden layers\n",
        " * Use a '**normal**' initializer as the kernal_intializer \n",
        "           Initializers define the way to set the initial random weights of Keras layers.\n",
        " * We will use mean_absolute_error as a loss function\n",
        " * Define the output layer with only one node\n",
        " * Use 'linear 'as the activation function for the output layer\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9fDpPrDgqlbp"
      },
      "outputs": [],
      "source": [
        "NN_model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N40Bt3gwqlbp"
      },
      "source": [
        "**The Input Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CELIlIMxqlbp"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q69JzBQJqlbp"
      },
      "source": [
        "**The Hidden Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6Nt3ci4rqlbq"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7NNLiKqlbq"
      },
      "source": [
        "**The Output Layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "giwGd8R0qlbq"
      },
      "outputs": [],
      "source": [
        "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwKuKexeqlbq"
      },
      "source": [
        "**Compile the network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ35DiPxqlbr",
        "outputId": "216212d5-a918-43c5-f99e-75c806d3a48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 128)               9472      \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,337\n",
            "Trainable params: 174,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
        "NN_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NJ_0pF3qlbr"
      },
      "source": [
        "**Define a checkpoint callback :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QuqVlO-1qlbr"
      },
      "outputs": [],
      "source": [
        "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWDhsKYzqlbr"
      },
      "source": [
        "## Train the model :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hloNBZnIqlbs",
        "outputId": "f257749f-f44f-4506-d06c-3038423e89dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 3645.4592 - mean_absolute_error: 3645.4592\n",
            "Epoch 1: val_loss improved from inf to 699.48254, saving model to Weights-001--699.48254.hdf5\n",
            "89/89 [==============================] - 2s 9ms/step - loss: 3471.7070 - mean_absolute_error: 3471.7070 - val_loss: 699.4825 - val_mean_absolute_error: 699.4825\n",
            "Epoch 2/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 522.7350 - mean_absolute_error: 522.7350\n",
            "Epoch 2: val_loss improved from 699.48254 to 435.08951, saving model to Weights-002--435.08951.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 521.6706 - mean_absolute_error: 521.6706 - val_loss: 435.0895 - val_mean_absolute_error: 435.0895\n",
            "Epoch 3/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 424.9319 - mean_absolute_error: 424.9319\n",
            "Epoch 3: val_loss improved from 435.08951 to 413.66296, saving model to Weights-003--413.66296.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 423.5937 - mean_absolute_error: 423.5937 - val_loss: 413.6630 - val_mean_absolute_error: 413.6630\n",
            "Epoch 4/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 390.5397 - mean_absolute_error: 390.5397\n",
            "Epoch 4: val_loss improved from 413.66296 to 391.93546, saving model to Weights-004--391.93546.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 387.6144 - mean_absolute_error: 387.6144 - val_loss: 391.9355 - val_mean_absolute_error: 391.9355\n",
            "Epoch 5/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 361.2519 - mean_absolute_error: 361.2519\n",
            "Epoch 5: val_loss improved from 391.93546 to 361.02432, saving model to Weights-005--361.02432.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 358.4271 - mean_absolute_error: 358.4271 - val_loss: 361.0243 - val_mean_absolute_error: 361.0243\n",
            "Epoch 6/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 343.6712 - mean_absolute_error: 343.6712\n",
            "Epoch 6: val_loss did not improve from 361.02432\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 344.1726 - mean_absolute_error: 344.1726 - val_loss: 391.3731 - val_mean_absolute_error: 391.3731\n",
            "Epoch 7/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 343.0569 - mean_absolute_error: 343.0569\n",
            "Epoch 7: val_loss did not improve from 361.02432\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 342.3943 - mean_absolute_error: 342.3943 - val_loss: 398.2050 - val_mean_absolute_error: 398.2050\n",
            "Epoch 8/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 323.1322 - mean_absolute_error: 323.1322\n",
            "Epoch 8: val_loss did not improve from 361.02432\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 323.7537 - mean_absolute_error: 323.7537 - val_loss: 405.0711 - val_mean_absolute_error: 405.0711\n",
            "Epoch 9/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 327.9948 - mean_absolute_error: 327.9948\n",
            "Epoch 9: val_loss improved from 361.02432 to 352.20654, saving model to Weights-009--352.20654.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 326.7168 - mean_absolute_error: 326.7168 - val_loss: 352.2065 - val_mean_absolute_error: 352.2065\n",
            "Epoch 10/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 327.2003 - mean_absolute_error: 327.2003\n",
            "Epoch 10: val_loss improved from 352.20654 to 350.46500, saving model to Weights-010--350.46500.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 326.8092 - mean_absolute_error: 326.8092 - val_loss: 350.4650 - val_mean_absolute_error: 350.4650\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 304.5465 - mean_absolute_error: 304.5465\n",
            "Epoch 11: val_loss did not improve from 350.46500\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 304.5465 - mean_absolute_error: 304.5465 - val_loss: 378.8872 - val_mean_absolute_error: 378.8872\n",
            "Epoch 12/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 314.2769 - mean_absolute_error: 314.2769\n",
            "Epoch 12: val_loss did not improve from 350.46500\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 314.4356 - mean_absolute_error: 314.4356 - val_loss: 373.4261 - val_mean_absolute_error: 373.4261\n",
            "Epoch 13/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 298.5603 - mean_absolute_error: 298.5603\n",
            "Epoch 13: val_loss improved from 350.46500 to 340.70468, saving model to Weights-013--340.70468.hdf5\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 299.4388 - mean_absolute_error: 299.4388 - val_loss: 340.7047 - val_mean_absolute_error: 340.7047\n",
            "Epoch 14/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 291.1961 - mean_absolute_error: 291.1961\n",
            "Epoch 14: val_loss improved from 340.70468 to 337.95374, saving model to Weights-014--337.95374.hdf5\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 291.5769 - mean_absolute_error: 291.5769 - val_loss: 337.9537 - val_mean_absolute_error: 337.9537\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 281.1401 - mean_absolute_error: 281.1401\n",
            "Epoch 15: val_loss did not improve from 337.95374\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 281.1401 - mean_absolute_error: 281.1401 - val_loss: 355.7787 - val_mean_absolute_error: 355.7787\n",
            "Epoch 16/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 291.7911 - mean_absolute_error: 291.7911\n",
            "Epoch 16: val_loss did not improve from 337.95374\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 291.4061 - mean_absolute_error: 291.4061 - val_loss: 348.5156 - val_mean_absolute_error: 348.5156\n",
            "Epoch 17/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 280.7212 - mean_absolute_error: 280.7212\n",
            "Epoch 17: val_loss did not improve from 337.95374\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 281.3813 - mean_absolute_error: 281.3813 - val_loss: 374.5394 - val_mean_absolute_error: 374.5394\n",
            "Epoch 18/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 277.6970 - mean_absolute_error: 277.6970\n",
            "Epoch 18: val_loss improved from 337.95374 to 337.14764, saving model to Weights-018--337.14764.hdf5\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 278.7105 - mean_absolute_error: 278.7105 - val_loss: 337.1476 - val_mean_absolute_error: 337.1476\n",
            "Epoch 19/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 272.8845 - mean_absolute_error: 272.8845\n",
            "Epoch 19: val_loss improved from 337.14764 to 334.41904, saving model to Weights-019--334.41904.hdf5\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 272.7740 - mean_absolute_error: 272.7740 - val_loss: 334.4190 - val_mean_absolute_error: 334.4190\n",
            "Epoch 20/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 272.1593 - mean_absolute_error: 272.1593\n",
            "Epoch 20: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 271.8127 - mean_absolute_error: 271.8127 - val_loss: 352.1109 - val_mean_absolute_error: 352.1109\n",
            "Epoch 21/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 269.6690 - mean_absolute_error: 269.6690\n",
            "Epoch 21: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 269.4741 - mean_absolute_error: 269.4741 - val_loss: 338.5971 - val_mean_absolute_error: 338.5971\n",
            "Epoch 22/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 262.6624 - mean_absolute_error: 262.6624\n",
            "Epoch 22: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 263.0089 - mean_absolute_error: 263.0089 - val_loss: 337.2517 - val_mean_absolute_error: 337.2517\n",
            "Epoch 23/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 263.7605 - mean_absolute_error: 263.7605\n",
            "Epoch 23: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 263.8163 - mean_absolute_error: 263.8163 - val_loss: 351.1546 - val_mean_absolute_error: 351.1546\n",
            "Epoch 24/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 263.6469 - mean_absolute_error: 263.6469\n",
            "Epoch 24: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 263.8067 - mean_absolute_error: 263.8067 - val_loss: 356.7484 - val_mean_absolute_error: 356.7484\n",
            "Epoch 25/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 263.6992 - mean_absolute_error: 263.6992\n",
            "Epoch 25: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 263.7011 - mean_absolute_error: 263.7011 - val_loss: 348.1477 - val_mean_absolute_error: 348.1477\n",
            "Epoch 26/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 261.5531 - mean_absolute_error: 261.5531\n",
            "Epoch 26: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 261.8932 - mean_absolute_error: 261.8932 - val_loss: 379.1110 - val_mean_absolute_error: 379.1110\n",
            "Epoch 27/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 259.7109 - mean_absolute_error: 259.7109\n",
            "Epoch 27: val_loss did not improve from 334.41904\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 259.3700 - mean_absolute_error: 259.3700 - val_loss: 341.2624 - val_mean_absolute_error: 341.2624\n",
            "Epoch 28/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 260.8005 - mean_absolute_error: 260.8005\n",
            "Epoch 28: val_loss improved from 334.41904 to 327.07266, saving model to Weights-028--327.07266.hdf5\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 261.2975 - mean_absolute_error: 261.2975 - val_loss: 327.0727 - val_mean_absolute_error: 327.0727\n",
            "Epoch 29/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 256.9803 - mean_absolute_error: 256.9803\n",
            "Epoch 29: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 257.3307 - mean_absolute_error: 257.3307 - val_loss: 389.2767 - val_mean_absolute_error: 389.2767\n",
            "Epoch 30/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 262.0826 - mean_absolute_error: 262.0826\n",
            "Epoch 30: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 263.3011 - mean_absolute_error: 263.3011 - val_loss: 343.2300 - val_mean_absolute_error: 343.2300\n",
            "Epoch 31/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 247.3956 - mean_absolute_error: 247.3956\n",
            "Epoch 31: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 250.1902 - mean_absolute_error: 250.1902 - val_loss: 364.5320 - val_mean_absolute_error: 364.5320\n",
            "Epoch 32/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 253.8512 - mean_absolute_error: 253.8512\n",
            "Epoch 32: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 254.1857 - mean_absolute_error: 254.1857 - val_loss: 336.1801 - val_mean_absolute_error: 336.1801\n",
            "Epoch 33/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 254.8061 - mean_absolute_error: 254.8061\n",
            "Epoch 33: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 253.9611 - mean_absolute_error: 253.9611 - val_loss: 341.5032 - val_mean_absolute_error: 341.5032\n",
            "Epoch 34/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 249.4464 - mean_absolute_error: 249.4464\n",
            "Epoch 34: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 249.4912 - mean_absolute_error: 249.4912 - val_loss: 328.8612 - val_mean_absolute_error: 328.8612\n",
            "Epoch 35/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 251.5952 - mean_absolute_error: 251.5952\n",
            "Epoch 35: val_loss did not improve from 327.07266\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 251.4328 - mean_absolute_error: 251.4328 - val_loss: 332.3788 - val_mean_absolute_error: 332.3788\n",
            "Epoch 36/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 245.7283 - mean_absolute_error: 245.7283\n",
            "Epoch 36: val_loss improved from 327.07266 to 325.95468, saving model to Weights-036--325.95468.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 243.6158 - mean_absolute_error: 243.6158 - val_loss: 325.9547 - val_mean_absolute_error: 325.9547\n",
            "Epoch 37/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 242.8811 - mean_absolute_error: 242.8811\n",
            "Epoch 37: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 244.0412 - mean_absolute_error: 244.0412 - val_loss: 326.2110 - val_mean_absolute_error: 326.2110\n",
            "Epoch 38/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 242.2657 - mean_absolute_error: 242.2657\n",
            "Epoch 38: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 242.9170 - mean_absolute_error: 242.9170 - val_loss: 329.4008 - val_mean_absolute_error: 329.4008\n",
            "Epoch 39/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 253.8051 - mean_absolute_error: 253.8051\n",
            "Epoch 39: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 254.0592 - mean_absolute_error: 254.0592 - val_loss: 337.5033 - val_mean_absolute_error: 337.5033\n",
            "Epoch 40/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 246.4328 - mean_absolute_error: 246.4328\n",
            "Epoch 40: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 248.4406 - mean_absolute_error: 248.4406 - val_loss: 333.9163 - val_mean_absolute_error: 333.9163\n",
            "Epoch 41/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 246.4425 - mean_absolute_error: 246.4425\n",
            "Epoch 41: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 248.7341 - mean_absolute_error: 248.7341 - val_loss: 337.5727 - val_mean_absolute_error: 337.5727\n",
            "Epoch 42/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 240.8780 - mean_absolute_error: 240.8780\n",
            "Epoch 42: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 241.7436 - mean_absolute_error: 241.7436 - val_loss: 334.6081 - val_mean_absolute_error: 334.6081\n",
            "Epoch 43/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 242.2406 - mean_absolute_error: 242.2406\n",
            "Epoch 43: val_loss did not improve from 325.95468\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 243.0922 - mean_absolute_error: 243.0922 - val_loss: 346.1904 - val_mean_absolute_error: 346.1904\n",
            "Epoch 44/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 245.5346 - mean_absolute_error: 245.5346\n",
            "Epoch 44: val_loss improved from 325.95468 to 325.55466, saving model to Weights-044--325.55466.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 247.3471 - mean_absolute_error: 247.3471 - val_loss: 325.5547 - val_mean_absolute_error: 325.5547\n",
            "Epoch 45/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 231.4349 - mean_absolute_error: 231.4349\n",
            "Epoch 45: val_loss improved from 325.55466 to 317.62238, saving model to Weights-045--317.62238.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 231.1324 - mean_absolute_error: 231.1324 - val_loss: 317.6224 - val_mean_absolute_error: 317.6224\n",
            "Epoch 46/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 230.9009 - mean_absolute_error: 230.9009\n",
            "Epoch 46: val_loss did not improve from 317.62238\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 230.0909 - mean_absolute_error: 230.0909 - val_loss: 318.2807 - val_mean_absolute_error: 318.2807\n",
            "Epoch 47/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 238.0401 - mean_absolute_error: 238.0401\n",
            "Epoch 47: val_loss did not improve from 317.62238\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 238.1262 - mean_absolute_error: 238.1262 - val_loss: 330.7088 - val_mean_absolute_error: 330.7088\n",
            "Epoch 48/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 240.7560 - mean_absolute_error: 240.7560\n",
            "Epoch 48: val_loss did not improve from 317.62238\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 241.3480 - mean_absolute_error: 241.3480 - val_loss: 327.6115 - val_mean_absolute_error: 327.6115\n",
            "Epoch 49/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 229.2789 - mean_absolute_error: 229.2789\n",
            "Epoch 49: val_loss improved from 317.62238 to 317.47485, saving model to Weights-049--317.47485.hdf5\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 229.2146 - mean_absolute_error: 229.2146 - val_loss: 317.4749 - val_mean_absolute_error: 317.4749\n",
            "Epoch 50/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 228.1939 - mean_absolute_error: 228.1939\n",
            "Epoch 50: val_loss did not improve from 317.47485\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 228.2075 - mean_absolute_error: 228.2075 - val_loss: 345.7991 - val_mean_absolute_error: 345.7991\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 239.8424 - mean_absolute_error: 239.8424\n",
            "Epoch 51: val_loss did not improve from 317.47485\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 239.8424 - mean_absolute_error: 239.8424 - val_loss: 329.4909 - val_mean_absolute_error: 329.4909\n",
            "Epoch 52/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 236.2775 - mean_absolute_error: 236.2775\n",
            "Epoch 52: val_loss did not improve from 317.47485\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 236.0970 - mean_absolute_error: 236.0970 - val_loss: 325.5135 - val_mean_absolute_error: 325.5135\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 229.4645 - mean_absolute_error: 229.4645\n",
            "Epoch 53: val_loss improved from 317.47485 to 314.34586, saving model to Weights-053--314.34586.hdf5\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 229.4645 - mean_absolute_error: 229.4645 - val_loss: 314.3459 - val_mean_absolute_error: 314.3459\n",
            "Epoch 54/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 222.8485 - mean_absolute_error: 222.8485\n",
            "Epoch 54: val_loss did not improve from 314.34586\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 224.6170 - mean_absolute_error: 224.6170 - val_loss: 322.6075 - val_mean_absolute_error: 322.6075\n",
            "Epoch 55/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 227.8238 - mean_absolute_error: 227.8238\n",
            "Epoch 55: val_loss improved from 314.34586 to 309.44333, saving model to Weights-055--309.44333.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 227.7914 - mean_absolute_error: 227.7914 - val_loss: 309.4433 - val_mean_absolute_error: 309.4433\n",
            "Epoch 56/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 221.9744 - mean_absolute_error: 221.9744\n",
            "Epoch 56: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 221.7734 - mean_absolute_error: 221.7734 - val_loss: 325.1959 - val_mean_absolute_error: 325.1959\n",
            "Epoch 57/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 229.5065 - mean_absolute_error: 229.5065\n",
            "Epoch 57: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 228.2509 - mean_absolute_error: 228.2509 - val_loss: 314.9822 - val_mean_absolute_error: 314.9822\n",
            "Epoch 58/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 223.3801 - mean_absolute_error: 223.3801\n",
            "Epoch 58: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 225.0677 - mean_absolute_error: 225.0677 - val_loss: 317.0195 - val_mean_absolute_error: 317.0195\n",
            "Epoch 59/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 221.7338 - mean_absolute_error: 221.7338\n",
            "Epoch 59: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 223.2298 - mean_absolute_error: 223.2298 - val_loss: 365.8726 - val_mean_absolute_error: 365.8726\n",
            "Epoch 60/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 232.9690 - mean_absolute_error: 232.9690\n",
            "Epoch 60: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 232.5589 - mean_absolute_error: 232.5589 - val_loss: 320.6119 - val_mean_absolute_error: 320.6119\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 220.9414 - mean_absolute_error: 220.9414\n",
            "Epoch 61: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 220.9414 - mean_absolute_error: 220.9414 - val_loss: 329.3717 - val_mean_absolute_error: 329.3717\n",
            "Epoch 62/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 224.6441 - mean_absolute_error: 224.6441\n",
            "Epoch 62: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 223.7729 - mean_absolute_error: 223.7729 - val_loss: 323.6330 - val_mean_absolute_error: 323.6330\n",
            "Epoch 63/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 241.5203 - mean_absolute_error: 241.5203\n",
            "Epoch 63: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 240.8488 - mean_absolute_error: 240.8488 - val_loss: 320.0076 - val_mean_absolute_error: 320.0076\n",
            "Epoch 64/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 224.5182 - mean_absolute_error: 224.5182\n",
            "Epoch 64: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 224.9648 - mean_absolute_error: 224.9648 - val_loss: 311.9684 - val_mean_absolute_error: 311.9684\n",
            "Epoch 65/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 219.9953 - mean_absolute_error: 219.9953\n",
            "Epoch 65: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 219.2407 - mean_absolute_error: 219.2407 - val_loss: 323.1590 - val_mean_absolute_error: 323.1590\n",
            "Epoch 66/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 217.9184 - mean_absolute_error: 217.9184\n",
            "Epoch 66: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 217.7733 - mean_absolute_error: 217.7733 - val_loss: 320.5935 - val_mean_absolute_error: 320.5935\n",
            "Epoch 67/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 216.0302 - mean_absolute_error: 216.0302\n",
            "Epoch 67: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 216.5529 - mean_absolute_error: 216.5529 - val_loss: 331.1046 - val_mean_absolute_error: 331.1046\n",
            "Epoch 68/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 222.8355 - mean_absolute_error: 222.8355\n",
            "Epoch 68: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 223.7203 - mean_absolute_error: 223.7203 - val_loss: 320.7910 - val_mean_absolute_error: 320.7910\n",
            "Epoch 69/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 223.7719 - mean_absolute_error: 223.7719\n",
            "Epoch 69: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 223.2709 - mean_absolute_error: 223.2709 - val_loss: 355.1250 - val_mean_absolute_error: 355.1250\n",
            "Epoch 70/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 236.1092 - mean_absolute_error: 236.1092\n",
            "Epoch 70: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 233.1140 - mean_absolute_error: 233.1140 - val_loss: 322.0415 - val_mean_absolute_error: 322.0415\n",
            "Epoch 71/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 219.1973 - mean_absolute_error: 219.1973\n",
            "Epoch 71: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 219.3629 - mean_absolute_error: 219.3629 - val_loss: 339.9611 - val_mean_absolute_error: 339.9611\n",
            "Epoch 72/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 219.0100 - mean_absolute_error: 219.0100\n",
            "Epoch 72: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 219.7629 - mean_absolute_error: 219.7629 - val_loss: 332.0016 - val_mean_absolute_error: 332.0016\n",
            "Epoch 73/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 234.2775 - mean_absolute_error: 234.2775\n",
            "Epoch 73: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 234.2616 - mean_absolute_error: 234.2616 - val_loss: 326.5598 - val_mean_absolute_error: 326.5598\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 216.8649 - mean_absolute_error: 216.8649\n",
            "Epoch 74: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 216.8649 - mean_absolute_error: 216.8649 - val_loss: 328.5996 - val_mean_absolute_error: 328.5996\n",
            "Epoch 75/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 215.5474 - mean_absolute_error: 215.5474\n",
            "Epoch 75: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 216.2736 - mean_absolute_error: 216.2736 - val_loss: 321.5530 - val_mean_absolute_error: 321.5530\n",
            "Epoch 76/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 224.2328 - mean_absolute_error: 224.2328\n",
            "Epoch 76: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 223.3102 - mean_absolute_error: 223.3102 - val_loss: 326.6967 - val_mean_absolute_error: 326.6967\n",
            "Epoch 77/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 219.5927 - mean_absolute_error: 219.5927\n",
            "Epoch 77: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 219.3572 - mean_absolute_error: 219.3572 - val_loss: 349.2450 - val_mean_absolute_error: 349.2450\n",
            "Epoch 78/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 217.9112 - mean_absolute_error: 217.9112\n",
            "Epoch 78: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 217.2890 - mean_absolute_error: 217.2890 - val_loss: 335.6778 - val_mean_absolute_error: 335.6778\n",
            "Epoch 79/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 220.7996 - mean_absolute_error: 220.7996\n",
            "Epoch 79: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 219.9932 - mean_absolute_error: 219.9932 - val_loss: 325.0119 - val_mean_absolute_error: 325.0119\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 217.4764 - mean_absolute_error: 217.4764\n",
            "Epoch 80: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 217.4764 - mean_absolute_error: 217.4764 - val_loss: 325.3393 - val_mean_absolute_error: 325.3393\n",
            "Epoch 81/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 213.3347 - mean_absolute_error: 213.3347\n",
            "Epoch 81: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 213.1534 - mean_absolute_error: 213.1534 - val_loss: 322.8860 - val_mean_absolute_error: 322.8860\n",
            "Epoch 82/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 214.6241 - mean_absolute_error: 214.6241\n",
            "Epoch 82: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 214.5623 - mean_absolute_error: 214.5623 - val_loss: 324.9666 - val_mean_absolute_error: 324.9666\n",
            "Epoch 83/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 225.2229 - mean_absolute_error: 225.2229\n",
            "Epoch 83: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 224.1951 - mean_absolute_error: 224.1951 - val_loss: 321.6483 - val_mean_absolute_error: 321.6483\n",
            "Epoch 84/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 222.8453 - mean_absolute_error: 222.8453\n",
            "Epoch 84: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 223.2290 - mean_absolute_error: 223.2290 - val_loss: 348.7521 - val_mean_absolute_error: 348.7521\n",
            "Epoch 85/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 224.8515 - mean_absolute_error: 224.8515\n",
            "Epoch 85: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 0s 5ms/step - loss: 224.0229 - mean_absolute_error: 224.0229 - val_loss: 322.5911 - val_mean_absolute_error: 322.5911\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 213.3560 - mean_absolute_error: 213.3560\n",
            "Epoch 86: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 213.3560 - mean_absolute_error: 213.3560 - val_loss: 324.6855 - val_mean_absolute_error: 324.6855\n",
            "Epoch 87/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 234.0033 - mean_absolute_error: 234.0033\n",
            "Epoch 87: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 233.3911 - mean_absolute_error: 233.3911 - val_loss: 331.4733 - val_mean_absolute_error: 331.4733\n",
            "Epoch 88/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 216.0597 - mean_absolute_error: 216.0597\n",
            "Epoch 88: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 0s 6ms/step - loss: 216.3140 - mean_absolute_error: 216.3140 - val_loss: 312.7518 - val_mean_absolute_error: 312.7518\n",
            "Epoch 89/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 209.2148 - mean_absolute_error: 209.2148\n",
            "Epoch 89: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 207.7198 - mean_absolute_error: 207.7198 - val_loss: 315.3116 - val_mean_absolute_error: 315.3116\n",
            "Epoch 90/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 209.9746 - mean_absolute_error: 209.9746\n",
            "Epoch 90: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 210.8594 - mean_absolute_error: 210.8594 - val_loss: 333.2031 - val_mean_absolute_error: 333.2031\n",
            "Epoch 91/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 216.3649 - mean_absolute_error: 216.3649\n",
            "Epoch 91: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 216.5720 - mean_absolute_error: 216.5720 - val_loss: 319.6751 - val_mean_absolute_error: 319.6751\n",
            "Epoch 92/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 212.5484 - mean_absolute_error: 212.5484\n",
            "Epoch 92: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 212.6535 - mean_absolute_error: 212.6535 - val_loss: 318.4359 - val_mean_absolute_error: 318.4359\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 208.5954 - mean_absolute_error: 208.5954\n",
            "Epoch 93: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 208.5954 - mean_absolute_error: 208.5954 - val_loss: 313.2858 - val_mean_absolute_error: 313.2858\n",
            "Epoch 94/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 204.0291 - mean_absolute_error: 204.0291\n",
            "Epoch 94: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 204.2065 - mean_absolute_error: 204.2065 - val_loss: 321.2850 - val_mean_absolute_error: 321.2850\n",
            "Epoch 95/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 214.3804 - mean_absolute_error: 214.3804\n",
            "Epoch 95: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 214.1795 - mean_absolute_error: 214.1795 - val_loss: 338.1618 - val_mean_absolute_error: 338.1618\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 212.8404 - mean_absolute_error: 212.8404\n",
            "Epoch 96: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 212.8404 - mean_absolute_error: 212.8404 - val_loss: 319.5258 - val_mean_absolute_error: 319.5258\n",
            "Epoch 97/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 210.0107 - mean_absolute_error: 210.0107\n",
            "Epoch 97: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 211.7730 - mean_absolute_error: 211.7730 - val_loss: 350.2739 - val_mean_absolute_error: 350.2739\n",
            "Epoch 98/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 224.0652 - mean_absolute_error: 224.0652\n",
            "Epoch 98: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 223.9392 - mean_absolute_error: 223.9392 - val_loss: 325.2865 - val_mean_absolute_error: 325.2865\n",
            "Epoch 99/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 220.0179 - mean_absolute_error: 220.0179\n",
            "Epoch 99: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 218.6042 - mean_absolute_error: 218.6042 - val_loss: 316.7829 - val_mean_absolute_error: 316.7829\n",
            "Epoch 100/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 206.5492 - mean_absolute_error: 206.5492\n",
            "Epoch 100: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 205.2817 - mean_absolute_error: 205.2817 - val_loss: 329.0865 - val_mean_absolute_error: 329.0865\n",
            "Epoch 101/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 209.0855 - mean_absolute_error: 209.0855\n",
            "Epoch 101: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 210.2771 - mean_absolute_error: 210.2771 - val_loss: 312.5799 - val_mean_absolute_error: 312.5799\n",
            "Epoch 102/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 205.1822 - mean_absolute_error: 205.1822\n",
            "Epoch 102: val_loss did not improve from 309.44333\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 204.9421 - mean_absolute_error: 204.9421 - val_loss: 328.7103 - val_mean_absolute_error: 328.7103\n",
            "Epoch 103/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 208.1614 - mean_absolute_error: 208.1614\n",
            "Epoch 103: val_loss improved from 309.44333 to 307.51535, saving model to Weights-103--307.51535.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 208.9939 - mean_absolute_error: 208.9939 - val_loss: 307.5154 - val_mean_absolute_error: 307.5154\n",
            "Epoch 104/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 209.3401 - mean_absolute_error: 209.3401\n",
            "Epoch 104: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 209.0105 - mean_absolute_error: 209.0105 - val_loss: 347.4727 - val_mean_absolute_error: 347.4727\n",
            "Epoch 105/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 209.4435 - mean_absolute_error: 209.4435\n",
            "Epoch 105: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 209.0934 - mean_absolute_error: 209.0934 - val_loss: 308.1274 - val_mean_absolute_error: 308.1274\n",
            "Epoch 106/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 205.1278 - mean_absolute_error: 205.1278\n",
            "Epoch 106: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 207.2040 - mean_absolute_error: 207.2040 - val_loss: 321.5496 - val_mean_absolute_error: 321.5496\n",
            "Epoch 107/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 216.2277 - mean_absolute_error: 216.2277\n",
            "Epoch 107: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 215.9193 - mean_absolute_error: 215.9193 - val_loss: 319.9510 - val_mean_absolute_error: 319.9510\n",
            "Epoch 108/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 205.9717 - mean_absolute_error: 205.9717\n",
            "Epoch 108: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 203.9889 - mean_absolute_error: 203.9889 - val_loss: 318.2369 - val_mean_absolute_error: 318.2369\n",
            "Epoch 109/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 200.3964 - mean_absolute_error: 200.3964\n",
            "Epoch 109: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 200.4635 - mean_absolute_error: 200.4635 - val_loss: 317.2504 - val_mean_absolute_error: 317.2504\n",
            "Epoch 110/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 201.3791 - mean_absolute_error: 201.3791\n",
            "Epoch 110: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 200.8806 - mean_absolute_error: 200.8806 - val_loss: 314.7946 - val_mean_absolute_error: 314.7946\n",
            "Epoch 111/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 206.3819 - mean_absolute_error: 206.3819\n",
            "Epoch 111: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 206.3452 - mean_absolute_error: 206.3452 - val_loss: 326.1681 - val_mean_absolute_error: 326.1681\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 213.6873 - mean_absolute_error: 213.6873\n",
            "Epoch 112: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 213.6873 - mean_absolute_error: 213.6873 - val_loss: 326.5406 - val_mean_absolute_error: 326.5406\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 203.7845 - mean_absolute_error: 203.7845\n",
            "Epoch 113: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 203.7845 - mean_absolute_error: 203.7845 - val_loss: 314.3422 - val_mean_absolute_error: 314.3422\n",
            "Epoch 114/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 210.8902 - mean_absolute_error: 210.8902\n",
            "Epoch 114: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 210.7698 - mean_absolute_error: 210.7698 - val_loss: 365.5419 - val_mean_absolute_error: 365.5419\n",
            "Epoch 115/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 212.0495 - mean_absolute_error: 212.0495\n",
            "Epoch 115: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 212.3672 - mean_absolute_error: 212.3672 - val_loss: 317.8365 - val_mean_absolute_error: 317.8365\n",
            "Epoch 116/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 213.7419 - mean_absolute_error: 213.7419\n",
            "Epoch 116: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 214.6954 - mean_absolute_error: 214.6954 - val_loss: 322.7950 - val_mean_absolute_error: 322.7950\n",
            "Epoch 117/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 202.2878 - mean_absolute_error: 202.2878\n",
            "Epoch 117: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 202.3896 - mean_absolute_error: 202.3896 - val_loss: 313.8369 - val_mean_absolute_error: 313.8369\n",
            "Epoch 118/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 211.2275 - mean_absolute_error: 211.2275\n",
            "Epoch 118: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 210.0984 - mean_absolute_error: 210.0984 - val_loss: 323.8905 - val_mean_absolute_error: 323.8905\n",
            "Epoch 119/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 198.5909 - mean_absolute_error: 198.5909\n",
            "Epoch 119: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 199.6123 - mean_absolute_error: 199.6123 - val_loss: 326.5023 - val_mean_absolute_error: 326.5023\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 215.0517 - mean_absolute_error: 215.0517\n",
            "Epoch 120: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 215.0517 - mean_absolute_error: 215.0517 - val_loss: 327.3741 - val_mean_absolute_error: 327.3741\n",
            "Epoch 121/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 201.0738 - mean_absolute_error: 201.0738\n",
            "Epoch 121: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 201.2612 - mean_absolute_error: 201.2612 - val_loss: 317.4925 - val_mean_absolute_error: 317.4925\n",
            "Epoch 122/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 203.0607 - mean_absolute_error: 203.0607\n",
            "Epoch 122: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 203.3029 - mean_absolute_error: 203.3029 - val_loss: 316.9683 - val_mean_absolute_error: 316.9683\n",
            "Epoch 123/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 194.2614 - mean_absolute_error: 194.2614\n",
            "Epoch 123: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 195.1440 - mean_absolute_error: 195.1440 - val_loss: 316.6179 - val_mean_absolute_error: 316.6179\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 194.1496 - mean_absolute_error: 194.1496\n",
            "Epoch 124: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 194.1496 - mean_absolute_error: 194.1496 - val_loss: 317.2346 - val_mean_absolute_error: 317.2346\n",
            "Epoch 125/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 200.9341 - mean_absolute_error: 200.9341\n",
            "Epoch 125: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 201.0892 - mean_absolute_error: 201.0892 - val_loss: 319.8289 - val_mean_absolute_error: 319.8289\n",
            "Epoch 126/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 221.3092 - mean_absolute_error: 221.3092\n",
            "Epoch 126: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 219.6095 - mean_absolute_error: 219.6095 - val_loss: 314.9276 - val_mean_absolute_error: 314.9276\n",
            "Epoch 127/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 201.0661 - mean_absolute_error: 201.0661\n",
            "Epoch 127: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 201.6390 - mean_absolute_error: 201.6390 - val_loss: 313.6508 - val_mean_absolute_error: 313.6508\n",
            "Epoch 128/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 206.9374 - mean_absolute_error: 206.9374\n",
            "Epoch 128: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 206.6942 - mean_absolute_error: 206.6942 - val_loss: 314.5369 - val_mean_absolute_error: 314.5369\n",
            "Epoch 129/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 203.0064 - mean_absolute_error: 203.0064\n",
            "Epoch 129: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 203.2969 - mean_absolute_error: 203.2969 - val_loss: 322.6029 - val_mean_absolute_error: 322.6029\n",
            "Epoch 130/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 197.2363 - mean_absolute_error: 197.2363\n",
            "Epoch 130: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 196.7474 - mean_absolute_error: 196.7474 - val_loss: 309.0677 - val_mean_absolute_error: 309.0677\n",
            "Epoch 131/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 207.5537 - mean_absolute_error: 207.5537\n",
            "Epoch 131: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 208.6193 - mean_absolute_error: 208.6193 - val_loss: 330.0342 - val_mean_absolute_error: 330.0342\n",
            "Epoch 132/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 191.6552 - mean_absolute_error: 191.6552\n",
            "Epoch 132: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 191.5279 - mean_absolute_error: 191.5279 - val_loss: 327.3142 - val_mean_absolute_error: 327.3142\n",
            "Epoch 133/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 194.6693 - mean_absolute_error: 194.6693\n",
            "Epoch 133: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 194.6772 - mean_absolute_error: 194.6772 - val_loss: 345.8138 - val_mean_absolute_error: 345.8138\n",
            "Epoch 134/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 195.1078 - mean_absolute_error: 195.1078\n",
            "Epoch 134: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 195.5543 - mean_absolute_error: 195.5543 - val_loss: 308.2153 - val_mean_absolute_error: 308.2153\n",
            "Epoch 135/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 199.8619 - mean_absolute_error: 199.8619\n",
            "Epoch 135: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 200.4412 - mean_absolute_error: 200.4412 - val_loss: 319.7883 - val_mean_absolute_error: 319.7883\n",
            "Epoch 136/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 199.1743 - mean_absolute_error: 199.1743\n",
            "Epoch 136: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 199.6530 - mean_absolute_error: 199.6530 - val_loss: 316.1283 - val_mean_absolute_error: 316.1283\n",
            "Epoch 137/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 196.7749 - mean_absolute_error: 196.7749\n",
            "Epoch 137: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 196.7265 - mean_absolute_error: 196.7265 - val_loss: 341.8116 - val_mean_absolute_error: 341.8116\n",
            "Epoch 138/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 195.8410 - mean_absolute_error: 195.8410\n",
            "Epoch 138: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 195.7739 - mean_absolute_error: 195.7739 - val_loss: 312.4867 - val_mean_absolute_error: 312.4867\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 196.9631 - mean_absolute_error: 196.9631\n",
            "Epoch 139: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 196.9631 - mean_absolute_error: 196.9631 - val_loss: 316.8116 - val_mean_absolute_error: 316.8116\n",
            "Epoch 140/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 187.0346 - mean_absolute_error: 187.0346\n",
            "Epoch 140: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 187.2688 - mean_absolute_error: 187.2688 - val_loss: 316.7484 - val_mean_absolute_error: 316.7484\n",
            "Epoch 141/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 186.1839 - mean_absolute_error: 186.1839\n",
            "Epoch 141: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 186.1180 - mean_absolute_error: 186.1180 - val_loss: 333.2621 - val_mean_absolute_error: 333.2621\n",
            "Epoch 142/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 196.8478 - mean_absolute_error: 196.8478\n",
            "Epoch 142: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 196.4592 - mean_absolute_error: 196.4592 - val_loss: 310.3907 - val_mean_absolute_error: 310.3907\n",
            "Epoch 143/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 191.3640 - mean_absolute_error: 191.3640\n",
            "Epoch 143: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 191.2719 - mean_absolute_error: 191.2719 - val_loss: 312.5964 - val_mean_absolute_error: 312.5964\n",
            "Epoch 144/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 198.2486 - mean_absolute_error: 198.2486\n",
            "Epoch 144: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 199.8173 - mean_absolute_error: 199.8173 - val_loss: 311.7588 - val_mean_absolute_error: 311.7588\n",
            "Epoch 145/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 191.2753 - mean_absolute_error: 191.2753\n",
            "Epoch 145: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 191.1516 - mean_absolute_error: 191.1516 - val_loss: 320.4320 - val_mean_absolute_error: 320.4320\n",
            "Epoch 146/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 190.7049 - mean_absolute_error: 190.7049\n",
            "Epoch 146: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 189.0468 - mean_absolute_error: 189.0468 - val_loss: 314.9405 - val_mean_absolute_error: 314.9405\n",
            "Epoch 147/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 189.0574 - mean_absolute_error: 189.0574\n",
            "Epoch 147: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 189.6196 - mean_absolute_error: 189.6196 - val_loss: 321.2703 - val_mean_absolute_error: 321.2703\n",
            "Epoch 148/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 206.8542 - mean_absolute_error: 206.8542\n",
            "Epoch 148: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 206.6150 - mean_absolute_error: 206.6150 - val_loss: 350.9161 - val_mean_absolute_error: 350.9161\n",
            "Epoch 149/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 203.2473 - mean_absolute_error: 203.2473\n",
            "Epoch 149: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 203.2374 - mean_absolute_error: 203.2374 - val_loss: 321.6933 - val_mean_absolute_error: 321.6933\n",
            "Epoch 150/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 190.9456 - mean_absolute_error: 190.9456\n",
            "Epoch 150: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 193.0268 - mean_absolute_error: 193.0268 - val_loss: 311.0408 - val_mean_absolute_error: 311.0408\n",
            "Epoch 151/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 186.2369 - mean_absolute_error: 186.2369\n",
            "Epoch 151: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 186.2549 - mean_absolute_error: 186.2549 - val_loss: 408.0259 - val_mean_absolute_error: 408.0259\n",
            "Epoch 152/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 196.5383 - mean_absolute_error: 196.5383\n",
            "Epoch 152: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 196.3964 - mean_absolute_error: 196.3964 - val_loss: 326.8938 - val_mean_absolute_error: 326.8938\n",
            "Epoch 153/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 202.4369 - mean_absolute_error: 202.4369\n",
            "Epoch 153: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 202.5679 - mean_absolute_error: 202.5679 - val_loss: 318.5364 - val_mean_absolute_error: 318.5364\n",
            "Epoch 154/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 205.7469 - mean_absolute_error: 205.7469\n",
            "Epoch 154: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 206.1212 - mean_absolute_error: 206.1212 - val_loss: 337.1772 - val_mean_absolute_error: 337.1772\n",
            "Epoch 155/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 197.5078 - mean_absolute_error: 197.5078\n",
            "Epoch 155: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 198.2348 - mean_absolute_error: 198.2348 - val_loss: 321.1262 - val_mean_absolute_error: 321.1262\n",
            "Epoch 156/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 193.9490 - mean_absolute_error: 193.9490\n",
            "Epoch 156: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 193.3696 - mean_absolute_error: 193.3696 - val_loss: 322.0891 - val_mean_absolute_error: 322.0891\n",
            "Epoch 157/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 200.8970 - mean_absolute_error: 200.8970\n",
            "Epoch 157: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 198.6061 - mean_absolute_error: 198.6061 - val_loss: 331.9246 - val_mean_absolute_error: 331.9246\n",
            "Epoch 158/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 193.6164 - mean_absolute_error: 193.6164\n",
            "Epoch 158: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 193.6727 - mean_absolute_error: 193.6727 - val_loss: 308.6017 - val_mean_absolute_error: 308.6017\n",
            "Epoch 159/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 205.4580 - mean_absolute_error: 205.4580\n",
            "Epoch 159: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 205.5004 - mean_absolute_error: 205.5004 - val_loss: 315.7960 - val_mean_absolute_error: 315.7960\n",
            "Epoch 160/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 186.6131 - mean_absolute_error: 186.6131\n",
            "Epoch 160: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 186.0527 - mean_absolute_error: 186.0527 - val_loss: 311.5455 - val_mean_absolute_error: 311.5455\n",
            "Epoch 161/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 188.6615 - mean_absolute_error: 188.6615\n",
            "Epoch 161: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 189.0083 - mean_absolute_error: 189.0083 - val_loss: 392.7069 - val_mean_absolute_error: 392.7069\n",
            "Epoch 162/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 202.0616 - mean_absolute_error: 202.0616\n",
            "Epoch 162: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 201.0809 - mean_absolute_error: 201.0809 - val_loss: 315.9550 - val_mean_absolute_error: 315.9550\n",
            "Epoch 163/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 197.6756 - mean_absolute_error: 197.6756\n",
            "Epoch 163: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 197.4910 - mean_absolute_error: 197.4910 - val_loss: 315.8984 - val_mean_absolute_error: 315.8984\n",
            "Epoch 164/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 188.8530 - mean_absolute_error: 188.8530\n",
            "Epoch 164: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 188.7937 - mean_absolute_error: 188.7937 - val_loss: 319.3074 - val_mean_absolute_error: 319.3074\n",
            "Epoch 165/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 187.6304 - mean_absolute_error: 187.6304\n",
            "Epoch 165: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 187.1908 - mean_absolute_error: 187.1908 - val_loss: 312.4239 - val_mean_absolute_error: 312.4239\n",
            "Epoch 166/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 187.5148 - mean_absolute_error: 187.5148\n",
            "Epoch 166: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 186.9604 - mean_absolute_error: 186.9604 - val_loss: 329.7757 - val_mean_absolute_error: 329.7757\n",
            "Epoch 167/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 194.8644 - mean_absolute_error: 194.8644\n",
            "Epoch 167: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 195.7210 - mean_absolute_error: 195.7210 - val_loss: 317.0962 - val_mean_absolute_error: 317.0962\n",
            "Epoch 168/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 191.1215 - mean_absolute_error: 191.1215\n",
            "Epoch 168: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 191.7529 - mean_absolute_error: 191.7529 - val_loss: 316.5328 - val_mean_absolute_error: 316.5328\n",
            "Epoch 169/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 198.3729 - mean_absolute_error: 198.3729\n",
            "Epoch 169: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 197.8561 - mean_absolute_error: 197.8561 - val_loss: 313.1233 - val_mean_absolute_error: 313.1233\n",
            "Epoch 170/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 192.0120 - mean_absolute_error: 192.0120\n",
            "Epoch 170: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 189.6560 - mean_absolute_error: 189.6560 - val_loss: 307.8624 - val_mean_absolute_error: 307.8624\n",
            "Epoch 171/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 189.0424 - mean_absolute_error: 189.0424\n",
            "Epoch 171: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 187.7668 - mean_absolute_error: 187.7668 - val_loss: 308.7683 - val_mean_absolute_error: 308.7683\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 187.4065 - mean_absolute_error: 187.4065\n",
            "Epoch 172: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 187.4065 - mean_absolute_error: 187.4065 - val_loss: 332.4231 - val_mean_absolute_error: 332.4231\n",
            "Epoch 173/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 184.3102 - mean_absolute_error: 184.3102\n",
            "Epoch 173: val_loss did not improve from 307.51535\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 184.9237 - mean_absolute_error: 184.9237 - val_loss: 308.4171 - val_mean_absolute_error: 308.4171\n",
            "Epoch 174/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 184.5408 - mean_absolute_error: 184.5408\n",
            "Epoch 174: val_loss improved from 307.51535 to 306.83484, saving model to Weights-174--306.83484.hdf5\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 184.5923 - mean_absolute_error: 184.5923 - val_loss: 306.8348 - val_mean_absolute_error: 306.8348\n",
            "Epoch 175/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 199.0461 - mean_absolute_error: 199.0461\n",
            "Epoch 175: val_loss did not improve from 306.83484\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 197.4805 - mean_absolute_error: 197.4805 - val_loss: 311.2502 - val_mean_absolute_error: 311.2502\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 184.6597 - mean_absolute_error: 184.6597\n",
            "Epoch 176: val_loss did not improve from 306.83484\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 184.6597 - mean_absolute_error: 184.6597 - val_loss: 319.6343 - val_mean_absolute_error: 319.6343\n",
            "Epoch 177/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 181.2867 - mean_absolute_error: 181.2867\n",
            "Epoch 177: val_loss improved from 306.83484 to 304.16364, saving model to Weights-177--304.16364.hdf5\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 183.3466 - mean_absolute_error: 183.3466 - val_loss: 304.1636 - val_mean_absolute_error: 304.1636\n",
            "Epoch 178/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 180.2360 - mean_absolute_error: 180.2360\n",
            "Epoch 178: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 178.8494 - mean_absolute_error: 178.8494 - val_loss: 306.7928 - val_mean_absolute_error: 306.7928\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 181.0830 - mean_absolute_error: 181.0830\n",
            "Epoch 179: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 181.0830 - mean_absolute_error: 181.0830 - val_loss: 317.7557 - val_mean_absolute_error: 317.7557\n",
            "Epoch 180/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 187.8150 - mean_absolute_error: 187.8150\n",
            "Epoch 180: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 188.0826 - mean_absolute_error: 188.0826 - val_loss: 315.7592 - val_mean_absolute_error: 315.7592\n",
            "Epoch 181/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 182.6887 - mean_absolute_error: 182.6887\n",
            "Epoch 181: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 182.9125 - mean_absolute_error: 182.9125 - val_loss: 327.2750 - val_mean_absolute_error: 327.2750\n",
            "Epoch 182/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 183.1428 - mean_absolute_error: 183.1428\n",
            "Epoch 182: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 182.7014 - mean_absolute_error: 182.7014 - val_loss: 321.6013 - val_mean_absolute_error: 321.6013\n",
            "Epoch 183/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 180.9036 - mean_absolute_error: 180.9036\n",
            "Epoch 183: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 180.0643 - mean_absolute_error: 180.0643 - val_loss: 314.9719 - val_mean_absolute_error: 314.9719\n",
            "Epoch 184/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 184.4987 - mean_absolute_error: 184.4987\n",
            "Epoch 184: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 17ms/step - loss: 184.5498 - mean_absolute_error: 184.5498 - val_loss: 320.6367 - val_mean_absolute_error: 320.6367\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 180.6422 - mean_absolute_error: 180.6422\n",
            "Epoch 185: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 2s 17ms/step - loss: 180.6422 - mean_absolute_error: 180.6422 - val_loss: 319.9959 - val_mean_absolute_error: 319.9959\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 184.4509 - mean_absolute_error: 184.4509\n",
            "Epoch 186: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 184.4509 - mean_absolute_error: 184.4509 - val_loss: 314.2770 - val_mean_absolute_error: 314.2770\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 193.7308 - mean_absolute_error: 193.7308\n",
            "Epoch 187: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 193.7308 - mean_absolute_error: 193.7308 - val_loss: 322.7359 - val_mean_absolute_error: 322.7359\n",
            "Epoch 188/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 178.5119 - mean_absolute_error: 178.5119\n",
            "Epoch 188: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 178.4156 - mean_absolute_error: 178.4156 - val_loss: 321.6793 - val_mean_absolute_error: 321.6793\n",
            "Epoch 189/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 184.7937 - mean_absolute_error: 184.7937\n",
            "Epoch 189: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 184.6092 - mean_absolute_error: 184.6092 - val_loss: 311.9279 - val_mean_absolute_error: 311.9279\n",
            "Epoch 190/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 175.6365 - mean_absolute_error: 175.6365\n",
            "Epoch 190: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 175.4552 - mean_absolute_error: 175.4552 - val_loss: 308.7458 - val_mean_absolute_error: 308.7458\n",
            "Epoch 191/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 179.0598 - mean_absolute_error: 179.0598\n",
            "Epoch 191: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 178.8193 - mean_absolute_error: 178.8193 - val_loss: 314.1198 - val_mean_absolute_error: 314.1198\n",
            "Epoch 192/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 180.0277 - mean_absolute_error: 180.0277\n",
            "Epoch 192: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 180.0379 - mean_absolute_error: 180.0379 - val_loss: 313.8822 - val_mean_absolute_error: 313.8822\n",
            "Epoch 193/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 185.7369 - mean_absolute_error: 185.7369\n",
            "Epoch 193: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 184.8691 - mean_absolute_error: 184.8691 - val_loss: 315.0135 - val_mean_absolute_error: 315.0135\n",
            "Epoch 194/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 177.8550 - mean_absolute_error: 177.8550\n",
            "Epoch 194: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 182.2788 - mean_absolute_error: 182.2788 - val_loss: 328.4924 - val_mean_absolute_error: 328.4924\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 177.8848 - mean_absolute_error: 177.8848\n",
            "Epoch 195: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 177.8848 - mean_absolute_error: 177.8848 - val_loss: 319.8616 - val_mean_absolute_error: 319.8616\n",
            "Epoch 196/500\n",
            "78/89 [=========================>....] - ETA: 0s - loss: 183.5118 - mean_absolute_error: 183.5118\n",
            "Epoch 196: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 184.0435 - mean_absolute_error: 184.0435 - val_loss: 340.1455 - val_mean_absolute_error: 340.1455\n",
            "Epoch 197/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 180.4993 - mean_absolute_error: 180.4993\n",
            "Epoch 197: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 179.8118 - mean_absolute_error: 179.8118 - val_loss: 327.3281 - val_mean_absolute_error: 327.3281\n",
            "Epoch 198/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 176.3691 - mean_absolute_error: 176.3691\n",
            "Epoch 198: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 176.6076 - mean_absolute_error: 176.6076 - val_loss: 314.0657 - val_mean_absolute_error: 314.0657\n",
            "Epoch 199/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 188.7270 - mean_absolute_error: 188.7270\n",
            "Epoch 199: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 189.1875 - mean_absolute_error: 189.1875 - val_loss: 306.6299 - val_mean_absolute_error: 306.6299\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 174.7422 - mean_absolute_error: 174.7422\n",
            "Epoch 200: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 174.7422 - mean_absolute_error: 174.7422 - val_loss: 307.6683 - val_mean_absolute_error: 307.6683\n",
            "Epoch 201/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 179.8611 - mean_absolute_error: 179.8611\n",
            "Epoch 201: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 180.1493 - mean_absolute_error: 180.1493 - val_loss: 306.7676 - val_mean_absolute_error: 306.7676\n",
            "Epoch 202/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 180.9802 - mean_absolute_error: 180.9802\n",
            "Epoch 202: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 181.5166 - mean_absolute_error: 181.5166 - val_loss: 333.2839 - val_mean_absolute_error: 333.2839\n",
            "Epoch 203/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 185.1326 - mean_absolute_error: 185.1326\n",
            "Epoch 203: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 185.2405 - mean_absolute_error: 185.2405 - val_loss: 312.7675 - val_mean_absolute_error: 312.7675\n",
            "Epoch 204/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 179.8507 - mean_absolute_error: 179.8507\n",
            "Epoch 204: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 179.7056 - mean_absolute_error: 179.7056 - val_loss: 318.5128 - val_mean_absolute_error: 318.5128\n",
            "Epoch 205/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 173.8537 - mean_absolute_error: 173.8537\n",
            "Epoch 205: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 173.8194 - mean_absolute_error: 173.8194 - val_loss: 307.5865 - val_mean_absolute_error: 307.5865\n",
            "Epoch 206/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 179.7309 - mean_absolute_error: 179.7309\n",
            "Epoch 206: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 179.2790 - mean_absolute_error: 179.2790 - val_loss: 306.5807 - val_mean_absolute_error: 306.5807\n",
            "Epoch 207/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 172.4538 - mean_absolute_error: 172.4538\n",
            "Epoch 207: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 173.7785 - mean_absolute_error: 173.7785 - val_loss: 328.0133 - val_mean_absolute_error: 328.0133\n",
            "Epoch 208/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 178.4693 - mean_absolute_error: 178.4693\n",
            "Epoch 208: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 178.2141 - mean_absolute_error: 178.2141 - val_loss: 331.3759 - val_mean_absolute_error: 331.3759\n",
            "Epoch 209/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 176.7867 - mean_absolute_error: 176.7867\n",
            "Epoch 209: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 175.2340 - mean_absolute_error: 175.2340 - val_loss: 309.9067 - val_mean_absolute_error: 309.9067\n",
            "Epoch 210/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 172.2203 - mean_absolute_error: 172.2203\n",
            "Epoch 210: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 171.6719 - mean_absolute_error: 171.6719 - val_loss: 314.1661 - val_mean_absolute_error: 314.1661\n",
            "Epoch 211/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 175.1205 - mean_absolute_error: 175.1205\n",
            "Epoch 211: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 174.8461 - mean_absolute_error: 174.8461 - val_loss: 310.1028 - val_mean_absolute_error: 310.1028\n",
            "Epoch 212/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 186.5440 - mean_absolute_error: 186.5440\n",
            "Epoch 212: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 186.6422 - mean_absolute_error: 186.6422 - val_loss: 317.7313 - val_mean_absolute_error: 317.7313\n",
            "Epoch 213/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 184.3849 - mean_absolute_error: 184.3849\n",
            "Epoch 213: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 183.5982 - mean_absolute_error: 183.5982 - val_loss: 316.2761 - val_mean_absolute_error: 316.2761\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 181.7820 - mean_absolute_error: 181.7820\n",
            "Epoch 214: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 181.7820 - mean_absolute_error: 181.7820 - val_loss: 324.1164 - val_mean_absolute_error: 324.1164\n",
            "Epoch 215/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 168.1699 - mean_absolute_error: 168.1699\n",
            "Epoch 215: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 168.1656 - mean_absolute_error: 168.1656 - val_loss: 315.8071 - val_mean_absolute_error: 315.8071\n",
            "Epoch 216/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 176.2067 - mean_absolute_error: 176.2067\n",
            "Epoch 216: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 175.1702 - mean_absolute_error: 175.1702 - val_loss: 320.9022 - val_mean_absolute_error: 320.9022\n",
            "Epoch 217/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 176.7917 - mean_absolute_error: 176.7917\n",
            "Epoch 217: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 177.6210 - mean_absolute_error: 177.6210 - val_loss: 310.5406 - val_mean_absolute_error: 310.5406\n",
            "Epoch 218/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 180.0566 - mean_absolute_error: 180.0566\n",
            "Epoch 218: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 181.2702 - mean_absolute_error: 181.2702 - val_loss: 313.3795 - val_mean_absolute_error: 313.3795\n",
            "Epoch 219/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 172.6441 - mean_absolute_error: 172.6441\n",
            "Epoch 219: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 172.8126 - mean_absolute_error: 172.8126 - val_loss: 310.1378 - val_mean_absolute_error: 310.1378\n",
            "Epoch 220/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 165.6844 - mean_absolute_error: 165.6844\n",
            "Epoch 220: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 165.0303 - mean_absolute_error: 165.0303 - val_loss: 313.0450 - val_mean_absolute_error: 313.0450\n",
            "Epoch 221/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 162.9621 - mean_absolute_error: 162.9621\n",
            "Epoch 221: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 163.1470 - mean_absolute_error: 163.1470 - val_loss: 324.2026 - val_mean_absolute_error: 324.2026\n",
            "Epoch 222/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 170.4788 - mean_absolute_error: 170.4788\n",
            "Epoch 222: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 170.0919 - mean_absolute_error: 170.0919 - val_loss: 309.5012 - val_mean_absolute_error: 309.5012\n",
            "Epoch 223/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 166.7122 - mean_absolute_error: 166.7122\n",
            "Epoch 223: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 167.4201 - mean_absolute_error: 167.4201 - val_loss: 314.6723 - val_mean_absolute_error: 314.6723\n",
            "Epoch 224/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 168.5548 - mean_absolute_error: 168.5548\n",
            "Epoch 224: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 168.2848 - mean_absolute_error: 168.2848 - val_loss: 318.8338 - val_mean_absolute_error: 318.8338\n",
            "Epoch 225/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 163.9805 - mean_absolute_error: 163.9805\n",
            "Epoch 225: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 164.7791 - mean_absolute_error: 164.7791 - val_loss: 335.4355 - val_mean_absolute_error: 335.4355\n",
            "Epoch 226/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 175.0773 - mean_absolute_error: 175.0773\n",
            "Epoch 226: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 174.8330 - mean_absolute_error: 174.8330 - val_loss: 323.0620 - val_mean_absolute_error: 323.0620\n",
            "Epoch 227/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 174.0490 - mean_absolute_error: 174.0490\n",
            "Epoch 227: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 174.2871 - mean_absolute_error: 174.2871 - val_loss: 322.6005 - val_mean_absolute_error: 322.6005\n",
            "Epoch 228/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 176.2988 - mean_absolute_error: 176.2988\n",
            "Epoch 228: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 175.9370 - mean_absolute_error: 175.9370 - val_loss: 316.7270 - val_mean_absolute_error: 316.7270\n",
            "Epoch 229/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 177.4165 - mean_absolute_error: 177.4165\n",
            "Epoch 229: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 177.6713 - mean_absolute_error: 177.6713 - val_loss: 312.6178 - val_mean_absolute_error: 312.6178\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 167.2854 - mean_absolute_error: 167.2854\n",
            "Epoch 230: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 167.2854 - mean_absolute_error: 167.2854 - val_loss: 313.8045 - val_mean_absolute_error: 313.8045\n",
            "Epoch 231/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 163.4997 - mean_absolute_error: 163.4997\n",
            "Epoch 231: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 164.5830 - mean_absolute_error: 164.5830 - val_loss: 344.6890 - val_mean_absolute_error: 344.6890\n",
            "Epoch 232/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 171.5593 - mean_absolute_error: 171.5593\n",
            "Epoch 232: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 171.6549 - mean_absolute_error: 171.6549 - val_loss: 322.7127 - val_mean_absolute_error: 322.7127\n",
            "Epoch 233/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 157.0151 - mean_absolute_error: 157.0151\n",
            "Epoch 233: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 158.5926 - mean_absolute_error: 158.5926 - val_loss: 319.8440 - val_mean_absolute_error: 319.8440\n",
            "Epoch 234/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 169.0482 - mean_absolute_error: 169.0482\n",
            "Epoch 234: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 168.4276 - mean_absolute_error: 168.4276 - val_loss: 314.0665 - val_mean_absolute_error: 314.0665\n",
            "Epoch 235/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 161.1222 - mean_absolute_error: 161.1222\n",
            "Epoch 235: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 160.7021 - mean_absolute_error: 160.7021 - val_loss: 315.8559 - val_mean_absolute_error: 315.8559\n",
            "Epoch 236/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 158.9720 - mean_absolute_error: 158.9720\n",
            "Epoch 236: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 158.9279 - mean_absolute_error: 158.9279 - val_loss: 319.8092 - val_mean_absolute_error: 319.8092\n",
            "Epoch 237/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 169.0812 - mean_absolute_error: 169.0812\n",
            "Epoch 237: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 169.3286 - mean_absolute_error: 169.3286 - val_loss: 335.4420 - val_mean_absolute_error: 335.4420\n",
            "Epoch 238/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 166.0278 - mean_absolute_error: 166.0278\n",
            "Epoch 238: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 167.0155 - mean_absolute_error: 167.0155 - val_loss: 311.0359 - val_mean_absolute_error: 311.0359\n",
            "Epoch 239/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 167.2986 - mean_absolute_error: 167.2986\n",
            "Epoch 239: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 167.4445 - mean_absolute_error: 167.4445 - val_loss: 309.8250 - val_mean_absolute_error: 309.8250\n",
            "Epoch 240/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 177.8860 - mean_absolute_error: 177.8860\n",
            "Epoch 240: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 175.5013 - mean_absolute_error: 175.5013 - val_loss: 313.4799 - val_mean_absolute_error: 313.4799\n",
            "Epoch 241/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 178.8090 - mean_absolute_error: 178.8090\n",
            "Epoch 241: val_loss did not improve from 304.16364\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 178.0922 - mean_absolute_error: 178.0922 - val_loss: 313.1654 - val_mean_absolute_error: 313.1654\n",
            "Epoch 242/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 154.8750 - mean_absolute_error: 154.8750\n",
            "Epoch 242: val_loss improved from 304.16364 to 303.41785, saving model to Weights-242--303.41785.hdf5\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 155.0870 - mean_absolute_error: 155.0870 - val_loss: 303.4178 - val_mean_absolute_error: 303.4178\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 166.4620 - mean_absolute_error: 166.4620\n",
            "Epoch 243: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 166.4620 - mean_absolute_error: 166.4620 - val_loss: 310.6766 - val_mean_absolute_error: 310.6766\n",
            "Epoch 244/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 165.9156 - mean_absolute_error: 165.9156\n",
            "Epoch 244: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 169.1811 - mean_absolute_error: 169.1811 - val_loss: 327.6507 - val_mean_absolute_error: 327.6507\n",
            "Epoch 245/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 170.6409 - mean_absolute_error: 170.6409\n",
            "Epoch 245: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 170.3074 - mean_absolute_error: 170.3074 - val_loss: 309.9289 - val_mean_absolute_error: 309.9289\n",
            "Epoch 246/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 159.0377 - mean_absolute_error: 159.0377\n",
            "Epoch 246: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 158.9549 - mean_absolute_error: 158.9549 - val_loss: 336.8635 - val_mean_absolute_error: 336.8635\n",
            "Epoch 247/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 164.0173 - mean_absolute_error: 164.0173\n",
            "Epoch 247: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 164.3439 - mean_absolute_error: 164.3439 - val_loss: 312.4699 - val_mean_absolute_error: 312.4699\n",
            "Epoch 248/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 167.7072 - mean_absolute_error: 167.7072\n",
            "Epoch 248: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 166.9440 - mean_absolute_error: 166.9440 - val_loss: 314.1211 - val_mean_absolute_error: 314.1211\n",
            "Epoch 249/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 164.2168 - mean_absolute_error: 164.2168\n",
            "Epoch 249: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 163.2852 - mean_absolute_error: 163.2852 - val_loss: 312.2760 - val_mean_absolute_error: 312.2760\n",
            "Epoch 250/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 162.5423 - mean_absolute_error: 162.5423\n",
            "Epoch 250: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 162.7132 - mean_absolute_error: 162.7132 - val_loss: 325.7088 - val_mean_absolute_error: 325.7088\n",
            "Epoch 251/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 159.8766 - mean_absolute_error: 159.8766\n",
            "Epoch 251: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 161.0052 - mean_absolute_error: 161.0052 - val_loss: 306.1936 - val_mean_absolute_error: 306.1936\n",
            "Epoch 252/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 171.9984 - mean_absolute_error: 171.9984\n",
            "Epoch 252: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 171.4346 - mean_absolute_error: 171.4346 - val_loss: 323.2769 - val_mean_absolute_error: 323.2769\n",
            "Epoch 253/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 168.6624 - mean_absolute_error: 168.6624\n",
            "Epoch 253: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 168.9294 - mean_absolute_error: 168.9294 - val_loss: 317.3359 - val_mean_absolute_error: 317.3359\n",
            "Epoch 254/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 173.1544 - mean_absolute_error: 173.1544\n",
            "Epoch 254: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 172.8851 - mean_absolute_error: 172.8851 - val_loss: 327.9553 - val_mean_absolute_error: 327.9553\n",
            "Epoch 255/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 168.1452 - mean_absolute_error: 168.1452\n",
            "Epoch 255: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 168.5835 - mean_absolute_error: 168.5835 - val_loss: 336.6764 - val_mean_absolute_error: 336.6764\n",
            "Epoch 256/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 162.6857 - mean_absolute_error: 162.6857\n",
            "Epoch 256: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 162.2542 - mean_absolute_error: 162.2542 - val_loss: 312.2038 - val_mean_absolute_error: 312.2038\n",
            "Epoch 257/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 162.8958 - mean_absolute_error: 162.8958\n",
            "Epoch 257: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 160.7315 - mean_absolute_error: 160.7315 - val_loss: 309.9849 - val_mean_absolute_error: 309.9849\n",
            "Epoch 258/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 155.6939 - mean_absolute_error: 155.6939\n",
            "Epoch 258: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 155.0815 - mean_absolute_error: 155.0815 - val_loss: 314.5036 - val_mean_absolute_error: 314.5036\n",
            "Epoch 259/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 155.0538 - mean_absolute_error: 155.0538\n",
            "Epoch 259: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 155.5142 - mean_absolute_error: 155.5142 - val_loss: 321.0370 - val_mean_absolute_error: 321.0370\n",
            "Epoch 260/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 157.9798 - mean_absolute_error: 157.9798\n",
            "Epoch 260: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 158.0188 - mean_absolute_error: 158.0188 - val_loss: 315.1520 - val_mean_absolute_error: 315.1520\n",
            "Epoch 261/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 162.1851 - mean_absolute_error: 162.1851\n",
            "Epoch 261: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 162.5918 - mean_absolute_error: 162.5918 - val_loss: 321.0639 - val_mean_absolute_error: 321.0639\n",
            "Epoch 262/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 164.0200 - mean_absolute_error: 164.0200\n",
            "Epoch 262: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 163.6751 - mean_absolute_error: 163.6751 - val_loss: 322.7957 - val_mean_absolute_error: 322.7957\n",
            "Epoch 263/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 157.2526 - mean_absolute_error: 157.2526\n",
            "Epoch 263: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 158.4516 - mean_absolute_error: 158.4516 - val_loss: 329.8348 - val_mean_absolute_error: 329.8348\n",
            "Epoch 264/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 154.8802 - mean_absolute_error: 154.8802\n",
            "Epoch 264: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 155.6085 - mean_absolute_error: 155.6085 - val_loss: 328.5412 - val_mean_absolute_error: 328.5412\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 152.6401 - mean_absolute_error: 152.6401\n",
            "Epoch 265: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 152.6401 - mean_absolute_error: 152.6401 - val_loss: 319.6402 - val_mean_absolute_error: 319.6402\n",
            "Epoch 266/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 157.5689 - mean_absolute_error: 157.5689\n",
            "Epoch 266: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 157.2106 - mean_absolute_error: 157.2106 - val_loss: 319.2667 - val_mean_absolute_error: 319.2667\n",
            "Epoch 267/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 157.3245 - mean_absolute_error: 157.3245\n",
            "Epoch 267: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 156.8230 - mean_absolute_error: 156.8230 - val_loss: 311.4592 - val_mean_absolute_error: 311.4592\n",
            "Epoch 268/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 153.5091 - mean_absolute_error: 153.5091\n",
            "Epoch 268: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 152.6882 - mean_absolute_error: 152.6882 - val_loss: 312.8345 - val_mean_absolute_error: 312.8345\n",
            "Epoch 269/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 157.7055 - mean_absolute_error: 157.7055\n",
            "Epoch 269: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 157.8942 - mean_absolute_error: 157.8942 - val_loss: 319.8812 - val_mean_absolute_error: 319.8812\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 153.9046 - mean_absolute_error: 153.9046\n",
            "Epoch 270: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 153.9046 - mean_absolute_error: 153.9046 - val_loss: 324.3894 - val_mean_absolute_error: 324.3894\n",
            "Epoch 271/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 145.1945 - mean_absolute_error: 145.1945\n",
            "Epoch 271: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 144.3379 - mean_absolute_error: 144.3379 - val_loss: 314.1526 - val_mean_absolute_error: 314.1526\n",
            "Epoch 272/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 159.2892 - mean_absolute_error: 159.2892\n",
            "Epoch 272: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 161.5147 - mean_absolute_error: 161.5147 - val_loss: 326.7498 - val_mean_absolute_error: 326.7498\n",
            "Epoch 273/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 154.9739 - mean_absolute_error: 154.9739\n",
            "Epoch 273: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 154.3986 - mean_absolute_error: 154.3986 - val_loss: 312.2507 - val_mean_absolute_error: 312.2507\n",
            "Epoch 274/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 150.4408 - mean_absolute_error: 150.4408\n",
            "Epoch 274: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 150.1277 - mean_absolute_error: 150.1277 - val_loss: 312.2654 - val_mean_absolute_error: 312.2654\n",
            "Epoch 275/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 153.5416 - mean_absolute_error: 153.5416\n",
            "Epoch 275: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 153.6777 - mean_absolute_error: 153.6777 - val_loss: 319.0681 - val_mean_absolute_error: 319.0681\n",
            "Epoch 276/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 154.5845 - mean_absolute_error: 154.5845\n",
            "Epoch 276: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 156.4915 - mean_absolute_error: 156.4915 - val_loss: 314.6333 - val_mean_absolute_error: 314.6333\n",
            "Epoch 277/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 151.5348 - mean_absolute_error: 151.5348\n",
            "Epoch 277: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 151.6615 - mean_absolute_error: 151.6615 - val_loss: 313.2475 - val_mean_absolute_error: 313.2475\n",
            "Epoch 278/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 146.0761 - mean_absolute_error: 146.0761\n",
            "Epoch 278: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 145.9059 - mean_absolute_error: 145.9059 - val_loss: 316.5911 - val_mean_absolute_error: 316.5911\n",
            "Epoch 279/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 156.7438 - mean_absolute_error: 156.7438\n",
            "Epoch 279: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 156.0064 - mean_absolute_error: 156.0064 - val_loss: 332.8956 - val_mean_absolute_error: 332.8956\n",
            "Epoch 280/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 153.5022 - mean_absolute_error: 153.5022\n",
            "Epoch 280: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 152.3572 - mean_absolute_error: 152.3572 - val_loss: 320.6560 - val_mean_absolute_error: 320.6560\n",
            "Epoch 281/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 154.4577 - mean_absolute_error: 154.4577\n",
            "Epoch 281: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 154.4930 - mean_absolute_error: 154.4930 - val_loss: 324.7624 - val_mean_absolute_error: 324.7624\n",
            "Epoch 282/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 144.7353 - mean_absolute_error: 144.7353\n",
            "Epoch 282: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 144.1362 - mean_absolute_error: 144.1362 - val_loss: 318.2295 - val_mean_absolute_error: 318.2295\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 154.5276 - mean_absolute_error: 154.5276\n",
            "Epoch 283: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 154.5276 - mean_absolute_error: 154.5276 - val_loss: 317.1988 - val_mean_absolute_error: 317.1988\n",
            "Epoch 284/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 153.4210 - mean_absolute_error: 153.4210\n",
            "Epoch 284: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 153.5864 - mean_absolute_error: 153.5864 - val_loss: 316.4252 - val_mean_absolute_error: 316.4252\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 145.2233 - mean_absolute_error: 145.2233\n",
            "Epoch 285: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 145.2233 - mean_absolute_error: 145.2233 - val_loss: 317.5011 - val_mean_absolute_error: 317.5011\n",
            "Epoch 286/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 145.8398 - mean_absolute_error: 145.8398\n",
            "Epoch 286: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 145.6548 - mean_absolute_error: 145.6548 - val_loss: 307.7087 - val_mean_absolute_error: 307.7087\n",
            "Epoch 287/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 149.2493 - mean_absolute_error: 149.2493\n",
            "Epoch 287: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 149.9041 - mean_absolute_error: 149.9041 - val_loss: 319.2652 - val_mean_absolute_error: 319.2652\n",
            "Epoch 288/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 148.2439 - mean_absolute_error: 148.2439\n",
            "Epoch 288: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 147.5301 - mean_absolute_error: 147.5301 - val_loss: 326.8786 - val_mean_absolute_error: 326.8786\n",
            "Epoch 289/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 156.7030 - mean_absolute_error: 156.7030\n",
            "Epoch 289: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 156.2208 - mean_absolute_error: 156.2208 - val_loss: 317.0037 - val_mean_absolute_error: 317.0037\n",
            "Epoch 290/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 155.2855 - mean_absolute_error: 155.2855\n",
            "Epoch 290: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 155.0334 - mean_absolute_error: 155.0334 - val_loss: 341.6983 - val_mean_absolute_error: 341.6983\n",
            "Epoch 291/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 146.0024 - mean_absolute_error: 146.0024\n",
            "Epoch 291: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 147.5125 - mean_absolute_error: 147.5125 - val_loss: 330.1163 - val_mean_absolute_error: 330.1163\n",
            "Epoch 292/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 144.1805 - mean_absolute_error: 144.1805\n",
            "Epoch 292: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 145.1113 - mean_absolute_error: 145.1113 - val_loss: 318.5154 - val_mean_absolute_error: 318.5154\n",
            "Epoch 293/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 145.5797 - mean_absolute_error: 145.5797\n",
            "Epoch 293: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 146.6525 - mean_absolute_error: 146.6525 - val_loss: 335.4080 - val_mean_absolute_error: 335.4080\n",
            "Epoch 294/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 154.0298 - mean_absolute_error: 154.0298\n",
            "Epoch 294: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 154.0177 - mean_absolute_error: 154.0177 - val_loss: 346.3268 - val_mean_absolute_error: 346.3268\n",
            "Epoch 295/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 153.9875 - mean_absolute_error: 153.9875\n",
            "Epoch 295: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 152.8435 - mean_absolute_error: 152.8435 - val_loss: 317.7509 - val_mean_absolute_error: 317.7509\n",
            "Epoch 296/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 148.2198 - mean_absolute_error: 148.2198\n",
            "Epoch 296: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 147.5718 - mean_absolute_error: 147.5718 - val_loss: 319.9530 - val_mean_absolute_error: 319.9530\n",
            "Epoch 297/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 151.0677 - mean_absolute_error: 151.0677\n",
            "Epoch 297: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 150.7224 - mean_absolute_error: 150.7224 - val_loss: 324.0352 - val_mean_absolute_error: 324.0352\n",
            "Epoch 298/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 146.3383 - mean_absolute_error: 146.3383\n",
            "Epoch 298: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 145.4447 - mean_absolute_error: 145.4447 - val_loss: 321.3308 - val_mean_absolute_error: 321.3308\n",
            "Epoch 299/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 147.0402 - mean_absolute_error: 147.0402\n",
            "Epoch 299: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 147.4616 - mean_absolute_error: 147.4616 - val_loss: 343.4692 - val_mean_absolute_error: 343.4692\n",
            "Epoch 300/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 149.1330 - mean_absolute_error: 149.1330\n",
            "Epoch 300: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 148.5476 - mean_absolute_error: 148.5476 - val_loss: 314.5665 - val_mean_absolute_error: 314.5665\n",
            "Epoch 301/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 144.6290 - mean_absolute_error: 144.6290\n",
            "Epoch 301: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 144.5531 - mean_absolute_error: 144.5531 - val_loss: 319.7404 - val_mean_absolute_error: 319.7404\n",
            "Epoch 302/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 139.5474 - mean_absolute_error: 139.5474\n",
            "Epoch 302: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 139.1358 - mean_absolute_error: 139.1358 - val_loss: 322.6781 - val_mean_absolute_error: 322.6781\n",
            "Epoch 303/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 148.2649 - mean_absolute_error: 148.2649\n",
            "Epoch 303: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 148.4637 - mean_absolute_error: 148.4637 - val_loss: 320.4623 - val_mean_absolute_error: 320.4623\n",
            "Epoch 304/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.1504 - mean_absolute_error: 142.1504\n",
            "Epoch 304: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 141.0525 - mean_absolute_error: 141.0525 - val_loss: 324.7974 - val_mean_absolute_error: 324.7974\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 139.6456 - mean_absolute_error: 139.6456\n",
            "Epoch 305: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 139.6456 - mean_absolute_error: 139.6456 - val_loss: 326.0755 - val_mean_absolute_error: 326.0755\n",
            "Epoch 306/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 148.7999 - mean_absolute_error: 148.7999\n",
            "Epoch 306: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 149.8225 - mean_absolute_error: 149.8225 - val_loss: 330.1873 - val_mean_absolute_error: 330.1873\n",
            "Epoch 307/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.1716 - mean_absolute_error: 142.1716\n",
            "Epoch 307: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 142.9536 - mean_absolute_error: 142.9536 - val_loss: 342.3112 - val_mean_absolute_error: 342.3112\n",
            "Epoch 308/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 142.3344 - mean_absolute_error: 142.3344\n",
            "Epoch 308: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 142.3968 - mean_absolute_error: 142.3968 - val_loss: 326.5880 - val_mean_absolute_error: 326.5880\n",
            "Epoch 309/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.3253 - mean_absolute_error: 142.3253\n",
            "Epoch 309: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 142.5380 - mean_absolute_error: 142.5380 - val_loss: 330.5198 - val_mean_absolute_error: 330.5198\n",
            "Epoch 310/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 141.5026 - mean_absolute_error: 141.5026\n",
            "Epoch 310: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 141.4216 - mean_absolute_error: 141.4216 - val_loss: 322.5533 - val_mean_absolute_error: 322.5533\n",
            "Epoch 311/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 144.6820 - mean_absolute_error: 144.6820\n",
            "Epoch 311: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 143.9696 - mean_absolute_error: 143.9696 - val_loss: 325.4712 - val_mean_absolute_error: 325.4712\n",
            "Epoch 312/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 147.8336 - mean_absolute_error: 147.8336\n",
            "Epoch 312: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 147.7173 - mean_absolute_error: 147.7173 - val_loss: 319.0146 - val_mean_absolute_error: 319.0146\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 145.0892 - mean_absolute_error: 145.0892\n",
            "Epoch 313: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 145.0892 - mean_absolute_error: 145.0892 - val_loss: 335.5729 - val_mean_absolute_error: 335.5729\n",
            "Epoch 314/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 144.0795 - mean_absolute_error: 144.0795\n",
            "Epoch 314: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 143.0167 - mean_absolute_error: 143.0167 - val_loss: 319.2166 - val_mean_absolute_error: 319.2166\n",
            "Epoch 315/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 150.9198 - mean_absolute_error: 150.9198\n",
            "Epoch 315: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 149.6354 - mean_absolute_error: 149.6354 - val_loss: 319.8687 - val_mean_absolute_error: 319.8687\n",
            "Epoch 316/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 140.4701 - mean_absolute_error: 140.4701\n",
            "Epoch 316: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 139.6955 - mean_absolute_error: 139.6955 - val_loss: 343.5574 - val_mean_absolute_error: 343.5574\n",
            "Epoch 317/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 152.5398 - mean_absolute_error: 152.5398\n",
            "Epoch 317: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 152.3663 - mean_absolute_error: 152.3663 - val_loss: 319.2865 - val_mean_absolute_error: 319.2865\n",
            "Epoch 318/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 136.3145 - mean_absolute_error: 136.3145\n",
            "Epoch 318: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 137.0717 - mean_absolute_error: 137.0717 - val_loss: 322.3301 - val_mean_absolute_error: 322.3301\n",
            "Epoch 319/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 143.4905 - mean_absolute_error: 143.4905\n",
            "Epoch 319: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 143.5644 - mean_absolute_error: 143.5644 - val_loss: 317.1242 - val_mean_absolute_error: 317.1242\n",
            "Epoch 320/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 146.9057 - mean_absolute_error: 146.9057\n",
            "Epoch 320: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 147.2244 - mean_absolute_error: 147.2244 - val_loss: 319.9233 - val_mean_absolute_error: 319.9233\n",
            "Epoch 321/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 143.1413 - mean_absolute_error: 143.1413\n",
            "Epoch 321: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 143.7337 - mean_absolute_error: 143.7337 - val_loss: 325.5557 - val_mean_absolute_error: 325.5557\n",
            "Epoch 322/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 143.7557 - mean_absolute_error: 143.7557\n",
            "Epoch 322: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 143.9030 - mean_absolute_error: 143.9030 - val_loss: 322.7977 - val_mean_absolute_error: 322.7977\n",
            "Epoch 323/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 133.9728 - mean_absolute_error: 133.9728\n",
            "Epoch 323: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 133.5666 - mean_absolute_error: 133.5666 - val_loss: 328.8602 - val_mean_absolute_error: 328.8602\n",
            "Epoch 324/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 148.9304 - mean_absolute_error: 148.9304\n",
            "Epoch 324: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 148.7779 - mean_absolute_error: 148.7779 - val_loss: 321.1139 - val_mean_absolute_error: 321.1139\n",
            "Epoch 325/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 144.2179 - mean_absolute_error: 144.2179\n",
            "Epoch 325: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 144.0465 - mean_absolute_error: 144.0465 - val_loss: 318.2439 - val_mean_absolute_error: 318.2439\n",
            "Epoch 326/500\n",
            "79/89 [=========================>....] - ETA: 0s - loss: 136.6549 - mean_absolute_error: 136.6549\n",
            "Epoch 326: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 136.8391 - mean_absolute_error: 136.8391 - val_loss: 323.9293 - val_mean_absolute_error: 323.9293\n",
            "Epoch 327/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 137.9213 - mean_absolute_error: 137.9213\n",
            "Epoch 327: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 139.6310 - mean_absolute_error: 139.6310 - val_loss: 324.7836 - val_mean_absolute_error: 324.7836\n",
            "Epoch 328/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 145.4672 - mean_absolute_error: 145.4672\n",
            "Epoch 328: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 145.4918 - mean_absolute_error: 145.4918 - val_loss: 335.0147 - val_mean_absolute_error: 335.0147\n",
            "Epoch 329/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 141.2535 - mean_absolute_error: 141.2535\n",
            "Epoch 329: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 141.2046 - mean_absolute_error: 141.2046 - val_loss: 328.0500 - val_mean_absolute_error: 328.0500\n",
            "Epoch 330/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 133.3067 - mean_absolute_error: 133.3067\n",
            "Epoch 330: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 133.1421 - mean_absolute_error: 133.1421 - val_loss: 320.1330 - val_mean_absolute_error: 320.1330\n",
            "Epoch 331/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 145.8425 - mean_absolute_error: 145.8425\n",
            "Epoch 331: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 145.8291 - mean_absolute_error: 145.8291 - val_loss: 318.1094 - val_mean_absolute_error: 318.1094\n",
            "Epoch 332/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 133.9321 - mean_absolute_error: 133.9321\n",
            "Epoch 332: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 133.5855 - mean_absolute_error: 133.5855 - val_loss: 342.2714 - val_mean_absolute_error: 342.2714\n",
            "Epoch 333/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 137.9687 - mean_absolute_error: 137.9687\n",
            "Epoch 333: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 137.5247 - mean_absolute_error: 137.5247 - val_loss: 334.0232 - val_mean_absolute_error: 334.0232\n",
            "Epoch 334/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 137.6443 - mean_absolute_error: 137.6443\n",
            "Epoch 334: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 137.8078 - mean_absolute_error: 137.8078 - val_loss: 332.4977 - val_mean_absolute_error: 332.4977\n",
            "Epoch 335/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 143.5588 - mean_absolute_error: 143.5588\n",
            "Epoch 335: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 143.4699 - mean_absolute_error: 143.4699 - val_loss: 332.2679 - val_mean_absolute_error: 332.2679\n",
            "Epoch 336/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 141.7074 - mean_absolute_error: 141.7074\n",
            "Epoch 336: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 142.7784 - mean_absolute_error: 142.7784 - val_loss: 319.0256 - val_mean_absolute_error: 319.0256\n",
            "Epoch 337/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 129.8607 - mean_absolute_error: 129.8607\n",
            "Epoch 337: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 131.9037 - mean_absolute_error: 131.9037 - val_loss: 328.8124 - val_mean_absolute_error: 328.8124\n",
            "Epoch 338/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 141.5307 - mean_absolute_error: 141.5307\n",
            "Epoch 338: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 141.3263 - mean_absolute_error: 141.3263 - val_loss: 318.4262 - val_mean_absolute_error: 318.4262\n",
            "Epoch 339/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 141.1382 - mean_absolute_error: 141.1382\n",
            "Epoch 339: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 141.1123 - mean_absolute_error: 141.1123 - val_loss: 329.2133 - val_mean_absolute_error: 329.2133\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 140.1818 - mean_absolute_error: 140.1818\n",
            "Epoch 340: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 140.1818 - mean_absolute_error: 140.1818 - val_loss: 321.1268 - val_mean_absolute_error: 321.1268\n",
            "Epoch 341/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 134.7123 - mean_absolute_error: 134.7123\n",
            "Epoch 341: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 134.9136 - mean_absolute_error: 134.9136 - val_loss: 326.8075 - val_mean_absolute_error: 326.8075\n",
            "Epoch 342/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 136.2358 - mean_absolute_error: 136.2358\n",
            "Epoch 342: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 137.5257 - mean_absolute_error: 137.5257 - val_loss: 322.9851 - val_mean_absolute_error: 322.9851\n",
            "Epoch 343/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 128.0274 - mean_absolute_error: 128.0274\n",
            "Epoch 343: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 128.7813 - mean_absolute_error: 128.7813 - val_loss: 319.9406 - val_mean_absolute_error: 319.9406\n",
            "Epoch 344/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 130.1935 - mean_absolute_error: 130.1935\n",
            "Epoch 344: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.2621 - mean_absolute_error: 129.2621 - val_loss: 321.5695 - val_mean_absolute_error: 321.5695\n",
            "Epoch 345/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.8226 - mean_absolute_error: 142.8226\n",
            "Epoch 345: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 142.5715 - mean_absolute_error: 142.5715 - val_loss: 333.4012 - val_mean_absolute_error: 333.4012\n",
            "Epoch 346/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 134.5609 - mean_absolute_error: 134.5609\n",
            "Epoch 346: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 133.7594 - mean_absolute_error: 133.7594 - val_loss: 327.7418 - val_mean_absolute_error: 327.7418\n",
            "Epoch 347/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 132.4354 - mean_absolute_error: 132.4354\n",
            "Epoch 347: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 132.4569 - mean_absolute_error: 132.4569 - val_loss: 320.9083 - val_mean_absolute_error: 320.9083\n",
            "Epoch 348/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 137.6547 - mean_absolute_error: 137.6547\n",
            "Epoch 348: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 137.3114 - mean_absolute_error: 137.3114 - val_loss: 323.9531 - val_mean_absolute_error: 323.9531\n",
            "Epoch 349/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 140.5500 - mean_absolute_error: 140.5500\n",
            "Epoch 349: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 140.9435 - mean_absolute_error: 140.9435 - val_loss: 336.2995 - val_mean_absolute_error: 336.2995\n",
            "Epoch 350/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 134.2134 - mean_absolute_error: 134.2134\n",
            "Epoch 350: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 134.4611 - mean_absolute_error: 134.4611 - val_loss: 324.9279 - val_mean_absolute_error: 324.9279\n",
            "Epoch 351/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 141.0129 - mean_absolute_error: 141.0129\n",
            "Epoch 351: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 140.9520 - mean_absolute_error: 140.9520 - val_loss: 329.7418 - val_mean_absolute_error: 329.7418\n",
            "Epoch 352/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 136.9939 - mean_absolute_error: 136.9939\n",
            "Epoch 352: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 136.9985 - mean_absolute_error: 136.9985 - val_loss: 321.4382 - val_mean_absolute_error: 321.4382\n",
            "Epoch 353/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 133.1479 - mean_absolute_error: 133.1479\n",
            "Epoch 353: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 133.3282 - mean_absolute_error: 133.3282 - val_loss: 323.6790 - val_mean_absolute_error: 323.6790\n",
            "Epoch 354/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 136.8573 - mean_absolute_error: 136.8573\n",
            "Epoch 354: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 136.0833 - mean_absolute_error: 136.0833 - val_loss: 323.5240 - val_mean_absolute_error: 323.5240\n",
            "Epoch 355/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 137.8271 - mean_absolute_error: 137.8271\n",
            "Epoch 355: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 135.8416 - mean_absolute_error: 135.8416 - val_loss: 332.7593 - val_mean_absolute_error: 332.7593\n",
            "Epoch 356/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 134.9941 - mean_absolute_error: 134.9941\n",
            "Epoch 356: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 135.5515 - mean_absolute_error: 135.5515 - val_loss: 315.0096 - val_mean_absolute_error: 315.0096\n",
            "Epoch 357/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 143.2573 - mean_absolute_error: 143.2573\n",
            "Epoch 357: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 143.3527 - mean_absolute_error: 143.3527 - val_loss: 327.4952 - val_mean_absolute_error: 327.4952\n",
            "Epoch 358/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 147.4327 - mean_absolute_error: 147.4327\n",
            "Epoch 358: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 144.9860 - mean_absolute_error: 144.9860 - val_loss: 331.1700 - val_mean_absolute_error: 331.1700\n",
            "Epoch 359/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 137.1022 - mean_absolute_error: 137.1022\n",
            "Epoch 359: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 137.1859 - mean_absolute_error: 137.1859 - val_loss: 327.9380 - val_mean_absolute_error: 327.9380\n",
            "Epoch 360/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 136.8727 - mean_absolute_error: 136.8727\n",
            "Epoch 360: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 6ms/step - loss: 137.1389 - mean_absolute_error: 137.1389 - val_loss: 331.5027 - val_mean_absolute_error: 331.5027\n",
            "Epoch 361/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 131.8562 - mean_absolute_error: 131.8562\n",
            "Epoch 361: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 131.4083 - mean_absolute_error: 131.4083 - val_loss: 323.4191 - val_mean_absolute_error: 323.4191\n",
            "Epoch 362/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 127.2784 - mean_absolute_error: 127.2784\n",
            "Epoch 362: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 126.2970 - mean_absolute_error: 126.2970 - val_loss: 326.0020 - val_mean_absolute_error: 326.0020\n",
            "Epoch 363/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 132.3329 - mean_absolute_error: 132.3329\n",
            "Epoch 363: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 132.0055 - mean_absolute_error: 132.0055 - val_loss: 338.6736 - val_mean_absolute_error: 338.6736\n",
            "Epoch 364/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 128.7609 - mean_absolute_error: 128.7609\n",
            "Epoch 364: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 128.7736 - mean_absolute_error: 128.7736 - val_loss: 336.2118 - val_mean_absolute_error: 336.2118\n",
            "Epoch 365/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 129.2035 - mean_absolute_error: 129.2035\n",
            "Epoch 365: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 128.5854 - mean_absolute_error: 128.5854 - val_loss: 326.6579 - val_mean_absolute_error: 326.6579\n",
            "Epoch 366/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 145.0339 - mean_absolute_error: 145.0339\n",
            "Epoch 366: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 144.2497 - mean_absolute_error: 144.2497 - val_loss: 322.5770 - val_mean_absolute_error: 322.5770\n",
            "Epoch 367/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 147.5087 - mean_absolute_error: 147.5087\n",
            "Epoch 367: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 147.9118 - mean_absolute_error: 147.9118 - val_loss: 334.1906 - val_mean_absolute_error: 334.1906\n",
            "Epoch 368/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 131.7972 - mean_absolute_error: 131.7972\n",
            "Epoch 368: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 132.9614 - mean_absolute_error: 132.9614 - val_loss: 330.1271 - val_mean_absolute_error: 330.1271\n",
            "Epoch 369/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 129.9397 - mean_absolute_error: 129.9397\n",
            "Epoch 369: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 129.7385 - mean_absolute_error: 129.7385 - val_loss: 323.1859 - val_mean_absolute_error: 323.1859\n",
            "Epoch 370/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 128.7199 - mean_absolute_error: 128.7199\n",
            "Epoch 370: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 128.9766 - mean_absolute_error: 128.9766 - val_loss: 336.4183 - val_mean_absolute_error: 336.4183\n",
            "Epoch 371/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 142.1692 - mean_absolute_error: 142.1692\n",
            "Epoch 371: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 142.0860 - mean_absolute_error: 142.0860 - val_loss: 326.1805 - val_mean_absolute_error: 326.1805\n",
            "Epoch 372/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 128.5404 - mean_absolute_error: 128.5404\n",
            "Epoch 372: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 129.0962 - mean_absolute_error: 129.0962 - val_loss: 319.2152 - val_mean_absolute_error: 319.2152\n",
            "Epoch 373/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 123.4644 - mean_absolute_error: 123.4644\n",
            "Epoch 373: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 123.2449 - mean_absolute_error: 123.2449 - val_loss: 322.1427 - val_mean_absolute_error: 322.1427\n",
            "Epoch 374/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 123.1289 - mean_absolute_error: 123.1289\n",
            "Epoch 374: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 123.4027 - mean_absolute_error: 123.4027 - val_loss: 327.9121 - val_mean_absolute_error: 327.9121\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 131.7064 - mean_absolute_error: 131.7064\n",
            "Epoch 375: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 131.7064 - mean_absolute_error: 131.7064 - val_loss: 324.5002 - val_mean_absolute_error: 324.5002\n",
            "Epoch 376/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 131.4144 - mean_absolute_error: 131.4144\n",
            "Epoch 376: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 131.3251 - mean_absolute_error: 131.3251 - val_loss: 330.8965 - val_mean_absolute_error: 330.8965\n",
            "Epoch 377/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 126.8860 - mean_absolute_error: 126.8860\n",
            "Epoch 377: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 127.8794 - mean_absolute_error: 127.8794 - val_loss: 331.4945 - val_mean_absolute_error: 331.4945\n",
            "Epoch 378/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 129.9087 - mean_absolute_error: 129.9087\n",
            "Epoch 378: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 130.1834 - mean_absolute_error: 130.1834 - val_loss: 325.1710 - val_mean_absolute_error: 325.1710\n",
            "Epoch 379/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 133.2541 - mean_absolute_error: 133.2541\n",
            "Epoch 379: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 132.8526 - mean_absolute_error: 132.8526 - val_loss: 334.6640 - val_mean_absolute_error: 334.6640\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 124.8744 - mean_absolute_error: 124.8744\n",
            "Epoch 380: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 124.8744 - mean_absolute_error: 124.8744 - val_loss: 348.1013 - val_mean_absolute_error: 348.1013\n",
            "Epoch 381/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 131.1592 - mean_absolute_error: 131.1592\n",
            "Epoch 381: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 130.1288 - mean_absolute_error: 130.1288 - val_loss: 331.4790 - val_mean_absolute_error: 331.4790\n",
            "Epoch 382/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 127.5860 - mean_absolute_error: 127.5860\n",
            "Epoch 382: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 128.3603 - mean_absolute_error: 128.3603 - val_loss: 325.3511 - val_mean_absolute_error: 325.3511\n",
            "Epoch 383/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 125.9207 - mean_absolute_error: 125.9207\n",
            "Epoch 383: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 126.6653 - mean_absolute_error: 126.6653 - val_loss: 320.7070 - val_mean_absolute_error: 320.7070\n",
            "Epoch 384/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 123.1595 - mean_absolute_error: 123.1595\n",
            "Epoch 384: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 125.6482 - mean_absolute_error: 125.6482 - val_loss: 326.0950 - val_mean_absolute_error: 326.0950\n",
            "Epoch 385/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 125.2020 - mean_absolute_error: 125.2020\n",
            "Epoch 385: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 125.2388 - mean_absolute_error: 125.2388 - val_loss: 325.7192 - val_mean_absolute_error: 325.7192\n",
            "Epoch 386/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 124.3658 - mean_absolute_error: 124.3658\n",
            "Epoch 386: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 124.2744 - mean_absolute_error: 124.2744 - val_loss: 324.3970 - val_mean_absolute_error: 324.3970\n",
            "Epoch 387/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 127.1590 - mean_absolute_error: 127.1590\n",
            "Epoch 387: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 126.8522 - mean_absolute_error: 126.8522 - val_loss: 334.5280 - val_mean_absolute_error: 334.5280\n",
            "Epoch 388/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 120.2761 - mean_absolute_error: 120.2761\n",
            "Epoch 388: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 123.9293 - mean_absolute_error: 123.9293 - val_loss: 331.5556 - val_mean_absolute_error: 331.5556\n",
            "Epoch 389/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 142.6163 - mean_absolute_error: 142.6163\n",
            "Epoch 389: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 142.4761 - mean_absolute_error: 142.4761 - val_loss: 339.5288 - val_mean_absolute_error: 339.5288\n",
            "Epoch 390/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 131.9998 - mean_absolute_error: 131.9998\n",
            "Epoch 390: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 131.7550 - mean_absolute_error: 131.7550 - val_loss: 330.8182 - val_mean_absolute_error: 330.8182\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 125.9984 - mean_absolute_error: 125.9984\n",
            "Epoch 391: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 125.9984 - mean_absolute_error: 125.9984 - val_loss: 324.3635 - val_mean_absolute_error: 324.3635\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 125.9932 - mean_absolute_error: 125.9932\n",
            "Epoch 392: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 125.9932 - mean_absolute_error: 125.9932 - val_loss: 330.5708 - val_mean_absolute_error: 330.5708\n",
            "Epoch 393/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 132.7726 - mean_absolute_error: 132.7726\n",
            "Epoch 393: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 132.7160 - mean_absolute_error: 132.7160 - val_loss: 327.3534 - val_mean_absolute_error: 327.3534\n",
            "Epoch 394/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 124.3184 - mean_absolute_error: 124.3184\n",
            "Epoch 394: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 124.0967 - mean_absolute_error: 124.0967 - val_loss: 329.5616 - val_mean_absolute_error: 329.5616\n",
            "Epoch 395/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 125.0645 - mean_absolute_error: 125.0645\n",
            "Epoch 395: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 125.0066 - mean_absolute_error: 125.0066 - val_loss: 331.0113 - val_mean_absolute_error: 331.0113\n",
            "Epoch 396/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 126.5410 - mean_absolute_error: 126.5410\n",
            "Epoch 396: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 126.4539 - mean_absolute_error: 126.4539 - val_loss: 330.0577 - val_mean_absolute_error: 330.0577\n",
            "Epoch 397/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 129.9044 - mean_absolute_error: 129.9044\n",
            "Epoch 397: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 129.8740 - mean_absolute_error: 129.8740 - val_loss: 334.8145 - val_mean_absolute_error: 334.8145\n",
            "Epoch 398/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 126.5382 - mean_absolute_error: 126.5382\n",
            "Epoch 398: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 126.1501 - mean_absolute_error: 126.1501 - val_loss: 332.1672 - val_mean_absolute_error: 332.1672\n",
            "Epoch 399/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 127.1910 - mean_absolute_error: 127.1910\n",
            "Epoch 399: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 127.0882 - mean_absolute_error: 127.0882 - val_loss: 334.8656 - val_mean_absolute_error: 334.8656\n",
            "Epoch 400/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 122.7542 - mean_absolute_error: 122.7542\n",
            "Epoch 400: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 121.7802 - mean_absolute_error: 121.7802 - val_loss: 327.4239 - val_mean_absolute_error: 327.4239\n",
            "Epoch 401/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 126.1067 - mean_absolute_error: 126.1067\n",
            "Epoch 401: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.0040 - mean_absolute_error: 126.0040 - val_loss: 324.3854 - val_mean_absolute_error: 324.3854\n",
            "Epoch 402/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 121.4696 - mean_absolute_error: 121.4696\n",
            "Epoch 402: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 121.7520 - mean_absolute_error: 121.7520 - val_loss: 329.5257 - val_mean_absolute_error: 329.5257\n",
            "Epoch 403/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 126.9442 - mean_absolute_error: 126.9442\n",
            "Epoch 403: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 127.5517 - mean_absolute_error: 127.5517 - val_loss: 344.0341 - val_mean_absolute_error: 344.0341\n",
            "Epoch 404/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 124.0109 - mean_absolute_error: 124.0109\n",
            "Epoch 404: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 124.1811 - mean_absolute_error: 124.1811 - val_loss: 328.7902 - val_mean_absolute_error: 328.7902\n",
            "Epoch 405/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 124.9029 - mean_absolute_error: 124.9029\n",
            "Epoch 405: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.9287 - mean_absolute_error: 124.9287 - val_loss: 324.3887 - val_mean_absolute_error: 324.3887\n",
            "Epoch 406/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 122.9152 - mean_absolute_error: 122.9152\n",
            "Epoch 406: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 122.7158 - mean_absolute_error: 122.7158 - val_loss: 327.2079 - val_mean_absolute_error: 327.2079\n",
            "Epoch 407/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 137.3001 - mean_absolute_error: 137.3001\n",
            "Epoch 407: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 135.0363 - mean_absolute_error: 135.0363 - val_loss: 332.5865 - val_mean_absolute_error: 332.5865\n",
            "Epoch 408/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 126.3344 - mean_absolute_error: 126.3344\n",
            "Epoch 408: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.6643 - mean_absolute_error: 126.6643 - val_loss: 328.6079 - val_mean_absolute_error: 328.6079\n",
            "Epoch 409/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 127.7159 - mean_absolute_error: 127.7159\n",
            "Epoch 409: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 126.9133 - mean_absolute_error: 126.9133 - val_loss: 326.0642 - val_mean_absolute_error: 326.0642\n",
            "Epoch 410/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 124.6514 - mean_absolute_error: 124.6514\n",
            "Epoch 410: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 124.5952 - mean_absolute_error: 124.5952 - val_loss: 335.9321 - val_mean_absolute_error: 335.9321\n",
            "Epoch 411/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 125.4389 - mean_absolute_error: 125.4389\n",
            "Epoch 411: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 125.9131 - mean_absolute_error: 125.9131 - val_loss: 329.3346 - val_mean_absolute_error: 329.3346\n",
            "Epoch 412/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 123.9519 - mean_absolute_error: 123.9519\n",
            "Epoch 412: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 124.4859 - mean_absolute_error: 124.4859 - val_loss: 328.7513 - val_mean_absolute_error: 328.7513\n",
            "Epoch 413/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 121.2485 - mean_absolute_error: 121.2485\n",
            "Epoch 413: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 120.8123 - mean_absolute_error: 120.8123 - val_loss: 326.0429 - val_mean_absolute_error: 326.0429\n",
            "Epoch 414/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 129.2593 - mean_absolute_error: 129.2593\n",
            "Epoch 414: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 130.7551 - mean_absolute_error: 130.7551 - val_loss: 331.6995 - val_mean_absolute_error: 331.6995\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 123.0058 - mean_absolute_error: 123.0058\n",
            "Epoch 415: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 123.0058 - mean_absolute_error: 123.0058 - val_loss: 336.1351 - val_mean_absolute_error: 336.1351\n",
            "Epoch 416/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 123.5777 - mean_absolute_error: 123.5777\n",
            "Epoch 416: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 123.7129 - mean_absolute_error: 123.7129 - val_loss: 335.4985 - val_mean_absolute_error: 335.4985\n",
            "Epoch 417/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 131.5663 - mean_absolute_error: 131.5663\n",
            "Epoch 417: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 132.3972 - mean_absolute_error: 132.3972 - val_loss: 329.7854 - val_mean_absolute_error: 329.7854\n",
            "Epoch 418/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 125.2683 - mean_absolute_error: 125.2683\n",
            "Epoch 418: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 124.8902 - mean_absolute_error: 124.8902 - val_loss: 327.9996 - val_mean_absolute_error: 327.9996\n",
            "Epoch 419/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 117.2274 - mean_absolute_error: 117.2274\n",
            "Epoch 419: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 117.1825 - mean_absolute_error: 117.1825 - val_loss: 330.7100 - val_mean_absolute_error: 330.7100\n",
            "Epoch 420/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 122.9547 - mean_absolute_error: 122.9547\n",
            "Epoch 420: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 122.8461 - mean_absolute_error: 122.8461 - val_loss: 331.9662 - val_mean_absolute_error: 331.9662\n",
            "Epoch 421/500\n",
            "81/89 [==========================>...] - ETA: 0s - loss: 127.4917 - mean_absolute_error: 127.4917\n",
            "Epoch 421: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.0941 - mean_absolute_error: 126.0941 - val_loss: 328.3512 - val_mean_absolute_error: 328.3512\n",
            "Epoch 422/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 119.9597 - mean_absolute_error: 119.9597\n",
            "Epoch 422: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 119.9785 - mean_absolute_error: 119.9785 - val_loss: 339.7473 - val_mean_absolute_error: 339.7473\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 128.9201 - mean_absolute_error: 128.9201\n",
            "Epoch 423: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 128.9201 - mean_absolute_error: 128.9201 - val_loss: 324.8448 - val_mean_absolute_error: 324.8448\n",
            "Epoch 424/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 128.9343 - mean_absolute_error: 128.9343\n",
            "Epoch 424: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 128.8659 - mean_absolute_error: 128.8659 - val_loss: 328.1100 - val_mean_absolute_error: 328.1100\n",
            "Epoch 425/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 120.4202 - mean_absolute_error: 120.4202\n",
            "Epoch 425: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 119.5640 - mean_absolute_error: 119.5640 - val_loss: 334.2084 - val_mean_absolute_error: 334.2084\n",
            "Epoch 426/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 124.8246 - mean_absolute_error: 124.8246\n",
            "Epoch 426: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.3547 - mean_absolute_error: 124.3547 - val_loss: 323.3749 - val_mean_absolute_error: 323.3749\n",
            "Epoch 427/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 118.6158 - mean_absolute_error: 118.6158\n",
            "Epoch 427: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 118.3943 - mean_absolute_error: 118.3943 - val_loss: 336.9224 - val_mean_absolute_error: 336.9224\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 116.1784 - mean_absolute_error: 116.1784\n",
            "Epoch 428: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 116.1784 - mean_absolute_error: 116.1784 - val_loss: 329.8976 - val_mean_absolute_error: 329.8976\n",
            "Epoch 429/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 120.4554 - mean_absolute_error: 120.4554\n",
            "Epoch 429: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 120.3323 - mean_absolute_error: 120.3323 - val_loss: 324.6663 - val_mean_absolute_error: 324.6663\n",
            "Epoch 430/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 122.8668 - mean_absolute_error: 122.8668\n",
            "Epoch 430: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 122.8438 - mean_absolute_error: 122.8438 - val_loss: 346.1805 - val_mean_absolute_error: 346.1805\n",
            "Epoch 431/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 118.0177 - mean_absolute_error: 118.0177\n",
            "Epoch 431: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 117.6802 - mean_absolute_error: 117.6802 - val_loss: 331.0567 - val_mean_absolute_error: 331.0567\n",
            "Epoch 432/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 117.0787 - mean_absolute_error: 117.0787\n",
            "Epoch 432: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 116.5540 - mean_absolute_error: 116.5540 - val_loss: 323.7629 - val_mean_absolute_error: 323.7629\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 125.0002 - mean_absolute_error: 125.0002\n",
            "Epoch 433: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 125.0002 - mean_absolute_error: 125.0002 - val_loss: 332.3001 - val_mean_absolute_error: 332.3001\n",
            "Epoch 434/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 117.3680 - mean_absolute_error: 117.3680\n",
            "Epoch 434: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 117.5684 - mean_absolute_error: 117.5684 - val_loss: 327.9818 - val_mean_absolute_error: 327.9818\n",
            "Epoch 435/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 113.1011 - mean_absolute_error: 113.1011\n",
            "Epoch 435: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 113.5759 - mean_absolute_error: 113.5759 - val_loss: 336.8547 - val_mean_absolute_error: 336.8547\n",
            "Epoch 436/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 133.4493 - mean_absolute_error: 133.4493\n",
            "Epoch 436: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 132.6930 - mean_absolute_error: 132.6930 - val_loss: 327.0747 - val_mean_absolute_error: 327.0747\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 123.2170 - mean_absolute_error: 123.2170\n",
            "Epoch 437: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 123.2170 - mean_absolute_error: 123.2170 - val_loss: 333.9476 - val_mean_absolute_error: 333.9476\n",
            "Epoch 438/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 124.4001 - mean_absolute_error: 124.4001\n",
            "Epoch 438: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 125.2761 - mean_absolute_error: 125.2761 - val_loss: 331.0894 - val_mean_absolute_error: 331.0894\n",
            "Epoch 439/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 120.9035 - mean_absolute_error: 120.9035\n",
            "Epoch 439: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 119.7587 - mean_absolute_error: 119.7587 - val_loss: 334.9519 - val_mean_absolute_error: 334.9519\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 117.4983 - mean_absolute_error: 117.4983\n",
            "Epoch 440: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 117.4983 - mean_absolute_error: 117.4983 - val_loss: 331.6120 - val_mean_absolute_error: 331.6120\n",
            "Epoch 441/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 117.9873 - mean_absolute_error: 117.9873\n",
            "Epoch 441: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 118.2076 - mean_absolute_error: 118.2076 - val_loss: 338.3700 - val_mean_absolute_error: 338.3700\n",
            "Epoch 442/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 128.8586 - mean_absolute_error: 128.8586\n",
            "Epoch 442: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 127.4730 - mean_absolute_error: 127.4730 - val_loss: 329.8174 - val_mean_absolute_error: 329.8174\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 114.5292 - mean_absolute_error: 114.5292\n",
            "Epoch 443: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 114.5292 - mean_absolute_error: 114.5292 - val_loss: 329.1454 - val_mean_absolute_error: 329.1454\n",
            "Epoch 444/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 119.8148 - mean_absolute_error: 119.8148\n",
            "Epoch 444: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 119.9702 - mean_absolute_error: 119.9702 - val_loss: 337.3546 - val_mean_absolute_error: 337.3546\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 114.4283 - mean_absolute_error: 114.4283\n",
            "Epoch 445: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 114.4283 - mean_absolute_error: 114.4283 - val_loss: 329.4843 - val_mean_absolute_error: 329.4843\n",
            "Epoch 446/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 127.8253 - mean_absolute_error: 127.8253\n",
            "Epoch 446: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 128.5472 - mean_absolute_error: 128.5472 - val_loss: 353.1494 - val_mean_absolute_error: 353.1494\n",
            "Epoch 447/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 134.4897 - mean_absolute_error: 134.4897\n",
            "Epoch 447: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 135.2556 - mean_absolute_error: 135.2556 - val_loss: 330.3965 - val_mean_absolute_error: 330.3965\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 117.2039 - mean_absolute_error: 117.2039\n",
            "Epoch 448: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 117.2039 - mean_absolute_error: 117.2039 - val_loss: 329.6866 - val_mean_absolute_error: 329.6866\n",
            "Epoch 449/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 113.0486 - mean_absolute_error: 113.0486\n",
            "Epoch 449: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 113.0532 - mean_absolute_error: 113.0532 - val_loss: 341.8337 - val_mean_absolute_error: 341.8337\n",
            "Epoch 450/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 113.8219 - mean_absolute_error: 113.8219\n",
            "Epoch 450: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 114.2562 - mean_absolute_error: 114.2562 - val_loss: 328.4471 - val_mean_absolute_error: 328.4471\n",
            "Epoch 451/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 117.4446 - mean_absolute_error: 117.4446\n",
            "Epoch 451: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 117.4959 - mean_absolute_error: 117.4959 - val_loss: 346.9521 - val_mean_absolute_error: 346.9521\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 116.2435 - mean_absolute_error: 116.2435\n",
            "Epoch 452: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 116.2435 - mean_absolute_error: 116.2435 - val_loss: 336.1261 - val_mean_absolute_error: 336.1261\n",
            "Epoch 453/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 122.0287 - mean_absolute_error: 122.0287\n",
            "Epoch 453: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 121.3162 - mean_absolute_error: 121.3162 - val_loss: 330.9742 - val_mean_absolute_error: 330.9742\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 117.7017 - mean_absolute_error: 117.7017\n",
            "Epoch 454: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 117.7017 - mean_absolute_error: 117.7017 - val_loss: 335.6060 - val_mean_absolute_error: 335.6060\n",
            "Epoch 455/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 121.7006 - mean_absolute_error: 121.7006\n",
            "Epoch 455: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 121.9727 - mean_absolute_error: 121.9727 - val_loss: 333.6796 - val_mean_absolute_error: 333.6796\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 113.1707 - mean_absolute_error: 113.1707\n",
            "Epoch 456: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 113.1707 - mean_absolute_error: 113.1707 - val_loss: 333.9553 - val_mean_absolute_error: 333.9553\n",
            "Epoch 457/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 115.5574 - mean_absolute_error: 115.5574\n",
            "Epoch 457: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 15ms/step - loss: 115.4780 - mean_absolute_error: 115.4780 - val_loss: 332.5483 - val_mean_absolute_error: 332.5483\n",
            "Epoch 458/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 114.1092 - mean_absolute_error: 114.1092\n",
            "Epoch 458: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 16ms/step - loss: 114.4996 - mean_absolute_error: 114.4996 - val_loss: 328.6488 - val_mean_absolute_error: 328.6488\n",
            "Epoch 459/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 118.6775 - mean_absolute_error: 118.6775\n",
            "Epoch 459: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 119.5015 - mean_absolute_error: 119.5015 - val_loss: 332.6765 - val_mean_absolute_error: 332.6765\n",
            "Epoch 460/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 112.1373 - mean_absolute_error: 112.1373\n",
            "Epoch 460: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 111.8754 - mean_absolute_error: 111.8754 - val_loss: 332.0905 - val_mean_absolute_error: 332.0905\n",
            "Epoch 461/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 127.8918 - mean_absolute_error: 127.8918\n",
            "Epoch 461: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 126.9669 - mean_absolute_error: 126.9669 - val_loss: 334.3675 - val_mean_absolute_error: 334.3675\n",
            "Epoch 462/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 123.5564 - mean_absolute_error: 123.5564\n",
            "Epoch 462: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 122.4426 - mean_absolute_error: 122.4426 - val_loss: 334.4605 - val_mean_absolute_error: 334.4605\n",
            "Epoch 463/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 114.0506 - mean_absolute_error: 114.0506\n",
            "Epoch 463: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 114.7887 - mean_absolute_error: 114.7887 - val_loss: 324.6828 - val_mean_absolute_error: 324.6828\n",
            "Epoch 464/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 121.4064 - mean_absolute_error: 121.4064\n",
            "Epoch 464: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 122.5039 - mean_absolute_error: 122.5039 - val_loss: 337.8646 - val_mean_absolute_error: 337.8646\n",
            "Epoch 465/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 125.3313 - mean_absolute_error: 125.3313\n",
            "Epoch 465: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 124.8934 - mean_absolute_error: 124.8934 - val_loss: 331.5773 - val_mean_absolute_error: 331.5773\n",
            "Epoch 466/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 110.8777 - mean_absolute_error: 110.8777\n",
            "Epoch 466: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 110.6801 - mean_absolute_error: 110.6801 - val_loss: 333.0464 - val_mean_absolute_error: 333.0464\n",
            "Epoch 467/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 124.1818 - mean_absolute_error: 124.1818\n",
            "Epoch 467: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 124.1333 - mean_absolute_error: 124.1333 - val_loss: 334.0236 - val_mean_absolute_error: 334.0236\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 114.8040 - mean_absolute_error: 114.8040\n",
            "Epoch 468: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 114.8040 - mean_absolute_error: 114.8040 - val_loss: 329.4174 - val_mean_absolute_error: 329.4174\n",
            "Epoch 469/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 111.4126 - mean_absolute_error: 111.4126\n",
            "Epoch 469: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 111.2545 - mean_absolute_error: 111.2545 - val_loss: 331.9879 - val_mean_absolute_error: 331.9879\n",
            "Epoch 470/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 124.0120 - mean_absolute_error: 124.0120\n",
            "Epoch 470: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 123.9018 - mean_absolute_error: 123.9018 - val_loss: 353.7862 - val_mean_absolute_error: 353.7862\n",
            "Epoch 471/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 139.1993 - mean_absolute_error: 139.1993\n",
            "Epoch 471: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 139.3811 - mean_absolute_error: 139.3811 - val_loss: 333.8322 - val_mean_absolute_error: 333.8322\n",
            "Epoch 472/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 120.2128 - mean_absolute_error: 120.2128\n",
            "Epoch 472: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 119.8053 - mean_absolute_error: 119.8053 - val_loss: 336.5873 - val_mean_absolute_error: 336.5873\n",
            "Epoch 473/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 119.6141 - mean_absolute_error: 119.6141\n",
            "Epoch 473: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 119.3828 - mean_absolute_error: 119.3828 - val_loss: 336.8762 - val_mean_absolute_error: 336.8762\n",
            "Epoch 474/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 115.0557 - mean_absolute_error: 115.0557\n",
            "Epoch 474: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 115.0237 - mean_absolute_error: 115.0237 - val_loss: 332.6104 - val_mean_absolute_error: 332.6104\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 118.9819 - mean_absolute_error: 118.9819\n",
            "Epoch 475: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 118.9819 - mean_absolute_error: 118.9819 - val_loss: 336.7054 - val_mean_absolute_error: 336.7054\n",
            "Epoch 476/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 114.3947 - mean_absolute_error: 114.3947\n",
            "Epoch 476: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 115.2131 - mean_absolute_error: 115.2131 - val_loss: 336.4594 - val_mean_absolute_error: 336.4594\n",
            "Epoch 477/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 115.3899 - mean_absolute_error: 115.3899\n",
            "Epoch 477: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 114.9237 - mean_absolute_error: 114.9237 - val_loss: 343.8712 - val_mean_absolute_error: 343.8712\n",
            "Epoch 478/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 115.2242 - mean_absolute_error: 115.2242\n",
            "Epoch 478: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 10ms/step - loss: 115.1326 - mean_absolute_error: 115.1326 - val_loss: 338.6964 - val_mean_absolute_error: 338.6964\n",
            "Epoch 479/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 114.0122 - mean_absolute_error: 114.0122\n",
            "Epoch 479: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 113.4841 - mean_absolute_error: 113.4841 - val_loss: 329.8650 - val_mean_absolute_error: 329.8650\n",
            "Epoch 480/500\n",
            "83/89 [==========================>...] - ETA: 0s - loss: 109.5590 - mean_absolute_error: 109.5590\n",
            "Epoch 480: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 110.5221 - mean_absolute_error: 110.5221 - val_loss: 336.6779 - val_mean_absolute_error: 336.6779\n",
            "Epoch 481/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 108.5980 - mean_absolute_error: 108.5980\n",
            "Epoch 481: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 109.1154 - mean_absolute_error: 109.1154 - val_loss: 336.2715 - val_mean_absolute_error: 336.2715\n",
            "Epoch 482/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 119.0749 - mean_absolute_error: 119.0749\n",
            "Epoch 482: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 117.7312 - mean_absolute_error: 117.7312 - val_loss: 333.2993 - val_mean_absolute_error: 333.2993\n",
            "Epoch 483/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 119.9813 - mean_absolute_error: 119.9813\n",
            "Epoch 483: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 119.0242 - mean_absolute_error: 119.0242 - val_loss: 332.4675 - val_mean_absolute_error: 332.4675\n",
            "Epoch 484/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 113.8854 - mean_absolute_error: 113.8854\n",
            "Epoch 484: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 113.7878 - mean_absolute_error: 113.7878 - val_loss: 336.7939 - val_mean_absolute_error: 336.7939\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 112.9242 - mean_absolute_error: 112.9242\n",
            "Epoch 485: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 112.9242 - mean_absolute_error: 112.9242 - val_loss: 334.1185 - val_mean_absolute_error: 334.1185\n",
            "Epoch 486/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 108.7234 - mean_absolute_error: 108.7234\n",
            "Epoch 486: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 109.6334 - mean_absolute_error: 109.6334 - val_loss: 332.3593 - val_mean_absolute_error: 332.3593\n",
            "Epoch 487/500\n",
            "80/89 [=========================>....] - ETA: 0s - loss: 113.2113 - mean_absolute_error: 113.2113\n",
            "Epoch 487: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 111.9482 - mean_absolute_error: 111.9482 - val_loss: 340.1332 - val_mean_absolute_error: 340.1332\n",
            "Epoch 488/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 112.2168 - mean_absolute_error: 112.2168\n",
            "Epoch 488: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 7ms/step - loss: 112.4625 - mean_absolute_error: 112.4625 - val_loss: 344.7953 - val_mean_absolute_error: 344.7953\n",
            "Epoch 489/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 114.0424 - mean_absolute_error: 114.0424\n",
            "Epoch 489: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 114.3394 - mean_absolute_error: 114.3394 - val_loss: 351.1355 - val_mean_absolute_error: 351.1355\n",
            "Epoch 490/500\n",
            "82/89 [==========================>...] - ETA: 0s - loss: 113.6435 - mean_absolute_error: 113.6435\n",
            "Epoch 490: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 113.6740 - mean_absolute_error: 113.6740 - val_loss: 336.7208 - val_mean_absolute_error: 336.7208\n",
            "Epoch 491/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 111.5378 - mean_absolute_error: 111.5378\n",
            "Epoch 491: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 8ms/step - loss: 112.7076 - mean_absolute_error: 112.7076 - val_loss: 351.7132 - val_mean_absolute_error: 351.7132\n",
            "Epoch 492/500\n",
            "88/89 [============================>.] - ETA: 0s - loss: 112.1160 - mean_absolute_error: 112.1160\n",
            "Epoch 492: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 112.0924 - mean_absolute_error: 112.0924 - val_loss: 332.2501 - val_mean_absolute_error: 332.2501\n",
            "Epoch 493/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 123.3275 - mean_absolute_error: 123.3275\n",
            "Epoch 493: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 122.6210 - mean_absolute_error: 122.6210 - val_loss: 336.4756 - val_mean_absolute_error: 336.4756\n",
            "Epoch 494/500\n",
            "87/89 [============================>.] - ETA: 0s - loss: 124.6838 - mean_absolute_error: 124.6838\n",
            "Epoch 494: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 14ms/step - loss: 124.9691 - mean_absolute_error: 124.9691 - val_loss: 339.2357 - val_mean_absolute_error: 339.2357\n",
            "Epoch 495/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 107.1813 - mean_absolute_error: 107.1813\n",
            "Epoch 495: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 13ms/step - loss: 106.7078 - mean_absolute_error: 106.7078 - val_loss: 344.2911 - val_mean_absolute_error: 344.2911\n",
            "Epoch 496/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 119.4684 - mean_absolute_error: 119.4684\n",
            "Epoch 496: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 119.2327 - mean_absolute_error: 119.2327 - val_loss: 338.6844 - val_mean_absolute_error: 338.6844\n",
            "Epoch 497/500\n",
            "85/89 [===========================>..] - ETA: 0s - loss: 120.1452 - mean_absolute_error: 120.1452\n",
            "Epoch 497: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 119.7036 - mean_absolute_error: 119.7036 - val_loss: 326.7099 - val_mean_absolute_error: 326.7099\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - ETA: 0s - loss: 113.6802 - mean_absolute_error: 113.6802\n",
            "Epoch 498: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 11ms/step - loss: 113.6802 - mean_absolute_error: 113.6802 - val_loss: 330.1258 - val_mean_absolute_error: 330.1258\n",
            "Epoch 499/500\n",
            "86/89 [===========================>..] - ETA: 0s - loss: 109.2042 - mean_absolute_error: 109.2042\n",
            "Epoch 499: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 12ms/step - loss: 109.6833 - mean_absolute_error: 109.6833 - val_loss: 341.6422 - val_mean_absolute_error: 341.6422\n",
            "Epoch 500/500\n",
            "84/89 [===========================>..] - ETA: 0s - loss: 113.9815 - mean_absolute_error: 113.9815\n",
            "Epoch 500: val_loss did not improve from 303.41785\n",
            "89/89 [==============================] - 1s 9ms/step - loss: 114.0718 - mean_absolute_error: 114.0718 - val_loss: 336.0079 - val_mean_absolute_error: 336.0079\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f51a6cc32b0>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O7pDMdKqlbs"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXqCMMRnqlbt",
        "outputId": "ef3c9c05-3b6e-4fb8-eea5-eae3dd65fc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "111/111 [==============================] - 0s 2ms/step\n",
            "28/28 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_train_pred = NN_model.predict(X_train)\n",
        "y_test_pred = NN_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pzJYvIA4qlbt"
      },
      "outputs": [],
      "source": [
        "y_train_pred = y_train_pred.flatten()\n",
        "y_test_pred = y_test_pred.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ds7AZ1DeAhXN"
      },
      "outputs": [],
      "source": [
        "y_train = y_train * train_df2[\"floor_area_sqm\"]\n",
        "y_train_pred = y_train_pred * train_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "tSKRSfr5AhXN"
      },
      "outputs": [],
      "source": [
        "y_test = y_test * test_df2[\"floor_area_sqm\"]\n",
        "y_test_pred = y_test_pred * test_df2[\"floor_area_sqm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yOMevqbLAhXN"
      },
      "outputs": [],
      "source": [
        "train_n = X_train.shape[0]\n",
        "train_p = X_train.shape[1]\n",
        "train_mape = mean_absolute_percentage_error(y_train, y_train_pred)\n",
        "train_rmse = mean_squared_error(y_train, y_train_pred, squared = False)\n",
        "train_r2 = r2_score(y_train, y_train_pred)\n",
        "train_adj_r2 = 1 - (1 - train_r2) * (train_n - 1) / (train_n - train_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "qsIr6UyAAhXO"
      },
      "outputs": [],
      "source": [
        "test_n = X_test.shape[0]\n",
        "test_p = X_test.shape[1]\n",
        "test_mape = mean_absolute_percentage_error(y_test, y_test_pred)\n",
        "test_rmse = mean_squared_error(y_test, y_test_pred, squared = False)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "test_adj_r2 = 1 - (1 - test_r2) * (test_n - 1) / (test_n - test_p - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P7aCEJMAhXO",
        "outputId": "69c4ceb5-3a5b-477d-e2c1-4e4fd29753fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train MAPE: 2.62%\n",
            "Train RMSE: 24825.28926661958\n",
            "Train R2: 0.9785344986231573\n",
            "Train Adj R2: 0.9780808270538147\n",
            "\n",
            "Test MAPE: 5.71%\n",
            "Test RMSE: 44579.44500686539\n",
            "Test R2: 0.9272039424792416\n",
            "Test Adj R2: 0.9206270709458068\n"
          ]
        }
      ],
      "source": [
        "print(\"Train MAPE: {:.2f}%\".format(train_mape * 100))\n",
        "print(\"Train RMSE:\", train_rmse)\n",
        "print(\"Train R2:\", train_r2)\n",
        "print(\"Train Adj R2:\", train_adj_r2)\n",
        "print()\n",
        "print(\"Test MAPE: {:.2f}%\".format(test_mape * 100))\n",
        "print(\"Test RMSE:\", test_rmse)\n",
        "print(\"Test R2:\", test_r2)\n",
        "print(\"Test Adj R2:\", test_adj_r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kp9Udji5AhXO"
      },
      "outputs": [],
      "source": [
        "train_residuals = y_train - y_train_pred\n",
        "test_residuals = y_test - y_test_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ85tBG9lnaW"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "lba-Q5fUAhXO",
        "outputId": "ed6a6fe2-f080-4887-f62e-b69dc86fd49f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#idk residual plot is vs predicted value or actual value lol\n",
        "plt.scatter(y_train_pred, train_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "r3zIg4uPAhXO",
        "outputId": "cbcb094c-9d5e-473e-b7ad-2e670e3bd0b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, train_residuals)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "mk33-hNuAhXO",
        "outputId": "d1e0b764-1b71-4d9f-9c49-800b91865bbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_train, y_train_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "3xO2QEsziBl2",
        "outputId": "de5b5105-bb96-42ab-b6ee-641dc3535abb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Residual Plot')"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_test_pred, test_residuals)\n",
        "plt.xlabel(\"Predicted Price\")\n",
        "plt.ylabel(\"Residual\")\n",
        "plt.title(\"Residual Plot\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Au9PmJViiBl3",
        "outputId": "fa19c8e0-86e7-4804-998b-797d19d0d22b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Actual vs Predicted Price')"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.scatter(y_test, y_test_pred)\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Actual vs Predicted Price\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
